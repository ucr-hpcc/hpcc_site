<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>HPCC â€“ Overview</title><link>https://hpcc.ucr.edu/about/overview/</link><description>Recent content in Overview on HPCC</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://hpcc.ucr.edu/about/overview/index.xml" rel="self" type="application/rss+xml"/><item><title>About: Welcome to the HPC Center (HPCC)</title><link>https://hpcc.ucr.edu/about/overview/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hpcc.ucr.edu/about/overview/introduction/</guid><description>
&lt;p>&lt;img align="right" title="hpclogo" src="../../img/background_small.jpg">&lt;img/>&lt;/p>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>The High-Performance Computing Center (HPCC) provides state-of-the-art research
computing infrastructure and training accessible to all UCR researchers and
affiliates at low cost. Currently, it supports over 150 research groups with
more than 800 active users. Its resources are also heavily used for instructing
undergraduate and graduate classes in a wide range of computational,
statistical, life science and engineering disciplines.&lt;/p>
&lt;div class="alert alert-primary" role="alert">
&lt;h4 class="alert-heading">News&lt;/h4>
This year the HPCC was awarded an MRI equipment grant (#2215705) by NSF for the acquisition of a Big Data HPC Cluster in the total amount of $942,829. For details see &lt;a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2215705&amp;HistoricalAwards=false">here&lt;/a>.&lt;/li>
&lt;/div>
&lt;h3 id="quick-start">Quick start&lt;/h3>
&lt;p>The following lists the most frequently visted pages of the HPCC site. They can also be accessed via the navigation system outlined below.&lt;/p>
&lt;h4 id="navigating-and-searching-this-site">Navigating and searching this site&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>Top menu&lt;/strong> located in bar on top of each page provides links to the main content categories&lt;/li>
&lt;li>&lt;strong>Section menu&lt;/strong> to the left links to subpages of each main category&lt;/li>
&lt;li>&lt;strong>Table of content&lt;/strong> to the right links to sections within each page&lt;/li>
&lt;/ul>
&lt;h4 id="gain-access">Gain access&lt;/h4>
&lt;ul>
&lt;li>&lt;a href="https://hpcc.ucr.edu/about/overview/access/">User account creation&lt;/a> for accessing HPCC&amp;rsquo;s infrastructure&lt;/li>
&lt;li>&lt;a href="https://hpcc.ucr.edu/about/overview/rates/">Latest recharging rates&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://hpcc.ucr.edu/manuals/access/login/">Log in instructions&lt;/a>&lt;/li>
&lt;/ul>
&lt;h4 id="hpc-usage">HPC usage&lt;/h4>
&lt;ul>
&lt;li>&lt;a href="https://hpcc.ucr.edu/manuals/">Usage instructions&lt;/a> are provided in the manual section&lt;/li>
&lt;li>To efficiently navigate the Manuals pages, use the &lt;a href="https://raw.githubusercontent.com/ucr-hpcc/ucr-hpcc.github.io/master/static/img/Manual_Navigation.png">Manual dropdown&lt;/a> in the menu bar on the top of this site.&lt;/li>
&lt;li>&lt;a href="https://hpcc.ucr.edu/events/events/">Event schedule&lt;/a> for workshops and user meetings&lt;/li>
&lt;/ul>
&lt;h4 id="infrastructure-description">Infrastructure description&lt;/h4>
&lt;ul>
&lt;li>&lt;a href="https://hpcc.ucr.edu/about/hardware/overview/">Infrastructure description&lt;/a> of HPCC&amp;rsquo;s clusters and parallel storage systems&lt;/li>
&lt;li>&lt;a href="https://goo.gl/43eOwQ">Facility description document&lt;/a> for grant applications and related purposes&lt;/li>
&lt;/ul>
&lt;h4 id="help-and-contacts">Help and contacts&lt;/h4>
&lt;ul>
&lt;li>For questions or requesting user accounts, please email &lt;a href="mailto:support@hpcc.ucr.edu">support@hpcc.ucr.edu&lt;/a>.&lt;/li>
&lt;li>For discussions with other users, please consider joining the &lt;a href="https://ucr-hpcc.slack.com/">ucr-hpcc&lt;/a> workspace on Slack.&lt;/li>
&lt;li>&lt;a href="https://hpcc.ucr.edu/about/overview/people/">Contact information&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>About: Access</title><link>https://hpcc.ucr.edu/about/overview/access/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hpcc.ucr.edu/about/overview/access/</guid><description>
&lt;h2 id="user-account-requests">User account requests&lt;/h2>
&lt;ul>
&lt;li>Email user account request to &lt;a href="mailto:support@hpcc.ucr.edu">support@hpcc.ucr.edu&lt;/a>. Please include full name, NetID and email address of both user and PI. Users need to be members of the PI&amp;rsquo;s group. Prefrentially, user account requests should come from the correspoding PI directly. If the request comes from a new user then the PI needs to be included in the email exchange (CC&amp;rsquo;ed).&lt;/li>
&lt;li>An FAU for the annual subscription fee (see below) is required if a PI&amp;rsquo;s lab is not registered yet.&lt;/li>
&lt;/ul>
&lt;p>After the access information for an account has been received via email, new users want to follow the login instructions &lt;a href="../../manuals/login">here&lt;/a>.&lt;/p>
&lt;h2 id="recharging-rates">Recharging rates&lt;/h2>
&lt;p>HPCC&amp;rsquo;s recharging rate structure is outlined below. A more formal summary is available &lt;a href="../../about/facility/rates">here&lt;/a>.&lt;/p>
&lt;h2 id="lab-based-registration-fee">Lab-based Registration Fee&lt;/h2>
&lt;p>An annual registration fee of $1,000 gives all members of a UCR lab access to our high-performance computing infrastructure.
The registration provides access to the following resources:&lt;/p>
&lt;ul>
&lt;li>Over 6,800 CPU cores (60% Intel and 40% AMD), ~60,000 cuda cores (Nvidia K80,P100 GPUs), ~2PB parallel GPFS-based disk space, 512GB-1TB of memory/node, etc. More details are available on the hardware page.&lt;/li>
&lt;li>Over 1000 software packages and community databases. Details are available on the software page.&lt;/li>
&lt;li>Free attendance of workshops offered by HPCC staff&lt;/li>
&lt;li>Free consultation services (up to 1 hour per month)&lt;/li>
&lt;li>Note: there is no extra charge for CPU usage but each user and lab have CPU quotas of 256 and 512 CPU cores, respectively. Computing jobs exceeding these quotas can be submitted but will stay in a queued state until resources within the quota limits become available.&lt;/li>
&lt;/ul>
&lt;h2 id="big-data-storage">Big data storage&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Standard user accounts have a storage quota of 20 GB. To gain access to much larger storage pools, PIs have the option to rent or own storage space.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Storage rental option&lt;/p>
&lt;ul>
&lt;li>$1000 per 10TB of usable and backed up storage space. Storage pool is shared among all user accounts of a registered lab.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="ownership-models">Ownership models&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Disk storage&lt;/p>
&lt;ul>
&lt;li>Lab purchases storage hardware (&lt;em>e.g.&lt;/em> hard drives) according to the specifications of the facility. Owned hard drives will be added to the facility&amp;rsquo;s parallel GPFS storage system. The annual support fee for owned disk storage is $250 per 10TB of usable and backed-up storage space. The owned storage space is only available to the users of a PI or those a PI wishes to give access to.&lt;/li>
&lt;li>Owned storage can be attractive for labs with storage needs above 40 TBs.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Compute nodes&lt;/p>
&lt;ul>
&lt;li>Lab purchases compatible computer nodes (e.g. with supported network cards). Examples of popular high-density architecture are quad node systems shown here: &lt;a href="http://www.thinkmate.com/system/hdx-xt24-5260v4-sas3">HDX XT24-5260V4&lt;/a> and &lt;a href="https://www.thinkmate.com/system/hdx-xn24-52s1">HDX XN24-52S1&lt;/a>. A quad node system includes 4 nodes each configured with two 16 core Intel chips (total physical core count 128), 512GB of RAM, 1.2TB SSD and FDR-IB interconnect. Similar options exist for &lt;a href="https://www.gigabyte.com/us/High-Density-Server/H262-Z63-rev-100#ov">AMD EPYC&lt;/a> and &lt;a href="https://www.thinkmate.com/systems/servers/gpx">GPU&lt;/a> nodes.&lt;/li>
&lt;li>Nodes are administered under a priority queueing system that gives users from an owner lab priority and also increases that lab&amp;rsquo;s overall CPU quota (see above) by the number of owned CPU cores.&lt;/li>
&lt;li>Owned computer nodes are an attractive solution for labs requiring 24/7 access to hundreds of CPU cores with no or only minor waiting times in queue.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="software-install">Software install&lt;/h2>
&lt;ul>
&lt;li>Registered users can email software install requests to HPCC&amp;rsquo;s issue tracking system @ &lt;a href="mailto:support@hpcc.ucr.edu">support@hpcc.ucr.edu&lt;/a>. Install requests are addressed in the order received. Simple installs are addressed within 1 to a few days. Complex installs may take longer.&lt;/li>
&lt;/ul>
&lt;!---
## Startup packages for new PIs
Startup packages are available for variable numbers and architectures of HPC nodes and storage amounts. This includes the following components:
Standard startup packages in the amount of $20K (N=1), $30K (N=2), $40K (N=3)
and so on are available. Note: N refers to the number of HPC nodes below. The
cost for these packages can be covered by the initial complement of new PIs.
* N HPC node(s): owned by lab for 5 yrs and administered under priority queueing model. After this time the node becomes part of the shared HPCC cluster resources.
* Each node with 32* Intel CPU cores (64* logical cores), 512GB RAM and Infiniband interconnect. *The core numbers might nearly double when newer and less expensive Intel chip sets will be released this year. However, the per node cost may be subject to rapid changes (e.g the cost of RAM has increased by several fold in last year).
* Alternative node architecture (_e.g._ GPU) are available upon request
* Owned HPC nodes with various CPU/GPU architectures, RAM and SSD specifications. Pricing is comptetitive, but will greatly depend on the current market value of HPC components, custom configurations and discounts provided by vendors.
* Rented big data storage @ $1000 for 10TB per yr covered for 5 yrs; or owned disk storage when storage needs are above 20TB
* HPCC subscription fee of $1000/yr covered for 5 yrs
To configure a startup HPC package, please contact the facility staff directly.
-->
&lt;h2 id="department-cluster-membership-with-owned-computing-nodes">Department cluster membership with owned computing nodes&lt;/h2>
&lt;p>This option addresses the need of department-level HPC access where the standard
PI-based membership is not practical, &lt;em>e.g.&lt;/em> provide cluster access to large number of undergraduate
students in classes. Under this model a department purchases computer nodes
that will be administered similarly as described above under the &lt;em>Ownership
model&lt;/em>. Due to the large number of expected users from departments, the
CPU quota per user is usually lower compared to the PI-based model.&lt;/p>
&lt;h2 id="external-user-accounts">External user accounts&lt;/h2>
&lt;p>Accounts for external customers can only be granted if a lab has a strong
affiliation with UC Riverside, such as a research collaboration with UCR
researchers. Both the corresponding UCR PI and external collaborator need to
maintain an HPCC subscription. External accounts are subject to an annual
review and approval process. To be approved, the external and internal PIs have
to complete this &lt;a href="https://bit.ly/32O1lC9">External Usage Justification&lt;/a>.&lt;/p>
&lt;h2 id="facility-description">Facility description&lt;/h2>
&lt;ul>
&lt;li>The latest hardware/facility description (&lt;em>e.g.&lt;/em> for grant applications) is available &lt;a href="https://goo.gl/43eOwQ">here&lt;/a>.&lt;/li>
&lt;/ul></description></item><item><title>About: Activity Report</title><link>https://hpcc.ucr.edu/about/overview/activity/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hpcc.ucr.edu/about/overview/activity/</guid><description>
&lt;h2 id="summary-report">Summary report&lt;/h2>
&lt;p>The following activity report is generated by &lt;code>Grafana&lt;/code> and refreshed on this page every 10 minutes.
It summarizes CPU cluster activity on HPCC&amp;rsquo;s computing resources including Intel, Batch, Highmem and GPU partitions:&lt;/p>
&lt;p style="text-align: center;">&lt;font color="red">Click image to enlarge!&lt;/font>&lt;/p>
&lt;div>
&lt;a href="https://cluster.hpcc.ucr.edu/activity-report/">
&lt;img alt="intel_part" border="0" src="https://cluster.hpcc.ucr.edu/activity-report/pane2.png" style="display:block;margin-right:auto;margin-left:auto;text-align:center;">
&lt;img alt="batch_part" border="0" src="https://cluster.hpcc.ucr.edu/activity-report/pane3.png" style="display:block;margin-right:auto;margin-left:auto;text-align:center;">
&lt;img alt="highmem_part" border="0" src="https://cluster.hpcc.ucr.edu/activity-report/pane4.png" style="display:block;margin-right:auto;margin-left:auto;text-align:center;">
&lt;img alt="gpu_part" border="0" src="https://cluster.hpcc.ucr.edu/activity-report/pane5.png" style="display:block;margin-right:auto;margin-left:auto;text-align:center;">
&lt;/a>
&lt;/div></description></item><item><title>About: Rates</title><link>https://hpcc.ucr.edu/about/overview/rates/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hpcc.ucr.edu/about/overview/rates/</guid><description>
&lt;h2 id="facility-description">Facility description&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://goo.gl/43eOwQ">Facility description&lt;/a> (&lt;em>e.g.&lt;/em> for grant applications)&lt;/li>
&lt;/ul>
&lt;h2 id="recharging-rates">Recharging rates&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://bit.ly/3IZUAQQ">Recharging rates: 2022/2023&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://bit.ly/3iPkbiv">Recharging rates: 2021/2022&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://bit.ly/3jeK3nF">Recharging rates: 2020/2021&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://bit.ly/2ZWbND7">Recharging rates: 2019/2020&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://goo.gl/1mVfLM">Recharging rates: 2018/2019&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://goo.gl/QjJgzu">Recharging rates: 2017/2018&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://goo.gl/jJWpon">Recharging rates: 2016/2017&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="external-user-accounts">External user accounts&lt;/h2>
&lt;p>Accounts for external customers can only be granted if a lab has a strong
affiliation with UC Riverside, such as a research collaboration with UCR
researchers. Both the corresponding UCR PI and external collaborator need to
maintain an HPCC subscription. External accounts are subject to an annual
review and approval process. To be approved, the external and internal PIs have
to complete this &lt;a href="https://bit.ly/32O1lC9">External Usage Justification&lt;/a>.&lt;/p></description></item><item><title>About: Contact</title><link>https://hpcc.ucr.edu/about/overview/contact/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hpcc.ucr.edu/about/overview/contact/</guid><description>
&lt;h2 id="facility">Facility&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="mailto:aleon008@ucr.edu">Austin Leong&lt;/a>, Sr. HPC Systems Administrator&lt;/li>
&lt;li>&lt;a href="mailto:tkapp001@ucr.edu">Tomin Kappiarumalayil&lt;/a>, HPC Systems Administrator, Assistant&lt;/li>
&lt;li>&lt;a href="mailto:ejaco020@ucr.edu">Emerson Jacobson&lt;/a>, HPC Systems Administrator, Assistant&lt;/li>
&lt;li>&lt;a href="http://girke.bioinformatics.ucr.edu">Thomas Girke&lt;/a>, Director of HPC Center&lt;/li>
&lt;/ul>
&lt;h2 id="advisory-board-executive-committee">Advisory Board (executive committee)&lt;/h2>
&lt;p>The responsibilities of the Advisory Board are outlined &lt;a href="https://goo.gl/X3p1VK">here&lt;/a>.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>UCR faculty members with hands-on experience in HPC&lt;/p>
&lt;ul>
&lt;li>Jason E Stajich (Microbiology &amp;amp; Plant Pathology)&lt;/li>
&lt;li>Wenxiu Ma (Statistics)&lt;/li>
&lt;li>Stefano Lonardi (CSE)&lt;/li>
&lt;li>Mark Alber (Mathematics)&lt;/li>
&lt;li>Adam Godzik (Biomedical Sciences)&lt;/li>
&lt;li>Laura Sales (Physics)&lt;/li>
&lt;li>Ahmed Eldawy (CSE)&lt;/li>
&lt;li>Xinping Cui (Statistics)&lt;/li>
&lt;li>Leonard Mueller (Chemistry)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>HPC expert staff members from UCR&lt;/p>
&lt;ul>
&lt;li>Keith Richards-Dinger (Earth Sciences)&lt;/li>
&lt;li>Victor Hill (CS)&lt;/li>
&lt;li>Bill Strossman (C&amp;amp;C)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>External members from academia and industry&lt;/p>
&lt;ul>
&lt;li>One of each to be added here.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="office-location-and-mailing-address">Office location and mailing address&lt;/h2>
&lt;p>1208/1207 Genomics Building (&lt;a href="https://goo.gl/OVKyxv">Google Map&lt;/a>)
3401 Watkins Drive
University of California
Riverside, CA 92521&lt;/p>
&lt;h2 id="server-rooms">Server rooms&lt;/h2>
&lt;h3 id="genomics">Genomics&lt;/h3>
&lt;p>HPCC&amp;rsquo;s main server room is in the Genomics Building, Rm 1120A.&lt;/p>
&lt;h3 id="colo-server-room">CoLo server room&lt;/h3>
&lt;p>School of Medicine CoLo&lt;/p>
&lt;h2 id="help">Help&lt;/h2>
&lt;p>For questions or requesting new user accounts please email &lt;a href="mailto:support@hpcc.ucr.edu">support@hpcc.ucr.edu&lt;/a>.&lt;/p></description></item><item><title>About: Acknowledgement of Facility</title><link>https://hpcc.ucr.edu/about/overview/acknowledgement/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hpcc.ucr.edu/about/overview/acknowledgement/</guid><description>
&lt;h2 id="acknowledgement-in-publications">Acknowledgement in Publications&lt;/h2>
&lt;p>We appreciate that you have chosen our facility to support your research and
would like to remind you to add the following statement to acknowledge the
High-Performance Computing Center in your publications and presentations.&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;p>Computations were performed using the computer clusters and data storage
resources of the HPCC, which were funded by grants from NSF (MRI-2215705, MRI-1429826) and
NIH (1S10OD016290-01A1).&lt;/p>
&lt;/div>
&lt;p>Your success is important to us and we would appreciate if you could share with us the references or URLs to any
publications or presentations that used our facilty.&lt;/p>
&lt;h2 id="grants">Grants&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2215705">NSF MRI-2215705&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1429826">NSF MRI-1429826&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://federalreporter.nih.gov/Projects/Details/?projectId=624283&amp;amp;ItemNum=881394&amp;amp;totalItems=892504&amp;amp;searchId=b850241613a74a58962c0bd1a1edd5d4&amp;amp;searchMode=Smart&amp;amp;page=8814&amp;amp;pageSize=100&amp;amp;sortField=Ic&amp;amp;sortOrder=asc&amp;amp;filters=&amp;amp;navigation=True">NIH 1S10OD016290-01A1&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>About:</title><link>https://hpcc.ucr.edu/about/overview/slides_backup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hpcc.ucr.edu/about/overview/slides_backup/</guid><description>
&lt;p>#&amp;mdash;
#type: docs
#linkTitle: Overview slides
#title: Overview slides
#weight: 6
#&amp;mdash;&lt;/p>
&lt;iframe src="https://docs.google.com/presentation/d/e/2PACX-1vQIuy-2Z50zj3wr5dNjyes5tnUjGP84vUBn2vFxM5y5qb_kCOpWfjKu_G-F9a-46JniTsgVWWmQn_9m/embed?start=false&amp;loop=false&amp;delayms=3000" frameborder="0" width="960" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true">&lt;/iframe></description></item></channel></rss>