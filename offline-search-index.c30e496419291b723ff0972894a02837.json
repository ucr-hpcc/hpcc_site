[{"body":" Note: HPCC’s latest Recharging Rates document is here.\n User account requests  Email user account request to support@hpcc.ucr.edu. Please include full name, NetID and email address of both user and PI. Users need to be members of the PI’s group. An FAU for the annual subscription fee (see below) is required if a PI’s lab is not registered yet. Subscribed labs can request additional user account by emailing support@hpcc.ucr.edu the same user information as above.  After the user account information has been received, follow the login instructions here.\nRecharging rates HPCC’s recharging rate structure is outlined below. A more formal summary is available here.\nLab-based Registration Fee An annual registration fee of $1,000 gives all members of a UCR lab access to our high-performance computing infrastructure. The registration provides access to the following resources:\n Over 6,800 CPU cores (60% Intel and 40% AMD), ~60,000 cuda cores (Nvidia K80,P100 GPUs), ~2PB parallel GPFS-based disk space, 512GB-1TB of memory/node, etc. More details are available on the hardware page. Over 1000 software packages and community databases. Details are available on the software page. Free attendance of workshops offered by HPCC staff Free consultation services (up to 1 hour per month) Note: there is no extra charge for CPU usage but each user and lab have CPU quotas of 256 and 512 CPU cores, respectively. Computing jobs exceeding these quotas can be submitted but will stay in a queued state until resources within the quota limits become available.  Big data storage   Standard user accounts have a storage quota of 20 GB. To gain access to much larger storage pools, PIs have the option to rent or own storage space.\n  Storage rental option\n $1000 per 10TB of usable and backed up storage space. Storage pool is shared among all user accounts of a registered lab.    Ownership models   Disk storage\n Lab purchases storage hardware (e.g. hard drives) according to the specifications of the facility. Owned hard drives will be added to the facility’s parallel GPFS storage system. The annual support fee for owned disk storage is $250 per 10TB of usable and backed-up storage space. The owned storage space is only available to the users of a PI or those a PI wishes to give access to. Owned storage can be attractive for labs with storage needs above 40 TBs.    Compute nodes\n Lab purchases compatible computer nodes (e.g. with supported network cards). Examples of popular high-density architecture are quad node systems shown here: HDX XT24-5260V4 and HDX XN24-52S1. A quad node system includes 4 nodes each configured with two 16 core Intel chips (total physical core count 128), 512GB of RAM, 1.2TB SSD and FDR-IB interconnect. Similar options exist for AMD EPYC and GPU nodes. Nodes are administered under a priority queueing system that gives users from an owner lab priority and also increases that lab’s overall CPU quota (see above) by the number of owned CPU cores. Owned computer nodes are an attractive solution for labs requiring 24/7 access to hundreds of CPU cores with no or only minor waiting times in queue.    Software install  Registered users can email software install requests to HPCC’s issue tracking system @ support@hpcc.ucr.edu. Install requests are addressed in the order received. Simple installs are addressed within 1 to a few days. Complex installs may take longer.  Department cluster membership with owned computing nodes This option addresses the need of department-level HPC access where the standard PI-based membership is not practical, e.g. provide cluster access to large number of undergraduate students in classes. Under this model a department purchases computer nodes that will be administered similarly as described above under the Ownership model. Due to the large number of expected users from departments, the CPU quota per user is lower compared to the PI-based model.\nExternal user accounts Accounts for external customers can only be granted if a lab has a strong affiliation with UC Riverside, such as a research collaboration with UCR researchers. Both the corresponding UCR PI and external collaborator need to maintain an HPCC subscription. External accounts are subject to an annual review and approval process. To be approved, the external and internal PIs have to complete this External Usage Justification.\nFacility description  The latest hardware/facility description (e.g. for grant applications) is available here.  ","excerpt":" Note: HPCC’s latest Recharging Rates document is here.\n User account …","ref":"/about/facility/access/","title":"Access"},{"body":"Note: for the most current information on exceptions on HPCC’s cluster please consult its Alerts or Twitter pages.\nFeb/Mar 2022  Rollout of Rocky and DUO: Feb 18 through Mar 17, 2022. For details see here.  Mar 2021  Viet Pham joins HPCC as HPC systems administrator. - Welcome Viet!  May/Jun 2020  Melody Asghari joins HPCC as HPC systems administrator assistant. - Welcome Melody!  Mar/Apr 2020  For updates on HPCC’s operation during the COVID-19 crisis please visit the Alerts page of this site.  Sep 2019  Travis Nasser joins HPCC as HPC systems administrator assistant. - Welcome Travis! Major updates applied. - Check here for important changes.  Aug 2019  Charles Forsyth left the position of HPC systems administrator and has moved on to new opportunities. - Good luck Chuck!  Apr 2018  Abraham Park joins HPCC as HPC systems administrator assistant. - Welcome Abe!  Jun 2017  Charles Forsyth joins HPCC as new full-time HPC systems administrator. - Welcome Chuck!  Feb 2017 With funding provided by Michael Pazzani’s office (RED) we were able to purchase and install major hardware upgrades. This included the following hardware resources:\n Added 28 Intel nodes with a total of 896 CPU cores (or 1,792 logical CPU cores) and 512 GB of RAM per node Added 8 NVIDIA K80 GPUs increasing total number of cuda cores in GPU queue to 59,904 Redesign of Infiniband network to support new computer nodes and enhance future scalabilty of IB network to over 1000 nodes  Dec 2016  UCR approval of plans to form HPC Center  Sept 2016  Expansion of GPFS storage system: 2 disk enclosures for 120 8TB drives Expansion of high-memory queue: 4 nodes Install of new Intel batch queue: 12 nodes  Mar 2016  Expansion of batch queues: 14 nodes  Apr 2015  Deployment of new FDR IB network @ 56Gbs Deployment of 28 AMD nodes (2,048 AMD cores), funded by NSF-MRI-2014 Deployment of high-memory Intel nodes (each with 1TB RAM) Deployment of GPU nodes (NVIDIA K80) Deployment of big data GPFS disk storage system, funded by NIH-S10-2014  May 2014 Award of equipment grants from NSF and NIH\n NIH-S10-2014 (1S10OD016290-01A1): $652,816 NSF-MRI-2014 (MRI-1429826): $783,537  ","excerpt":"Note: for the most current information on exceptions on HPCC’s cluster …","ref":"/news/announce/","title":"News and announcements"},{"body":"Mission The High-Performance Computing Center (HPCC) provides state-of-the-art research computing infrastructure and training accessible to all UCR researchers and affiliates at low cost. This includes access to the shared HPC resources and services.\nFacility description  Facility description (e.g. for grant applications)  Office location and mailing address 1208/1207 Genomics Building (Google Map) 3401 Watkins Drive University of California Riverside, CA 92521\nServer rooms Genomics Genomics Building, Rm 1120A\nCoLo server room School of Medicine CoLo\nMailing address (delivery room) 3401 Watkins Drive 1202 Genomics Riverside, CA 92521\n","excerpt":"Mission The High-Performance Computing Center (HPCC) provides …","ref":"/about/facility/","title":"Facility"},{"body":"What software is installed? There are hundreds of software tools installed on HPCC’s systems. Most software is administered under a module system. To find out what software is installed, users want to consult the software listing here or run from a user account the following command.\nmodule avail  More details on this is available on the HPCC manual pages here.\nSoftware install requests Registered users can email software install requests to HPCC’s issue tracking system @ support@hpcc.ucr.edu. Install requests are addressed in the order received. Simple installs are addressed within 1 to a few days. Complex installs may take longer.\n","excerpt":"What software is installed? There are hundreds of software tools …","ref":"/about/software/installs/","title":"Software Installs"},{"body":"Introduction Getting started using HPCC and Amazon Web Service (AWS) to quickly create an on-demand cluster private to you.\nHPCC AWS Cluster benefits.  Build a private cluster in 10 min.  Any number of nodes, auto scaling (up and down), limit 20 to start Any type of compute nodes  High memory High CPU single/multi node GPUs Choose HPCC default configuration     For only as long as you need it - delete when done Familiar interface and job scheduler Easy ability to have the software you need installed by HPCC, if it’s not there already Build as many of these clusters as you need (even at the same time) Pay for only the time you use it - per/min billing  How to get started The manual menu on the left will walk you through the process while the simple steps are outlined below.\n Create a Master AWS Account (if you do not have one yet) and one or more AWS IAM account(s). Associate your AWS Master account with Amazon as a UCR educational member to take advantage of a data egress waiver. Set up your HPCC account to access and use your HPCC AWS Cluster. Create your cluster and begin computing  ","excerpt":"Introduction Getting started using HPCC and Amazon Web Service (AWS) …","ref":"/manuals/ext_cloud/aws/intro/","title":"Introduction"},{"body":"Introduction This manual provides an introduction to the usage of the HPCC cluster. All servers and compute resources of the HPCC cluster are available to researchers from all departments and colleges at UC Riverside for a minimal recharge fee (see rates). To request an account, please email support@hpcc.ucr.edu. The latest hardware/facility description for grant applications is available here.\nOverview Storage  Four enterprise class HPC storage systems Approximately 2 PB (2048 TB) of network storage GPFS (NFS and SAMBA via GPFS) Automatic snapshots and archival backups  Network  Ethernet  1 Gb/s switch x 5 1 Gb/s switch 10 Gig uplink 10 Gb/s switch for Campus wide Science DMZ redundant, load balanced, robust mesh topology   Interconnect  56 Gb/s InfiniBand (FDR)    Head Nodes All users should access the cluster via ssh through cluster.hpcc.ucr.edu, this address will automatically balance traffic to one of the available head nodes.\n Penguin  Resources: 8 cores, 64 GB memory Primary function: submitting jobs to the queuing system Secondary function: development; code editing and running small (under 50 % CPU and under 1 GB RAM) sample jobs   Pigeon  Resources: 16 cores, 128 GB memory Primary function: submitting jobs to the queuing system Secondary function: development; code editing and running small (under 50 % CPU and under 1 GB RAM) sample jobs   Pelican  Resources: 32 cores, 64 GB memory Primary function: submitting jobs to the queuing system Secondary function: development; code editing and running small (under 50 % CPU and under 1 GB RAM) sample jobs   Parrot  Resources: 32 cores, 32 GB memory Primary function: submitting jobs to the queuing system Secondary function: development; code editing and running small (under 50 % CPU and under 1 GB RAM) sample jobs    Worker Nodes  Batch  c01-c48: each with 64 AMD cores and 512 GB memory   Highmem  h01-h06: each with 32 Intel cores and 1024 GB memory   GPU  gpu01-gpu02: each with 32 (HT) cores Intel Haswell CPUs and 2 x NVIDIA Tesla K80 GPUs (~10000 CUDA cores each) and 128 GB memory gpu03-gpu04: each with 32 (HT) cores Intel Haswell CPUs and 4 x NVIDIA Tesla K80 GPUs (~10000 CUDA cores each) and 128 GB memory   Intel  i01-i40: each with 32 Intel Broadwell cores and 512 GB memory    ","excerpt":"Introduction This manual provides an introduction to the usage of the …","ref":"/manuals/hpc_cluster/intro/","title":"Introduction"},{"body":"","excerpt":"","ref":"/manuals/linux_basics/","title":"Linux Basics"},{"body":"Computer cluster  Over 6,500 CPU cores (60% Intel, 40% AMD) 512-1024GB RAM per node GPU (K80, P100): 67,000 cuda cores IB network @ 56Gbs Queueing system Slurm  Parallel data storage system  2PB GPFS storage (scales to \u003e50PB) Home directories on dedicated system  Backup system  2PB GPFS storage Geographically separated server room  Server room  Genomics Building, Rm 1120A Size 600 sqft Raised floor cooling with redundant AC units Backup power: UPS plus generator  ","excerpt":"Computer cluster  Over 6,500 CPU cores (60% Intel, 40% AMD) 512-1024GB …","ref":"/about/hardware/overview/","title":"Hardware Overview"},{"body":"   Date \u0026 Time Location Instructors Title and Description Material     September 2, 2021  10:00am - 11:00am Virtually via Zoom Jordan Hayes Coffee Hour - Conda InstallsTopics:1. Why conda?2. Configure3. Install Examples Registration SlidesZoom   June 14, 2021  10:00am - 11:00am Virtually via Zoom Viet Pham Coffee Hour - Remote EditingTopics:1. Linux filesystems2. Basic Linux navigation3. Remote editing Registration SlidesZoom   December 18, 2020  1:00pm - 4:30pm Virtually via Zoom Jordan Hayes, Melody Asghari, Isaac Salinas, Abraham Park \u0026 Thomas Girke Introduction to HPCC’s Cluster UsageTopics:1. HPCC Infrastructure (Slides) 2. Linux Basics for HPC (Slides A, Slides B) 3. Cluster Usage (Slides)  4. Special Topic: R on HPC (Slides) 5. Special Topic: R Use Case from Statistics (Slides)  Registration Agenda Video Recordings   April 15, 2020  2:45pm - 5:00pm Virtually via Zoom Abraham Park, Travis Nasser, Jordan Hayes Intro to HPCC This workshop will cover topics including: * The cluster scheduler, Slurm* Cluster policies and best practices* Basic job submissions to the cluster* and others ..  Registration HPCC Intro Basics:SlidesVideo   April 14, 2020  12:15pm - 2:15pm Virtually via Zoom Travis Nasser, Abraham Park, Jordan Hayes Intro to Linux This workshop will cover topics including: * Basic Linux commands* Linux File systems* Creating Scripts Registration Linux Intro Basics:SlidesVideo   January 29, 2020  1:00pm - 3:15pm Genomics Bldg Auditorium 1102A Jordan Hayes Intro to HPCC This workshop will cover topics including: * The cluster scheduler, Slurm* Cluster policies and best practices* Basic job submissions to the cluster* and others ..  HPCC Intro Basics:Slides   January 27, 2020  1:30pm - 3:15pm Genomics Bldg Auditorium 1102A Abraham Park Intro to Linux This workshop will cover topics including: * Basic Linux commands* Linux File systems* Creating Scripts Linux Intro Basics:Slides\n   November 7, 2019  1:00pm - 3:00pm Genomics Bldg Auditorium 1102A Jordan Hayes \u0026 Abraham Park Intro to Linux and HPCC This workshop will cover topics including: * Connecting to the Cluster* Basic Linux commands* File systems* The cluster scheduler, Slurm* Cluster policies and best practices* Basic job submissions to the cluster* and others ..  Linux Intro Basics:Slides\nHPCC Intro Basics:Slides   June 17, 2019  2:00pm - 5:00pm Genomics Bldg Auditorium 1102A Charles Forsyth Linux Basics and HPCC Cluster Introduction This workshop will cover topics including: * Connecting to the Cluster* Basic Linux commands* File systems* The cluster scheduler, Slurm* Cluster policies and best practices* Basic job submissions to the cluster* and others ..  Registration HPCC Linux/Cluster Basics:Slides\nFiles   April 5, 2019  2:00pm - 5:00pm Genomics Bldg Auditorium 1102A Charles Forsyth Linux Basics and HPCC Cluster Introduction This workshop will cover topics including: * Connecting to the Cluster* Basic Linux commands* File systems* The cluster scheduler, Slurm* Cluster policies and best practices* Basic job submissions to the cluster* and others ..  Registration HPCC Linux/Cluster Basics:Slides\nFiles   February 15, 2019  2:00pm - 5:00pm Genomics Bldg Auditorium 1102A Charles Forsyth HPCC Workshop - Linux Basics and HPCC Cluster Introduction This workshop will cover topics including: * Connecting to the Cluster* Basic Linux commands* File systems* The cluster scheduler, Slurm* Cluster policies and best practices* Basic job submissions to the cluster* and others ..  Registration HPCC Linux/Cluster Basics:Slides\nFiles   December 14, 2018  2:00pm - 5:00pm Genomics Bldg Auditorium 1102A Charles Forsyth HPCC Workshop - Linux Basics and HPCC Cluster Introduction This workshop will cover topics including: * Connecting to the Cluster* Basic Linux commands* File systems* The cluster scheduler, Slurm* Cluster policies and best practices* Basic job submissions to the cluster* and others ..  Registration HPCC Linux/Cluster Basics:Slides\nFiles   November, 2, 2018  9:30pm - 4:30pm Morning: HUB-355; Afternoon: HUB-268/269 Amit Majumadar, Nicole Wolter, Mahidhar Tatineni, Robert Sinkovits, Paul Rodriguez, Mai Nguyen San Diego Supercomputer Center Workshop: Workshop presented by staff from the San Diego Super Computer Center. The morning will provide general information and resources, and the afternoon will split into two parallel tracks (Data Science and Compute Intensive Tasks). Registration  Agenda  Slides \u0026 Tutorials   October 26, 2018  9:00pm - 12:00pm Genomics Bldg Auditorium 1102A Charles Forsyth, Jordan Hayes \u0026 Thomas Girke Research Computing in UCR’s HPC Center: This event will provide an overview of the resources provided by UCR’s HPC Center along with a usage tutorial. This includes a seminar-style introduction to the center’s hardware and software resources, including usage statistics, training events, online tutorials and access options via subscription and/or ownership models. The introduction will be followed by a tutorial about the general usage of the infrastructure covering the following topics: (i) user accounts, (ii) big data storage and backups, (iii) software management, installs and containerization; (iv) queuing system; (v) web-based access options; and (vi) external alternatives including AWS and XSEDE. During the tutorial section participants are welcome to follow along on their laptops. Guest accounts will be provided to new users. Registration  Agenda  Intro Slides  Tutorial Slides   October 18, 2018  1:00pm - 3:30pm Genomics Bldg Auditorium 1102A Charles Forsyth HPCC Workshop - Introduction to Python This workshop will cover topics including: * What is Python* Logic Control* Functions and Classes* Simple Data/File Operations* Using Python in HPC Environments* Installing and Importing Python Modules* and others ..  Registration Python Intro:Slides\nFiles   October 4, 2018  1:00pm - 3:30pm Genomics Bldg Auditorium 1102A Charles Forsyth HPCC Workshop - Linux Basics and HPCC Cluster Introduction This workshop will cover topics including: * Connecting to the Cluster* Basic Linux commands* File systems* The cluster scheduler, Slurm* Cluster policies and best practices* Basic job submissions to the cluster* and others ..  Registration HPCC Linux/Cluster Basics:Slides\nFiles   September 6, 2018  2:00pm - 5:00pm Genomics Bldg Auditorium 1102A Charles Forsyth \u0026 Jordan Hayes HPCC Workshop - Basic Linux/Cluster Intro and Software InstallationPart One: Basic Linux and Cluster Introduction Part Two: Installing software on the HPCC Cluster (Python, R, Perl, Basic Compiled Applications). Registration HPCC Linux/Cluster Basics:Slides\nFilesSoftware Install:SlidesFiles   August 3, 2018  2:00pm - 5:00pm Genomics Bldg Auditorium 1102A Charles Forsyth \u0026 Jordan Hayes HPCC Cluster (Biocluster) Intro WorkshopThree part basic introdution to Linux and the HPCC Cluster along with a user interaction session. Registration Linux Basics:SlidesVideoHPCC Basics:Slides\nFilesVideo   June 15, 2018  1:00pm - 4:00pm Genomics Bldg Auditorium 1102A Charles Forsyth, Jordan Hayes \u0026 Thomas Girke Introduction to HPCC ClusterI. Linux Basics; II. HPCC Cluster; III. How to work in R on HPC systems? Linux Basics:Slides\nHPCC Basics:Slides\nFiles\nR on HPCC:Slides   May 4, 2018  2:00pm - 5:00pm Genomics Bldg Auditorium 1102A Charles Forsyth \u0026 Jordan Hayes HPCC Cluster (Biocluster) Intro WorkshopThree part basic introdution to Linux and the HPCC Cluster along with a user interaction session. Registration Linux Basics:SlidesVideoHPCC Basics:SlidesVideo   April 27, 2018  2:00pm - 3:30pm Genomics Bldg Auditorium 1102A Thomas Girke Annual PI and User Meeting of HPC Center Overview of services, hardware/software resources, usage stats, training and recharging. Followed by discussion of future directions. Slides  Agenda   April 5, 2018  1:00pm - 2:30pm Genomics Bldg Auditorium 1102A Charles Forsyth HPCC Cloud Service PresentationLearn about the HPCC Cloud Service (beta). We will demonstrate how to use HPCC and Amazon Web Service (AWS) to quickly create an on-demand HPC Cluster private to you. We also explain important topics such as billing and cost control, cluster operation, types of clusters (GPU, Highmem, etc.), data management and more. Registration Slides   April 6, 2018  2:00pm - 5:00pm Genomics Bldg Auditorium 1102A Charles Forsyth \u0026 Jordan Hayes HPCC Cluster (Biocluster) Intro WorkshopThree part basic introdution to Linux and the HPCC Cluster along with a user interaction session. Registration Basic LinuxHPCC Cluster Intro   March 9, 2018  2:00pm - 5:00pm Genomics Bldg Auditorium 1102A Charles Forsyth \u0026 Jordan Hayes HPCC Cluster (Biocluster) Intro WorkshopThree part basic introdution to Linux and the HPCC Cluster along with a user interaction session. Registration Basic LinuxHPCC Cluster Intro   February 9, 2018  2:00pm - 5:00pm Genomics Bldg Auditorium 1102A Charles Forsyth \u0026 Jordan Hayes HPCC Cluster (Biocluster) Intro WorkshopThree part basic introdution to Linux and the HPCC Cluster along with a user interaction session. Registration Basic LinuxHPCC Cluster Intro   December 14, 2017  10:00am - 12:00pm Genomics Bldg Auditorium 1102A Randy Ridgley - AWS Solutions Architect Introduction to AWS for Research Introduction to AWS: Overview of Amazon Web Services (AWS), key concepts and terminology for compute, storage, database and networking. AWS for Researchers: With AWS scientists can deploy and test software, analyze their data, and share their results with collaborators around the world. This section will include a demonstration how to build HPC clusters in AWS using cfnCluster and AWSBatch. Registration, Slides   November 3, 2017  2:00pm - 5:00pm Genomics Bldg Auditorium 1102A Charles Forsyth \u0026 Jordan Hayes HPCC Cluster (Biocluster) Intro WorkshopThree part basic introdution to Linux and the HPCC Cluster along with a user interaction session. Registration Basic LinuxHPCC Cluster Intro   October 11, 2017  2:00pm - 4:00pm ULB 104 Charles Forsyth Linux, Scripting and HPCC Cluster Intro WorkshopIntrodution to Linux concepts basic bash scripting and HPCC Cluster intro focused on job submission. Slides   September 1, 2017  2:00pm - 5:00pm Genomics Bldg Auditorium 1102A Charles Forsyth \u0026 Jordan Hayes HPCC Cluster (Biocluster) Intro WorkshopThree part basic introdution to Linux and the HPCC Cluster along with a user interaction session. Registration Basic LinuxHPCC Cluster Intro   July 24, 2017  9:00am - 11:00am Genomics Bldg Auditorium 1102A Charles Forsyth HPCC Cluster (Biocluster) Intro WorkshopBasic introdution to the HPCC Cluster and Services. Registration   Jan 20, 2017  12:00pm - 2:00pm Genomics Bldg 1208 Jordan Hayes Slurm and Basic Linux Q\u0026AJob submissions, account managment and basic Linux scripting. Examples    ","excerpt":"   Date \u0026 Time Location Instructors Title and Description Material …","ref":"/events/small/","title":"HPC and Big Data Processing Tutorials"},{"body":"Introduction The scope of this manual section is am an introduction on how to get started using the Amazon cloud AWS to quickly create an on-demand cluster private to you.\nLogin to your Main/Master AWS account  Click this Link    Click Sign In  Enter your two-factor authentication code  Enter Auth code  Click Sign In  Identity and Access Managemenet (IAM)  Naviagte to the IAM Page   Click on Users  Add User  This is the user managment page  Click Add user  Account creation wizard  Create a new account for creating clusters  Fill out the user name and the access type (Programmatic access should be all that’s needed for users of a lab). Click Next: Permissions.  Assign Permissions to the new account  Permissions are assigned to groups and users are organized into those groups.  If you do not have any existing groups create an admin group Choose what group you would like the new user to below too. You can create a new group here also. Click Next: Review  Account creation review  Review account choices before creation  Click Create user.  Account creation complete.  The account has been created and here are the credentials  Ensure you click on Download.csv! (this contains the Access key ID and Secret key for this account and is needed later) Save this file. Click on Close.  Naviagte to the AWS EC2 Console  Click this Link   Click Key Pairs  Key Pair management page  Click Create Key Pair  Name new Key Pair  Give it a name representing the new user. Click Create You will be prompted to save the new key file. Note where you save this file.  Send credentials and key file to the new user  Send an email to the new user with the credentials file and the key file attached. Any other private form of file transfer can also be used to distribute the files to the new user.  You now have all the information needed for a user to create their own cluster.\nNext step: Setup and create a new cluster.\n","excerpt":"Introduction The scope of this manual section is am an introduction on …","ref":"/manuals/ext_cloud/aws/account/","title":"Account Creation"},{"body":"Acknowledgement in Publications We appreciate that you have chosen our facility to support your research and would like to remind you to add the following statement to acknowledge the High-Performance Computing Center in your publications and presentations.\nComputations were performed using the computer clusters and data storage resources of the HPCC, which were funded by grants from NSF (MRI-1429826) and NIH (1S10OD016290-01A1).\nYour success is important to us and we would appreciate the URLs to any publications or presentations for which the data was acquired using our facilty.\nGrants  NSF MRI-1429826 NIH 1S10OD016290-01A1  ","excerpt":"Acknowledgement in Publications We appreciate that you have chosen our …","ref":"/about/facility/acknowledgement/","title":"Acknowledgement of Facility"},{"body":"HPCC Operation During COVID-19 Crisis  Since the research computing infrastructure of the HPCC is designed to be accessed remotely, we are currently not expecting any major downtimes or restrictions for users due to the campus access restrictions caused by the COVID-19 pandemic. However, even this may change depending on the extent of the restrictions our university may have to implement. A few exceptions that are currently in effect for the HPCC facility are the following:\n Our offices in 1208/1207 Genomics Building are closed. The HPCC staff will continue to work as usual but exclusively via remote access from home. All in person contacts such as office visits or in-person training events are not possible during the current COVID-19 crisis. Alternatively, we will be offering remote training sessions via video conferencing using Zoom.  If users need to get in contact with us then please email your questions to support@hpcc.ucr.edu. For immediate help, users can also post questions on Slack (ucr-hpcc.slack.com). For latest news and updates, please also visit us on Twitter. With some delay, updates will also be posted on this website’s News and Alerts pages.\nUnannounced exceptions 25-Jun-2021: Bigdata storage repaired  5:00 PM - Server running our bigdata storage have been recovered, and all functions of bigdata directory is now back to normal.  25-Jun-2021: Bigdata storage failed  3:30 PM - Server running our bigdata storage crashed, and bigdata directory went down with it.  12-Jan-2020: AC unit repaired  5:00 PM - AC repairs have been completed. The reservation has been removed, and new Slurm jobs are now no longer suspended.  11-Jan-2020: AC unit failed  3:00 PM - One of our AC units is under emergency repairs. A Slurm reservation was put in place to suspend new jobs from running.  Scheduled exceptions and downtimes None currently scheduled.\nStandard Operating Procedures SOP for unscheduled outages When unforeseen issues arise they are categorized by severity:\nGreen - Normal operation, no current issues Yellow - Minor issue[s], likely not observed by users (ie. jobs are not affected) Orange - Medium issue[s], likely observed by users but not fatal (ie. jobs may perform slower than usual) Red - Critical issue[s], major service or entire cluster is not functioning as expected (ie. jobs have terminated prematurely)  Email notifications are only sent to users if there is a Red critical issue.\nSOP for scheduled shutdowns The following outlines the timeline for advance email notifications on scheduled shutdowns of the HPCC cluster and other exceptions:\n Four weeks advance notice Followed by weekly reminders Final reminder the day before the outage  Twitter feed For additional news and information, please consult the HPCC Twitter site. Also see the Tweets window at the bottom of this and other pages of the HPCC website.\nTeam collaborations with Slack Sign up and use Slack Team Collaboration app here: ucr-hpcc.slack\nPast exceptions 28-Oct-2020: Cluster jobs failed due to storage suspension\n 3:00 PM - During a routine extension of the bigdata filesystem, there were some complications and disk i/o had to be suspended. 5:30 PM - We have repaired the issue, and everything should be functioning as usual. However, this means that all computing jobs running during timeframe were stopped and will need to be restarted.  19-Aug-2020: Cluster inaccessible due to power outage in Genomics Bdg\n 11:30 PM - All systems were restored by Jordan Hayes and are opterational again. 10:30 PM - HPC systems admin Jordan Hayes is trying to restart the network, storage and cluster again. 10:00 PM - Facilities was able to bring up the power and cooling again. 8:30 PM - Facilities is investigating and trying to reactivate power and cooling.  10-Aug-2020: Cluster inaccessible due to power outage in Genomics Bdg\nAt 5:10 PM: Facilities has restored power and cooling systems in the server room. HPC systems admin Jordan Hayes is restarting the cluster and storage systems. At 10:10 PM: All HPCC services were restored (computing cluster, storage systems, web services).\n22-Mar-2020: Cluster inaccessible due to campus-wide network outage\nDue to a campus-wide network outage at UCR, many HPCC services were not accessible between 8:00 AM and 1:00 PM. Currently, most HPCC services are accessible again. Note, running jobs on the cluster should not have been affected by this disruption. Updates about the current situations can be found here.\n13-Mar-2020: Routine maintenance shutdown\nWe have scheduled an HPCC Cluster Maintenance Shutdown for Friday, March 13th. This will require a shutdown of ALL services: Slurm (queuing system), hosted websites, virtual environments/machines, storage systems, backup systems, and network services. We ask that you please make sure that you do not have any jobs running in the queue, and that you completely logout of the cluster (pigeon, pelican, parrot) before the shutdown.\n08-Jan-2020: Storage outage\nWe had some issues with our storage systems this evening that may have caused disruptions in your work. These issues should be resolved. We’re continuing to monitor the situation to ensure everything is operational, and we apologize for any inconveniences this may have caused. Please let us know at support@hpcc.ucr.edu if you require any assistance regarding job status and recovery.\n21-Nov-2019: Routine filesystem maintenance and diagnostics\nWe have scheduled an HPCC Cluster Maintenance Shutdown for this Thursday, November 21st. This will require a shutdown of ALL services: Slurm (queuing system), hosted websites, virtual environments/machines, storage systems, backup systems, and network services. We ask that you please make sure that you do not have any jobs running in the queue, and that you completely logout of the cluster (pigeon, pelican, parrot) before the shutdown.\n23-Aug-2019: Routine maintenance shutdown\nWe have scheduled an HPCC Cluster Maintenance Shutdown for Friday, Aug 23, 2019. This will require a shutdown of ALL services: Slurm (queuing system), hosted websites, virtual environments/machines, storage systems, backup systems, and network services. We ask that you please make sure that you do not have any jobs running in the queue, and that you completely logout of the cluster (pigeon, pelican, globus) before the shutdown. Status: completed. For user changes related to this maintenance please see here.\n01-Mar-2019: Routine Maintenance Shutdown\nWe have scheduled an HPCC Cluster Maintenance Shutdown for Friday, March 1st. This will require a shutdown of ALL services: Slurm (queuing system), hosted websites, virtual environments/machines, storage systems, backup systems, and network services. We ask that you please make sure that you do not have any jobs running in the queue, and that you completely logout of the cluster (pigeon, pelican, globus) before the shutdown. Status: successfully completed.\n1:00 PM, 20-Dec-18: Outage due to AC failure\nAll systems were down for 3 hours due to a failure of the AC units in our server room. Electricians and AC technicians have repaired the units.\n2:30 PM, 11-Jul-18: Storage Issues\nFor the past several weeks we have been observing slower storage access. In some cases the /bigdata storage was inaccessible for several minutes and caused some jobs to terminate prematurely. We have identified the issue and have taken steps to ensure that this problem does not reoccur.\n6:00 PM, 02-Jul-18: Storage Issues\nStorage issues on the afternoon of July 2, 2018 caused disruptions in some cluster services. The issues should be resolved, but we’re continuing to monitor the situation for any other developments.\n12:00 AM, 31-Jan-18: routine maintenance shutdown\nFor routine maintenance and upgrades we have scheduled an HPCC (Biocluster) shutdown for 12:00AM, Jan-31-2018 to 12:00AM, Feb-01-2018. (complete)\n12:00 AM, 05-Dec-17: NFS \u0026 SMB issues\nNFS and SMB services have been suspended temporarily. This will cause many of our web services to not function properly. These include, but not limited to:\n https://rstudio.bioinfo.ucr.edu \u0026 https://rstudio2.bioinfo.ucr.edu https://galaxy.bioinfo.ucr.edu https://dashboard.bioinfo.ucr.edu https://biocluster.ucr.edu/~username (.html directories) mysql://bioclusterdb.int.bioinfo.ucr.edu (databases)  Note, this issue was resolved soon after it occurred.\n11:00 AM, 13-Aug-17: Cooling problem\nSince Sat morning one of the HVAC units is not working properly. To avoid overheating, we have shut down most of the idle nodes (1:30PM, Sun). As soon as the HVAC unit is repaired we will power these nodes back on. Note, this issue was resolved on 17-Aug-17. UCR facility services has repaired the broken HVAC unit and serviced the second one.\n12:00 AM, 16-Jun-17 to 17-Jun-17: maintenance shutdown\nTo sustain future growth, the power load in the HPCC server room needs to be optimized. For this we have scheduled an HPCC (Biocluster) shutdown in four weeks from now which will start at noon on June 16th and last until noon June 17th. This will require a shutdown of ALL services: Slurm (queuing system), hosted websites, virtual environments/machines, storage access, backup systems and network services. We ask that you please make sure that you do not have any jobs running in the queue, and that you completely logout of the cluster (pigeon, owl, penguin, pelican, globus) before the shutdown.\n10:02 AM, 13-Apr-17: UPS failure\nOur UPS unit went down some time last night causing a power failure on all systems. Jordan is bypassing the UPS to bring things back up in the next few hours. Nationwide Power will come in asap to repair the UPS. Note, this failure has not resulted in any overheating problems since the AC units are running on a different power cricuit.\n11:22 AM, 13-Apr-17: Cluster back up running\nSituation is resolved for now and things are working. We are currently discussing the situation with our electricians to avoid future instances.\n","excerpt":"HPCC Operation During COVID-19 Crisis  Since the research computing …","ref":"/news/alerts/","title":"User alerts for HPCC's computing resources"},{"body":"Basics Syntax and Notes   Remember the UNIX/Linux command line is case sensitive!\n  The hash (pound) sign # indicates end of a command and the start of a comment.\n  The notation \u003c...\u003e refers to variables and file names that need to be specified by the user. The symbols \u003c and \u003e need to be excluded.\n  No need to memorize all of these commands, by using these commands you will naturally memorize the most frequently used.\n  When specifying file names:\n The . (dot) refers to the present working directory The ~ (tilde) refers to user’s home directory    Commands Navigation and Exploration pwd # \"Print working directory\"; show your current path ls # \"List\" contents of current directory ls -l # Similar to ls, but provides additional info on files and directories ls -a # List all files, including hidden files (.name) as well ls -R # Lists subdirectories recursively ls -t # Lists files in chronological order cd \u003cdir_name\u003e # \"Change directory\" to specified path cd # Brings you to your home directory cd ~ # Also bring you to your home directory cd .. # Moves one directory up cd ../../ # Moves two directories up (and so on) cd - # Go back to you were previously (before the last directory change)  Informative file \u003cfile-name\u003e # Show type of file (text, binary, compressed, etc...) id # Shows your user name and associated groups hostname # Shows the name of the machine your shell is currently on  Files and Directories mkdir \u003cdir_name\u003e # Creates specified directory rmdir \u003cdir_name\u003e # Removes empty directory rm \u003cfile_name\u003e # Removes file_name rm -r \u003cdir_name\u003e # Removes directory including its contents, but asks for confirmation rm -rf \u003cdir_name\u003e # Same as above, but turns confirmation off. Use with caution cp \u003cname\u003e \u003cpath\u003e # Copy file/directory as specified in path (-r to include content in directories) mv \u003cname1\u003e \u003cname2\u003e # Renames directories or files mv \u003cname\u003e \u003cpath\u003e # Moves file/directory as specified in path  Copy and paste The methods to copy and paste on the command line differ depending on your operating systems (ie. Mac OSX, MS Windows, Linux) and your SSH application (ie. Terminal, MobaXTerm).\n Linux (xterm)  # Copy CTRL+SHIFT+C # Paste CTRL+SHIFT+V   MS Windows (MobaXTerm)  # Copy by highlighting with mouse # Paste SHIFT+INSERT   Mac OSX (Terminal)  # Copy COMMAND+c # Paste COMMAND+v  Shortcuts Command History  ↑ # Up arrow key scrolls backwards through command history ↓ # Down arrow key scrolls forwards through command history history # Shows all commands you have used recently  Auto-completion The tab (⇥) key auto completes commands or file names if there is only one option. Hitting the tab (⇥) key twice will list multiple options. Keep in mind that there are no spaces between the tab (⇥) keys and the partial names of commands or files.\nShow all directories under my home that I can cd into:\ncd ~/⇥⇥\nShow all files that I can ls with names that start with “myfile”:\nls myfile⇥⇥\nShow all commands that I can run with names that start with “sp”:\nsp⇥⇥\nCursor Ctrl+a # Cursor to beginning of command line Ctrl+e # Cursor to end of command line Ctrl+w # Cut last word Ctrl+k # Cut to the end of the line Ctrl+y # Paste (\"yank\") content that was cut earlier (by Ctrl-w or Ctrl-k)  Other Useful Unix Commands df -h /scratch # Show local disk space for /scratch, do not use for /rhome or /bigdata free -h # Show memory of current machine bc # Command-line calculator (to exit type 'quit') wget \u003cURL\u003e # Download a file or directory from the web ln -s \u003cFILENAME1\u003e \u003cFILENAME2\u003e # Creates symbolic link (shortcut, or alias) for file or directory du -sh . # Shows size of current directory du -sh \u003cFILENAME\u003e # Shows size of individual file du -s * | sort -nr # Shows size of each file within current directory, sorted by size  Help Not all command have help documentation available, however one of these methods will likely work:\nhelp \u003cCOMMAND\u003e # Show help for a Bash command man \u003cCOMMAND\u003e # Show the manual page for a program (press the 'q' key to exit) \u003cCOMMAND\u003e --help # Show help documentation for command \u003cCOMMAND\u003e -h # Show help documentation for command  Online help: Google is your friend.\nUniversally available Linux commands, with detailed examples and explanations: https://www.linuxconfig.org/linux-commands\n","excerpt":"Basics Syntax and Notes   Remember the UNIX/Linux command line is case …","ref":"/manuals/linux_basics/cmdline_basics/","title":"Command Line Basics"},{"body":"Storage  Four enterprise class HPC storage systems Approximately 3 PB production and 3 PB backup storage (total 6 PB or 6,144 TB) GPFS (NFS and SAMBA via GPFS) Automatic snapshots and archival backups  Network  Ethernet  5 x 1 Gb/s switch 5 x 1 Gb/s switch 10 Gig uplink 1 x 10 Gb/s switch for campus high performance research network Redundant, load balanced, robust mesh topology   Interconnect  56 Gb/s InfiniBand (FDR)    Head Nodes All users should access the cluster via SSH through cluster.hpcc.ucr.edu. This address will automatically balance traffic to one of the available head nodes.\n Pigeon  Resources: 16 cores, 128 GB memory Primary function: submitting jobs to the queuing system Secondary function: development; code editing and running small (under 50 % CPU and under 30 % RAM) sample jobs   Pelican  Resources: 32 cores, 64 GB memory Primary function: submitting jobs to the queuing system Secondary function: development; code editing and running small (under 50 % CPU and under 30 % RAM) sample jobs   Penguin  Resources: 8 cores, 64 GB memory Primary function: submitting jobs to the queuing system Secondary function: development; code editing and running small (under 50 % CPU and under 30 % RAM) sample jobs   Parrot  Resources: 32 cores, 32 GB memory Primary function: submitting jobs to the queuing system Secondary function: development; code editing and running small (under 50 % CPU and under 30 % RAM) sample jobs    Worker Nodes  Batch  c01-c48: each with 64 AMD cores and 512 GB memory   Highmem  h01-h06: each with 32 Intel cores and 1024 GB memory   GPU  gpu01-gpu02: each with 32 (HT) cores Intel Haswell CPUs and 2 x NVIDIA Tesla K80 GPUs (~10000 CUDA cores) and 128 GB memory gpu03-gpu04: each with 48 (HT) cores Intel Broadwell CPUs and 4 x NVIDIA Tesla K80 GPUs (~20000 CUDA cores) and 512 GB memory gpu05: with 64 (HT) cores Intel Broadwell CPUs and 2 x NVIDIA Tesla P100 GPUs (~5200 CUDA cores) and 256 GB memory   Intel  i01-i54: each with 32 Intel Broadwell cores and 256-512 GB memory    ","excerpt":"Storage  Four enterprise class HPC storage systems Approximately 3 PB …","ref":"/about/hardware/details/","title":"Hardware Details"},{"body":"HPC To be added soon.\nBig data processing To be added soon.\nCloud computing To be added soon.\n","excerpt":"HPC To be added soon.\nBig data processing To be added soon.\nCloud …","ref":"/events/large/","title":"Annual workshop on HPC and big data processing"},{"body":"Login from Mac, Linux, MobaXTerm The initial login brings users into the cluster head node (i.e. pigeon, pelican, parrot). From there, users can submit jobs via srun/sbatch to the compute nodes to perform intensive tests. Since all machines are mounting a centralized file system, users will always see the same home directory on all systems. Therefore, there is no need to copy files from one machine to another.\nOpen the terminal and type\nssh -X username@cluster.hpcc.ucr.edu  Login from Windows Please refer to the login instructions of our Linux Basics manual.\nChange Password  Login via SSH using the Terminal on Mac/Linux or MobaXTerm on Windows   Once you have logged in type the following command:  passwd   Enter the old password (the random characters that you were given as your initial password) Enter your new password  The password minimum requirements are:\n Total length at least 8 characters long Must have at least 3 of the following:  Lowercase character Uppercase character Number Punctuation character    Modules All software used on the HPC cluster is managed through a simple module system. You must explicitly load and unload each package as needed. More advanced users may want to load modules within their bashrc, bash_profile, or profile files.\nAvailable Modules To list all available software modules, execute the following:\nmodule avail  This should output something like:\n------------------------- /usr/local/Modules/versions -------------------------- 3.2.9 --------------------- /usr/local/Modules/3.2.9/modulefiles --------------------- BEDTools/2.15.0(default) modules PeakSeq/1.1(default) python/3.2.2 SOAP2/2.21(default) samtools/0.1.18(default) bowtie2/2.0.0-beta5(default) stajichlab cufflinks/1.3.0(default) subread/1.1.3(default) matrix2png/1.2.1(default) tophat/1.4.1(default) module-info  Using Modules To load a module, run:\nmodule load \u003csoftware name\u003e[/\u003cversion\u003e]  For example, to load R version 4.1.2, run:\nmodule load R/4.1.2  To load the default version of the tophat module, run:\nmodule load tophat  Show Loaded Modules To show what modules you have loaded at any time, you can run:\nmodule list  Depending on what modules you have loaded, it will produce something like this:\nCurrently Loaded Modulefiles: 1) vim/7.4.1952 3) slurm/16.05.4 5) R/3.3.0 7) less-highlight/1.0 9) python/3.6.0 2) tmux/2.2 4) openmpi/2.0.1-slurm-16.05.4 6) perl/5.20.2 8) iigb_utilities/1  Unloading Software Sometimes you want to no longer have a piece of software in path. To do this you unload the module by running:\nmodule unload \u003csoftware name\u003e  Databases Loading Databases NCBI, PFAM, and Uniprot, do not need to be downloaded by users. They are installed as modules on the cluster.\nmodule load db-ncbi module load db-pfam module load db-uniprot  Specific database release numbers can be identified by the version label on the module:\nmodule avail db-ncbi ----------------- /usr/local/Modules/3.2.9/modulefiles ----------------- db-ncbi/20140623(default)  Using Databases In order to use the loaded database users can simply provide the corresponding environment variable (NCBI_DB, UNIPROT_DB, PFAM_DB, etc…) for the proper path in their executables.\nThis is the old deprecated BLAST and it may not work in the near future, however if you require it:\nblastall -p blastp -i proteins.fasta -d $NCBI_DB/nr -o blastp.out  You can can also use this method if you require the old version of BLAST (old BLAST with legacy support):\nBLASTBIN=`which legacy_blast.pl | xargs dirname` legacy_blast.pl blastall -p blastp -i proteins.fasta -d $NCBI_DB/nr -o blast.out --path $BLASTBIN  This is the preferred/recommended method (BLAST+):\nblastp -query proteins.fasta -db $NCBI_DB/nr -out proteins_blastp.txt  Usually, we store the most recent release and 2-3 previous releases of each database. This way time consuming projects can use the same database version throughout their lifetime without always updating to the latest releases.\nAdditional Features There are additional features and operations that can be done with the module command. Please run the following to get more information:\nmodule help  Quotas CPU Currently, the maximum number of CPU cores a user can use simultaneously on the cluster is 256 CPU cores when the load on the cluster is \u003c30% and 128 CPU cores when the load is above 30%. If a user submits jobs for more than 256/128 CPU cores then the additional requests will be queued until resources within the user’s CPU quota become available. Upon request a user’s upper CPU quota can be extended temporarily, but only if sufficient CPU resources are available. To avoid monopolisation of the cluster by a small number of users, the high load CPU quota of 128 cores is dynamically readjusted by an algorithm that considers the number of CPU hours accumulated by each user over a period of 2 weeks along with the current overall CPU usage on the cluster. If the CPU hour average over the 2 week window exceeds an allowable amount then the default CPU quota will be reduced for such a heavy user to 64 CPU cores, and if it exceeds the allowable amount by two-fold it will be reduced to 32 CPU cores. Once the average usage of a heavy user drops again below those limits, the upper CPU limit will be raised accordingly. Note: when the overall CPU load on the cluster is below 70% then the dynamically readjusted CPU quotas are not applied. At those low load times every user has the same CPU quota: 256 CPU cores at \u003c30% load and 128 CPU cores at 30-70% load.\nData Storage A standard user account has a storage quota of 20GB. Much more storage space, in the range of many TBs, can be made available in a user account’s bigdata directory. The amount of storage space available in bigdata depends on a user group’s annual subscription. The pricing for extending the storage space in the bigdata directory is available here.\nMemory From the cluster head node users can submit jobs to the batch queue or the highmem queue. The nodes associated with the batch queue are mainly for CPU intensive tasks, while the nodes of the highmem queue are dedicated to memory intensive tasks. The batch nodes allow a 1GB RAM minimum limit on jobs and and the highmem nodes allow 100GB-1024GB RAM jobs.\nWhat’s Next? You should now know the following:\n Basic orginization of the cluster   How to login to the cluster How to use the Module system to gain access to the cluster software CPU, storage, and memory limitations (quotas and hardware limits)  Now you can start using the cluster.\nThe HPCC cluster uses the Slurm queuing system and thus the recommended way to run your jobs (scripts, pipelines, experiments, etc…) is to submit them to this queuing system by using sbatch. Please DO NOT RUN ANY computationally intensive tasks on any head node (i.e. pigeon, pelican, parrot). If this policy is violated, your process will either run very slow or be killed. The head nodes (login nodes) are a shared resource and should be accessible by all users. Negatively impacting performance would affect all users on the system and will not be tolerated.\n","excerpt":"Login from Mac, Linux, MobaXTerm The initial login brings users into …","ref":"/manuals/hpc_cluster/start/","title":"Getting Started"},{"body":"","excerpt":"","ref":"/manuals/hpc_cluster/","title":"HPC Cluster"},{"body":"The following packages are available on the cluster as modules.\nTo load the default version of a module, use: module load [package]\nTo load a specific version of a module, use: module load [package]/[version]\nTo view this list at the command line, use: module avail\n   Package Name Versions     AAFTF 0.3.0   abyss 2.3.4   alphafold 2.1.2   amptk 1.5   anaconda 2021.11-py39_0   angsd 0.937   ant 1.10.12   antismash 5.1.2   antismash 6.0.0   arcs 1.2.3   aspera 3.6.0   ASTRAL-Pro 1.1.3   ASTRAL 4.7.8   ASTRAL 4.7.12   ASTRAL 4.10.7   ASTRAL 4.10.12   ASTRAL 5.5.6   ASTRAL 5.6.1   ASTRAL 5.6.3   ASTRAL 5.7.1   ASTRAL 5.14.3   ASTRAL 5.15.4   augustus 3.3.3   augustus 3.4.0   autometa 1.0.3   autometa 2.0.2   bamtools 2.5.2   bat 0.18.3   BBMap 38.95   bcftools 1.15   bcl2fastq 2.20.0.422   beagle-lib 3.1.2   beast 1.10.4   beast 2.6.6   bedops 2.4.40   bedtools 2.29.1   bioperl 1.7   bioperl 1.7.8   blobtools 1.1.1   blobtools2 3.1.0   boost 1.78.0   bowtie 1.3.1   bowtie2 2.4.5   busco 5.3.0   busco 5.3.2   bwa 0.7.17   canu 2.2   cap3 2005.06.07.amd64   cap3 2005.06.07.intel   cap3 2015-02-10   cblaster 1.3.12   cd-hit 4.8.1   centos 7.9   cgal 0.9.6   clinker 1.3.12   clipkit 1.3.0   CodingQuarry 2.0   cp2k 7.1_oneapi-2022.1.2.146   cuda 10.2   cuda 11.4   cufflinks 2.2.1   cutadapt 3.7   db-cazy 3.0   db-cazy 4.0   db-cazy 5.0   db-cazy 9.0(default)   db-checkv 0.6   db-diamond 20161103   db-diamond 20190125   db-diamond 20210403(default)   db-kaiju 20170113-e   db-merops 110   db-merops 120(default)   db-merops 2017-02-02   db-ncbi 20140623   db-ncbi 20150112   db-ncbi 20160708   db-ncbi 20170707   db-ncbi 20171212   db-ncbi 20190121   db-ncbi 20190709   db-ncbi 20190805(default)   db-ncbi 20220131   db-ncbi v5.20190517   db-panther 10.0(default)   db-panther 13.1   db-pfam 27.0   db-pfam 28.0   db-pfam 30.0   db-pfam 31.0   db-pfam 32.0   db-pfam 33.0   db-pfam 33.1   db-pfam 34.0   db-pfam 35.0(default)   db-pfam 2013-03-14   db-pfam 2015-07-27   db-pfam 2016-09-15   db-pfam 2017-06-11   db-pfam 2018-10-18   db-pfam 2020-03-19   db-pfam 2020-05-05   db-pfam 2021-03-19   db-pfam 2021-11-25   db-priam mar15(default)   db-rfam 11.0   db-rfam 14.5   db-rfam 14.7   db-swissprot 2021_01   db-trinotate r20150720(default)   db-uniprot 2014_12   db-uniprot 2015_01   db-uniprot 2015_02   db-uniprot 2015_03   db-uniprot 2015_04   db-uniprot 2015_05   db-uniprot 2016_11   db-uniprot 2018_03(default)   deeptools 3.5.1   diamond 2.0.13   diamond 2.0.14   du-dust 0.7.5   eggnog-mapper 2.1.7   elpa 2020.11.001_oneapi-2022.1.2.146   emboss 6.6.0   espresso 7.0_oneapi-2022.1.2.146   ete3 3.1.2   exonerate 2.2.0   exonerate 2.4.0   extra 1   fasta 36.3.8h   fastani 1.33   fastme 2.1.6.3   fastqc 0.11.9   fasttree 2.1.11   fastx_toolkit 0.0.13   ffmpeg 5.0   fftw 2.1.5   Flye 2.9   freefem 4.10   funannotate 1.8   fzf 0.28.0   gadget2 2.0.7   gadgetviewer 1.1.2   gap 4.11.1   gatk 3   gatk 3.3-0   gatk 3.3.0   gatk 3.4-0   gatk 3.4-46   gatk 3.4.0   gatk 3.4.46   gatk 3.5   gatk 3.6   gatk 3.7   gatk 3.8   gatk 4   gatk 4.0.1.2   gatk 4.0.4.0   gatk 4.0.8.1   gatk 4.0.12.0   gatk 4.1.1.0   gatk 4.1.4.1   gatk 4.1.8.1   gatk 4.2.0.0   gatk 4.2.5.0   gaussian 16   gaussian 16_AVX2   gaussian 16_SSE4   gcc 9.2.1   genemarkESET 4.69_lic   genometools 1.6.2   gmap 2021-12-17   gocryptfs 2.2.1   golang 1.17.6   gtdbtk 2.0.0   hdf5 1.12.1   hdf5 1.12.1_oneapi-2022.1.2.146   hic-pro 3.1.0   hisat2 2.1.0   hisat2 2.2.0   hisat2 2.2.1   hmmer 1.8.5   hmmer 2   hmmer 2.3.2   hmmer 3   hmmer 3.3.2   hmmer 3.3.2-mpi   hpcc_user_utils current   htslib 1.14   htslib 1.15   hub 2.14.2   igv 2.12.3   infernal 1.1.4   interproscan 5.54-87.0   iprscan 5.54-87.0   IQ-TREE 1.6.12   IQ-TREE 2.1.1   IQ-TREE 2.1.1_mpi   IQ-TREE 2.1.3   IQ-TREE 2.2.0   iqtree 1.6.12   iqtree 2.1.1   iqtree 2.1.1_mpi   iqtree 2.1.3   iqtree 2.2.0   jags 4.3.0   java 17.0.2   jbrowse 1.16.8   jbrowse2 1.6.7   jellyfish 2.3.0   jemalloc 5.2.1   julia 1.7.2   jupyterlab 3.3.1   kallisto 0.46.1   kallisto 0.48.0   kent-tools 427   libdeflate 1.10   LINKS 1.8.4   longranger 2.2.2   macs2 2.2.6   macs2 2.2.7   macs3 3.0.0a7   mafft 7.490   mathematica 12.3.1   matlab R2021b   mcl 14-137   medaka 1.6   medaka 1.6-gpu   meme 5.4.1   miniconda3 py39_4.10.3   miniconda3 py39_4.12.0   minimap2 2.24   MMseqs2 13-45111   mmseqs2 13-45111   mosdepth 0.3.3   mothur 1.47.0   mpich 4.0.1   mpich 4.0.1_gcc-9.2.1   mummer 3.23   mummer 4.0.0   muscle 3.8.31   muscle 5.1   mutt 2.1.5   ncbi-asn_tools 2022-04-25   ncbi-blast 2.2.22+   ncbi-blast 2.2.25+   ncbi-blast 2.2.26   ncbi-blast 2.2.26+   ncbi-blast 2.2.29+   ncbi-blast 2.2.30+   ncbi-blast 2.2.31+   ncbi-blast 2.3.0+   ncbi-blast 2.4.0+   ncbi-blast 2.5.0+   ncbi-blast 2.6.0+   ncbi-blast 2.7.1+   ncbi-blast 2.8.0+   ncbi-blast 2.8.1+   ncbi-blast 2.9.0+   ncbi-blast 2.11.0+   ncbi-blast 2.12.0+   ncbi-blast 2.13.0+   ncbi-ngs-tools 2.11.2   ncbi-rmblast 2.11.0   ncbi_datasets 13.4.0   ncbi_edirect 16.8.20220329   NECAT 0.0.1   neovim 0.6.0   netcdf-c 4.8.1_oneapi-2022.1.2.146   netcdf-fortran 4.5.4_onaapi-2022.1.2.146   NextDenovo 2.5.0   NextPolish 1.4.0   oneapi 2022.1.2.146   openbabel 3.1.1   openmpi 4.1.2   openmpi 4.1.2-slurm-19.05.0   openmpi 4.1.2_slurm-21.08.5   orthofinder 2.5.4   pandoc 2.17.1.1   parallel-fastq-dump 0.6.7   parallel 20220122   PASA 2.5.2   pbzip2 1.1.13   phred-phrap-consed 0.990329   phykit 1.11   picard 2.26.11   pilon 1.24   plink 1.90b6.25   pmix 3.1.3rc2   polypolish 0.5.0   primer3 2.6.1   prodigal 2.6.3   prymetime 0.2   pyvcf 0.6.8   qiime2 2022.2   quast 5.1.0rc1   quickmerge 0.3   R 4.1.2(default)   R 4.1.3   ragtag 2.1.0   raxml-ng 1.1.0   raxml-ng 1.1.0-mpi   RAxML 8.2.12   raxml 8.2.12   RAxML_NG 1.1.0   RAxML_NG 1.1.0-mpi   rclone 1.58.0   REDUCE_suite 2.2   reduce_suite 2.2   RepeatMasker 4-1-2   RepeatModeler 2.0.3   rmate 1.5.10   rstudio-server 2022.02.0-443   ruby 2.7.5   ruby 3.1.1   salmon 1.7.0   samtools 1.14   seqtk 1.3   shovill 1.1.0   signalp 3.0   signalp 4.1c   signalp 5.0b   signalp 6.0g   singularity-ce 3.9.3   singularity 3.9.3   slurm 19.05.0   slurm 21.08.5   snpEff 4.1G   snpEff 4.1K   snpEff 4.3m   snpEff 4.3P   snpEff 4.3r   snpEff 4.3t   snpEff 5.0e   snpEff 5.1   spades 3.15.4   sparsehash 2.0.4   sratoolkit 3.0.0   stacks 2.60   star 2.7.10a   stringtie 2.2.1   subread 2.0.3   tbb 2020.3   tbb 2021.5.0   texlive 20210325   texlive 20220403   tmhmm 2.0c   tmux 3.3   tophat 2.1.1   transdecoder 5.5.0   trf 4.09   trim_galore 0.6.7   trimal 1.4.1   trimmomatic 0.39   trinity-rnaseq 2.13.2   trinity-rnaseq 2.13.2_sing   trnascan-se 2.0.9   unimap 0.1   usearch 8   usearch 8.0.1623_32bit   usearch 8.1.1861_32bit   usearch 9   usearch 9.0.2132_i86linux32   usearch 9.1.13_i86linux32   usearch 9.2.64_i86linux32   usearch 10   usearch 10.0.240_i86linux32   usearch 11   usearch 11.0.667_i86linux32   valgrind 3.18.1   vasp 5.4.1_oneapi-2022.1.2.146   vcftools 0.1.16-18   viennarna 2.4.17   viennarna 2.5.0   vsearch 2.21.1   wannier90 3.1.0_oneapi-2022.1.2.146   workspace scratch   xpdf 4.03   yq 4.23.1   zoem 21-341    ","excerpt":"The following packages are available on the cluster as modules.\nTo …","ref":"/about/software/modules/","title":"Software Modules"},{"body":"Learn how to establish an Amazon Web Services (AWS) account under the UC Agreement. A UC-wide agreement for Amazon Web Services (AWS) has been established. The agreement provides Data Egress Fee waiver for up to 15% of total monthly AWS fees.\nPlease follow the instructions below to ensure that an AWS account for UCR business is covered under this agreement and in compliance with UC policy and the law.\nUnderstand Appropriate Use Review the following to understand the applicable terms and conditions, and allowable data use for AWS:\n University of California AWS Enterprise Customer Agreement  Applies to all AWS accounts UC-wide, except those used for:  Commercial Web Hosting Media Streaming Massive Open Online Courses (MOOCs)   80%+ of data egress must be via an approved National Research and Education Network (NREN). This includes CENIC and Internet2, so normal UC usage meets this requirement.   Determine whether the data you are working with should be hosted in the cloud. HIPAA Business Associate Agreement (BAA)  In order to cover your locations AWS accounts under the terms of the UC AWS Enterprise Agreement (EA) and HIPAA Business Associate Agreement (BAA), follow the instructions found on the UCOP Website (PDF).    Connect AWS Account to UC Agreement and PO Send an email to UCs AWS account representative, Devinder Narula dsnarula@amazon.com to:\n Activate your new AWS account (if pertinent) Include your AWS account under the terms of the UC-wide AWS agreement Connect your AWS account to your Purchase Order number  To accomplish this, the email needs to include the following information:\n AWS 12-Digit Account Number Company Name = University of California, Riverside (UCR) Your Name  AWS Account Activation You will receive an email response from AWS confirming that your AWS account is now set up for invoicing under the UC AWS agreement, and providing final instructions to activate your account. Follow the instructions in the AWS activation email and your account will be active and ready for use.\n","excerpt":"Learn how to establish an Amazon Web Services (AWS) account under the …","ref":"/manuals/ext_cloud/aws/egress/","title":"Account Egress Waiver"},{"body":"Summary report The following activity report is generated by Grafana and refreshed on this page every 10 minutes. It summarizes CPU cluster activity on HPCC’s computing resources including Intel, Batch, Highmem and GPU partitions:\nClick image to enlarge!\n   ","excerpt":"Summary report The following activity report is generated by Grafana …","ref":"/about/facility/activity/","title":"Activity Report"},{"body":"","excerpt":"","ref":"/manuals/ext_cloud/","title":"Cloud/External"},{"body":"UCR Library  Software Carpentry Workshop - Working with Data  The Unix shell 7-24-20 Version Control with Git 7-31-20   Software Carpentry Workshop - (Python, git, and bash) 3-25-19, 3-26-19  Graduate Quantitative Methods Center - GradQuant  Drop-in Hours Tuesday - Thursday Experimental Design 4-4-19 Hypothesis Testing and Statistical Power 4-9-19 Data Management in Python 4-11-19 JAVA for Beginners 4-15-19 Introduction to General Linear Models 4-17-19 Computational Bayesian Inference 4-24-19 Stata Workshop with Dr. Chuck Huber 4-25-19 Web Scraping with R 4-30-19 Introduction to SAS 5-2-19 Data Visualization in Python 5-7-19 Machine Learning in R 5-8-19 A Primer on Neural Networks with Python 5-13-19 Writing Articles in LaTeX 5-28-19 Machine Learning in MATLAB 6-03-19  Big data analysis programming in genome biology  R/Bioconductor workshops including annual 5-day events Graduate courses using HPC  Data Analysis in Genome Biology: GEN242 Computational Analysis of High Throughput Biological Data: GEN220    Others  10th consecutive workshop on analyzing sequencing data, ANGUS 2019! July 1 - July 12, 2019  This intensive two week summer course introduces attendees with a strong biology background to the practice of analyzing big shotgun sequencing data sets (Illumina, PacBio, and Nanopore). We introduce students to computational thinking and large-scale data analysis on UNIX platforms, and cover genome and transcriptome analysis. We also cover computational topics including R scripting, software installation with bioconda, cloud computing, and building efficient and automated workflows. We use hands-on tutorials, live coding, group notes, and in-class exercises to support an effective learning experience.    Please send us URLs of events we should list here.\n","excerpt":"UCR Library  Software Carpentry Workshop - Working with Data  The Unix …","ref":"/events/related/","title":"Related HPC and big data analysis events"},{"body":"File Systems The file system in Linux is where you can save data, files, scripts, etc. There are different storage pools based on the path. In Linux you can provide any storage pool from any directory, not like MS Windows systems, where a drive letter is assigned to each storage pool (ie. “C:”,“D:\"). This means that by navigating through nested directories, you may find different capacity limits, depending on where you are.\nLocations Most unix system, including Linux, have a common directory hierarchy. The following is called the root level, since it is at the “top” like roots of a inverted tree:\n/ |-- bigdata |-- bin |-- boot |-- dev |-- etc |-- home |-- lib |-- lib64 |-- media |-- mnt |-- opt |-- proc |-- rhome |-- root |-- run |-- sbin |-- srv |-- sys |-- tmp |-- usr `-- var  The two most important directories are /rhome and /bigdata, since this is where your code and data will be stored. These two directories are IBM Spectrum Scale (GPFS) pools, so storage quotas apply. Your home directory lives directly under /rhome and your groups shared storage lives under /bigdata (if extra storage was purchased). These two “bigdata” directories /bigdata/groupname/username and /bigdata/groupname/shared are symlinked (alias/shortcut) to your home directory for convenience, as seen here:\n/ |-- bigdata |-- groupname (Quota based on purchase) |-- username \u003c-------------| |-- shared \u003c----------| | |-- bin | | |-- boot | | |-- dev | | |-- etc | | |-- home | | |-- lib | | |-- lib64 | | |-- media | | |-- mnt | | |-- opt | | |-- proc | | |-- rhome | | |-- username (20GB Quota) | | |-- shared ----------\u003e| | |-- bigdata --------------\u003e| |-- root |-- run |-- sbin |-- srv |-- sys |-- tmp |-- usr `-- var  For more information regarding these locations, and others, visit HPCC Cluster: Data Storage.\nCase sensitive All paths and commands are case sensitive, an uppercase letter is not the same as a lowercase letter.\nPath Types An absolute path is a full path from top to bottom, from the root to the leaf:\n/rhome/username/example_dir/example_file  A relative path is a partial path with the current working directory is the starting point:\nexample_dir/example_file  Commands Here are many common commands related to files and file systems (run man \u003ccommand\u003e for more information):\npwd # Print working directory ls # List files in directory touch # Make an empty file mkdir # Make a directory cd # Change to directory cp # Copy file[s] from a directory to a directory mv # Move file[s] from a directory to a directory rm # Remove a file rmdir # Remove an empty directory df # Check size of storage pool du # Check size of file or directory check_quota # Check quota for home and bigdata   Note: CTRL+c will cancel a running command\n File Transfers This section has moved to the Data Sharing page.\n","excerpt":"File Systems The file system in Linux is where you can save data, …","ref":"/manuals/linux_basics/filesystems/","title":"File Systems and Transfers"},{"body":"What is a Job? Submitting and managing jobs is at the heart of using the cluster. A ‘job’ refers to the script, pipeline or experiment that you run on the nodes in the cluster.\nPartitions In the past we used queues under the old Torque system, we now refer to these logically grouped nodes as partitions. There are several different partitions available for cluster users to send jobs to:\n intel  Default partition Nodes: i01-02,i17-i40 Cores: Intel, 256 per user RAM: 1 GB default Time (walltime): 168 hours (7 days) default   batch  Nodes: c01-c48 Cores: AMD, 256 per user RAM: 1 GB default Time (walltime): 168 hours (7 days) default   highmem  Nodes: h01-h06 Cores: Intel, 32 per user RAM: 100 GB min and 1000 GB max Time (walltime): 48 hours (2 days) default   gpu  Nodes: gpu01-gpu05 GPUs: 8 per group RAM: 1 GB default Time (walltime): 48 hours (2 days) default   short  Nodes: Mixed set of nodes from batch, intel, and group partitions Cores: AMD/Intel, 256 per user RAM: 1 GB default Time (walltime): 2 hours Maximum   Group Partition  This partition is unique to the group, if your lab has purchased nodes then you will have a priority partition with the same name as your group (ie. girkelab). In order to submit a job to different partitions add the optional ‘-p’ parameter with the name of the partition you want to use:    sbatch -p batch SBATCH_SCRIPT.sh sbatch -p highmem SBATCH_SCRIPT.sh sbatch -p gpu SBATCH_SCRIPT.sh sbatch -p intel SBATCH_SCRIPT.sh sbatch -p mygroup SBATCH_SCRIPT.sh  Slurm Slurm is now our default queuing system across all head nodes. SSH directly into the cluster and your connection will be automatically load balanced to a head node:\nssh -XY cluster.hpcc.ucr.edu  Resources and Limits To see your limits you can do the following:\nslurm_limits  Check total number of cores used by your group in the all partitions:\ngroup_cpus  However this does not tell you when your job will start, since it depends on the duration of each job. The best way to do this is with the “–start” flag on the squeue command:\nsqueue --start -u $USER  Submitting Jobs There are 2 basic ways to submit jobs; non-interactive, interactive. Slurm will automatically start within the directory where you submitted the job from, so keep that in mind when you use relative file paths. Non-interactive submission of a SBATCH script:\nsbatch SBATCH_SCRIPT.sh  Here is an example of an SBATCH script:\n#!/bin/bash -l #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=10 #SBATCH --mem=10G #SBATCH --time=1-00:15:00 # 1 day and 15 minutes #SBATCH --mail-user=useremail@address.com #SBATCH --mail-type=ALL #SBATCH --job-name=\"just_a_test\" #SBATCH -p intel # This is the default partition, you can use any of the following; intel, batch, highmem, gpu # Print current date date # Load samtools module load samtools # Concatenate BAMs samtools cat -h header.sam -o out.bam in1.bam in2.bam # Print name of node hostname  The above job will request 1 node, 10 cores (parallel threads), 10GB of memory, for 1 day and 15 minutes. An email will be sent to the user when the status of the job changes (Start, Failed, Completed). For more information regarding parallel/multi core jobs refer to Parallelization.\nInteractive submission:\nsrun --pty bash -l  If you do not specify a partition then the intel partition is used by default.\nHere is a more complete example:\nsrun --mem=1gb --cpus-per-task 1 --ntasks 1 --time 10:00:00 --pty bash -l  The above example enables X11 forwarding and requests, 1GB of memory, 1 cores, for 10 hours within an interactive session.\nMonitoring Jobs To check on your jobs states, run the following:\nsqueue -u $USER --start  To list all the details of a specific job, run the following:\nscontrol show job JOBID  To view past jobs and their details, run the following:\nsacct -u $USER -l  You can also adjust the start -S time and/or end -E time to view, using the YYYY-MM-DD format. For example, the following command uses start and end times:\nsacct -u $USER -S 2018-01-01 -E 2018-08-30 -l | less -S # Type 'q' to quit  Canceling Jobs In cancel/stop your job run the following:\nscancel \u003cJOBID\u003e  You can also cancel multiple jobs:\nscancel \u003cJOBID1\u003e \u003cJOBID2\u003e \u003cJOBID3\u003e  If you want to cancel/stop/kill ALL your jobs it is possible with the following:\n# Be very careful when running this, it will kill all your jobs. squeue --user $USER --noheader --format '%i' | xargs scancel  For more information please refer to Slurm scancel documentation.\nAdvanced Jobs There is a third way of submitting jobs by using steps. Single Step submission:\nsrun \u003ccommand\u003e  Under a single step job your command will hang until appropriate resources are found and when the step command is finished the results will be sent back on STDOUT. This may take some time depending on the job load of the cluster. Multi Step submission:\nsalloc -N 4 bash -l srun \u003ccommand\u003e ... srun \u003ccommand\u003e exit  Under a multi step job the salloc command will request resources and then your parent shell will be running on the head node. This means that all commands will be executed on the head node unless preceeded by the srun command. You will also need to exit this shell in order to terminate your job.\nHighmem Jobs The highmem partition does not have a default amount of memory set, however it does has a minimum limit of 100GB per job. This means that you need to explicity request at least 100GB or more of memory.\nNon-Interactive:\nsbatch -p highmem --mem=100g --time=24:00:00 SBATCH_SCRIPT.sh  Interactive\nsrun -p highmem --mem=100g --time=24:00:00 --pty bash -l  Of course you should adjust the time argument according to your job requirements.\nGPU Jobs GPU nodes have multiple GPUs, and very in type (K80 or P100). This means you need to request how many GPUs and of what type that you would like to use.\nTo request a gpu of any type, only indicate how many GPUs you would like to use.\nNon-Interactive:\nsbatch -p gpu --gres=gpu:1 --mem=100g --time=1:00:00 SBATCH_SCRIPT.sh  Interactive\nsrun -p gpu --gres=gpu:4 --mem=100g --time=1:00:00 --pty bash -l  Since the HPCC Cluster has two types of GPUs installed (K80s and P100s), GPUs can be requested explicitly by type.\nNon-Interactive:\nsbatch -p gpu --gres=gpu:k80:1 --mem=100g --time=1:00:00 SBATCH_SCRIPT.sh sbatch -p gpu --gres=gpu:p100:1 --mem=100g --time=1:00:00 SBATCH_SCRIPT.sh  Interactive\nsrun -p gpu --gres=gpu:k80:1 --mem=100g --time=1:00:00 --pty bash -l srun -p gpu --gres=gpu:p100:1 --mem=100g --time=1:00:00 --pty bash -l  Of course you should adjust the time argument according to your job requirements.\nOnce your job starts your code must reference the environment variable “CUDA_VISIBLE_DEVICES” which will indicate which GPUs have been assigned to your job. Most CUDA enabled software, like MegaHIT, will check this environment variable and automatically limit accordingly.\nFor example, when reserving 4 GPUs for a NAMD2 job:\necho $CUDA_VISIBLE_DEVICES 0,1,2,3 namd2 +idlepoll +devices $CUDA_VISIBLE_DEVICES MD1.namd  Each group is limited to a maximum of 8 GPUs on the gpu partition. Please be respectful of others and keep in mind that the GPU nodes are a limited shared resource. Since the CUDA libraries will only run with GPU hardward, development and compiling of code must be done within a job session on a GPU node.\nHere are a few more examples of jobs that utilize more complex features (ie. array, dependency, MPI etc): Slurm Examples\nWeb Browser Access Ports Some jobs require web browser access in order to utilize the software effectively. These kinds of jobs typically use (bind) ports in order to provide a graphical user interface (GUI) through a web browser. Users are able to run jobs that use (bind) ports on a compute node. Any port can be used on any compute node, as long as the port number is greater than 1000 and it is not already in use (bound).\nTunneling Once a job is running on a compute node and bound to a port, you may access this compute node via a web browser. This is accomplished by using 2 chained SSH tunnels to route traffic through our firewall. This acts much like 2 runners in a relay race, handing the baton to the next runer, to get past a security checkpoint.\nWe will create a tunnel that goes though a headnode and connect to a compute node on a particular port:\nssh -NL 8888:NodeName:8888 username@cluster.hpcc.ucr.edu  Port 8888 (first) is the local port you will be using on your laptop. NodeName is the compute node where where job is running, which can be found by using the squeue -u $USER command. Port 8888 (second) is the remote port on the compute node. Again, the NodeName and ports will be different depending on where your job runs and what port your job uses.\nAt this point you may need to provide a password to make the SSH tunnel. Once this has succeeded, the command will hang (this is normal). Leave this session connected, if you close it your tunnel will be closed.\nThen open a browser on your local computer (PC/laptop) and point it to:\nhttp://localhost:8888  If your job uses TSL/SSL, so you may need to try https if the above does not work:\nhttps://localhost:8888  Examples   A perfect example of this method is used for Jupyter Lab/Notebook. For more details please refer to the following Jupyter Example.\n  RStudio Server instances can also be started directly on a compute node and accessed via an SSH tunnel. For details see here.\n  Desktop Environments VNC Server (cluster) Start VNC Server\nLog into the cluster:\nssh username@cluster.hpcc.ucr.edu  The first time you run the vncserver it will need to be configured:\nvncserver -fg  You should set a password for yourself, and the read-only password is optional.\nThen configure X Startup with the following command:\necho '/usr/bin/ssh-agent /usr/bin/dbus-launch --exit-with-session /usr/bin/gnome-session --session=gnome-classic' \u003e /rhome/$USER/.vnc/xstartup  After your vncserver is configured, submit a vncserver job to get it started:\nsbatch -p short,batch --cpus-per-task=4 --mem=10g --time=2:00:00 --wrap='vncserver -fg' --output='vncserver-%j.out'   Note: Appropriate job resources should be requested based on the processes you will be running from within the VNC session.\n Check the contents of your job log to determine the NodeName and Port you were assigned:\ncat vncserver-*.out  The contents of your slurm job log should be similar to the following:\nvncserver New 'i54:1' desktop is i54:1 Creating default startup script /rhome/username/.vnc/xstartup Starting applications specified in /rhome/username/.vnc/xstartup Log file is /rhome/username/.vnc/i54:1.log  The VNC Port used should be 5900+N, N being the display number mentioned above in the format NodeName:DisplayNumber (ie. i54:1). In this example (default), the port is 5901, if this Port were already in use then the vncserver will automatically increment the DisplayNumber and you might find something like i54:2 or i54:3 and so on.\nStop VNC Server\nTo stop the vncserver, you can click on the logout option from the upper right hand menu from within your VNC desktop environment. If you want to kill your vncserver manually, then you will need to do the following:\nssh NodeName 'vncserver -kill :DisplayNumber'  You will need to replace NodeName with the node name of your where your job is running, and the DisplayNumber with the DisplayNumber from your slurm job log.\nVNC Client (Desktop/Laptop) After you know the NodeName and VNC Port you should be able to create an SSH tunnel to your vncserver, like so:\nssh -N -L Port:NodeName:Port cluster.hpcc.ucr.edu  Now let us create an SSH tunnel on your local machine (desktop/laptop) using the NodeName and VNC Port from above:\nssh -L 5901:i54:5901 cluster.hpcc.ucr.edu  After you have logged into the cluster with this shell, log into the node where your VNC server is running:\nssh NodeName  After you have logged into the correct NodeName, just let this terminal sit here, do not close it.\nThen launch vncviewer on your local system (laptop/workstation), like so:\nvncviewer localhost:5901  After launching the vncviewer, and providing your VNC password (not your cluster password), you should be able to see a Linux desktop environment.\nFor more information regarding tunnels and VNC in MS Windows, please refer More VNC Info.\nLicenses The cluster currently supports Commercial Software. Since most of the licenses are campus wide there is no need to track individual jobs. One exception is the Intel Parallel Suite, which contains the Intel compilers.\nThe --licenses flag is used to request a license for Intel compilers, for example:\nsrun --license=intel:1 -p short --mem=10g --cpus-per-task=10 --time=2:00:00 --pty bash -l module load intel icc -help  The above interactive submission will request 1 Intel license, 10GB of RAM, 10 CPU cores for 2 hours on the short partition. The short parititon can only be used for a maximum of 2 hours, however for compilation this could be sufficient. It is recommended that you separate your compilation job from your computation/analysis job. This way you will only have the license checked out for the duration of compilation, and not the during the execution of the analysis.\nParallelization There are 3 major ways to parallelize work on the cluster:\n Batch Thread MPI  Parallel Methods For batch jobs, all that is required is that you have a way to split up the data and submit multiple jobs running with the different chunks. Some data sets, for example a FASTA file is very easy to split up (ie. fasta-splitter). This can also be more easily achieved by submitting an array job. For more details please refer to Advanced Jobs.\nFor threaded jobs, your software must have an option referring to “number of threads” or “number of processors”. Once the thread/processor option is identified in the software, (ie. blastn flag -num_threads 4) you can use that as long as you also request the same number of CPU cores (ie. slurm flag --cpus-per-task=4).\nFor MPI jobs, your software must be MPI enabled. This generally means that it was compiled with MPI libraries. Please refer to the user manual of the software you wish to use as well as our documentation regarding MPI. It is important that the number of cores used is equal to the number requested.\nIn Slurm you will need 2 different flags to request cores, which may seem similar, however they have different purposes:\n The --cpus-per-task=N will provide N number of virtual cores with locality as a factor. Closer virtual cores can be faster, assuming there is a need for rapid communication between threads. Generally, this is good for threading, however not so good for independent subprocesses nor for MPI. The --ntasks=N flag will provide N number of physical cores on a single or even multiple nodes. These cores can be further away, since the need for physical CPUs and dedicated memory is more important. Generally this is good for independent subprocesses, and MPI, however not so good for threading.  Here is a table to better explain when to use these Slurm options:\n   Slurm Flag Single Threaded Multi Threaded (OpenMP) MPI only MPI + Multi Threaded (hybrid)     --cpus-per-task  X  X   --ntasks   X X    As you can see:\n A single threaded job would use neither Slurm option, since Slurm already assumes at least a single core. A multi threaded OpenMP job would use --cpus-per-task. A MPI job would use --ntasks. A Hybrid job would use both.  For more details on how these Slurm options work please review Slurm Multi-core/Multi-thread Support.\nMPI MPI stands for the Message Passing Interface. MPI is a standardized API typically used for parallel and/or distributed computing. The HPCC cluster has a custom compiled versions of MPI that allows users to run MPI jobs across multiple nodes. These types of jobs have the ability to take advantage of hundreds of CPU cores symultaniously, thus improving compute time.\nMany implementations of MPI exists, however we only support the following:\n Open MPI MPICH IMPI  For general information on MPI under Slurm look here. If you need to compile an MPI application then please email support@hpcc.ucr.edu for assistance.\nWhen submitting MPI jobs it is best to ensure that the nodes are identical, since MPI is sensitive to differences in CPU and/or memory speeds. The batch and intel partitions are designed to be homogeneous, however, the short partition is a mixed set of nodes. When using the short partition for MPI append the constraint flag for Slurm.\nShort Example\nHere is an example that shows how to ensure that your job will only run on intel nodes from the short partition:\nsbatch -p short --constraint=intel myJobScript.sh  NAMD Example\nTo run a NAMD2 process as an OpenMPI job on the cluster:\n  Log-in to the cluster\n  Create SBATCH script\n#!/bin/bash -l #SBATCH -J c3d_cr2_md #SBATCH -p batch #SBATCH --ntasks=32 #SBATCH --mem=16gb #SBATCH --time=01:00:00 # Load needed modules # You could also load frequently used modules from within your ~/.bashrc module load slurm # Should already be loaded module load openmpi # Should already be loaded module load namd # Run job utilizing all requested processors # Please visit the namd site for usage details: http://www.ks.uiuc.edu/Research/namd/ mpirun --mca btl ^tcp namd2 run.conf \u0026\u003e run_namd.log    Submit SBATCH script to Slurm queuing system\nsbatch run_namd.sh    Maker Example\nOpenMPI does not function properly with Maker, you must use MPICH. Our version of MPICH does not use the mpirun/mpiexec wrappers, instead use srun:\n#!/bin/bash -l #SBATCH -p intel #SBATCH --ntasks=32 #SBATCH --mem=16gb #SBATCH --time=01:00:00 # Load maker module load maker/2.31.11 mpirun maker # Provide appropriate maker options here  More examples The range of differing jobs and how to submit them is endless:\n1. Singularity containers 2. Database services 3. Graphical user interfaces 4. Etc ...  For a growing list of examples please visit HPCC Slurm Examples.\n","excerpt":"What is a Job? Submitting and managing jobs is at the heart of using …","ref":"/manuals/hpc_cluster/jobs/","title":"Managing Jobs"},{"body":"The following packages are provided as part of the CentOS 7 operating system. They can be used without loading any modules.\n   Package Name Version     CharLS 1.0   GConf2 3.2.6   ImageMagick 6.7.8.9   ImageMagick-devel 6.7.8.9   ImageMagick-perl 6.7.8.9   NetworkManager 1.0.0   NetworkManager-glib 1.0.0   NetworkManager-libnm 1.0.0   NetworkManager-team 1.0.0   NetworkManager-tui 1.0.0   ORBit2 2.14.19   OpenEXR-libs 1.7.1   OpenIPMI 2.0.19   OpenIPMI-libs 2.0.19   OpenIPMI-modalias 2.0.19   PyQt4 4.10.1   PyQt4-devel 4.10.1   SuperLU 4.3   a2ps 4.14   abattis-cantarell-fonts 0.0.12   acl 2.2.51   adwaita-cursor-theme 3.8.4   adwaita-gtk2-theme 3.8.4   adwaita-gtk3-theme 3.8.4   aic94xx-firmware 30   alsa-firmware 1.0.28   alsa-lib 1.0.28   alsa-tools-firmware 1.0.27   apr 1.4.8   apr-util 1.5.2   ar_mgr 1.0   armadillo 4.320.0   arpack 3.1.3   aspell 0.60.6.1   at-spi2-atk 2.8.1   at-spi2-atk-devel 2.8.1   at-spi2-core 2.8.0   atk 2.8.0   atk-devel 2.8.0   atlas 3.10.1   atlas-devel 3.10.1   attica 0.4.1   attr 2.4.46   audit 2.4.1   audit-libs 2.4.1   augeas 1.1.0   augeas-libs 1.1.0   authconfig 6.2.8   autoconf 2.69   autogen 5.18   autogen-libopts 5.18   automake 1.13.4   avahi-autoipd 0.6.31   avahi-compat-libdns_sd 0.6.31   avahi-glib 0.6.31   avahi-libs 0.6.31   basesystem 10.0   bash 4.2.46   bc 1.06.95   bibutils-libs 5.0   bind-libs 9.9.4   bind-libs-lite 9.9.4   bind-license 9.9.4   bind-utils 9.9.4   binutils 2.23.52.0.1   binutils-c6x-linux-gnu 2.23.88.0.1   biosdevname 0.6.1   bison 2.7   blas 3.4.2   blas-devel 3.4.2   boost 1.53.0   boost-atomic 1.53.0   boost-chrono 1.53.0   boost-context 1.53.0   boost-date-time 1.53.0   boost-devel 1.53.0   boost-filesystem 1.53.0   boost-graph 1.53.0   boost-iostreams 1.53.0   boost-locale 1.53.0   boost-math 1.53.0   boost-program-options 1.53.0   boost-python 1.53.0   boost-random 1.53.0   boost-regex 1.53.0   boost-serialization 1.53.0   boost-signals 1.53.0   boost-static 1.53.0   boost-system 1.53.0   boost-test 1.53.0   boost-thread 1.53.0   boost-timer 1.53.0   boost-wave 1.53.0   btrfs-progs 3.16.2   bupc 2.18.0   bwidget 1.9.0   byacc 1.9.20130304   bzip2 1.0.6   bzip2-devel 1.0.6   bzip2-libs 1.0.6   ca-certificates 2015.2.4   cairo 1.12.14   cairo-devel 1.12.14   cairo-gobject 1.12.14   cairo-gobject-devel 1.12.14   cairomm 1.10.0   cc_mgr 1.0   cdparanoia-libs 10.2   centos-logos 70.0.6   centos-release 7   cfitsio 3.370   check 0.9.9   check-devel 0.9.9   chkconfig 1.3.61   clucene-core 2.3.3.4   cmake 2.8.11   colord-libs 1.0.4   compat-db-headers 4.7.25   compat-db47 4.7.25   compat-libf2c-34 3.4.6   compat-libstdc++-33 3.2.3   coreutils 8.22   cpio 2.11   cpp 4.8.3   cracklib 2.9.0   cracklib-dicts 2.9.0   cronie 1.4.11   cronie-anacron 1.4.11   crontabs 1.11   cross-binutils-common 2.23.88.0.1   cross-gcc-common 4.8.1   cryptsetup 1.6.6   cryptsetup-libs 1.6.6   csdiff 1.3.0   ctags 5.8   cups 1.6.3   cups-client 1.6.3   cups-filesystem 1.6.3   cups-filters 1.0.35   cups-filters-libs 1.0.35   cups-libs 1.6.3   cups-lpd 1.6.3   curl 7.29.0   cyrus-sasl 2.1.26   cyrus-sasl-devel 2.1.26   cyrus-sasl-lib 2.1.26   cyrus-sasl-plain 2.1.26   dapl 2.1.3mlnx   dapl-devel 2.1.3mlnx   dapl-devel-static 2.1.3mlnx   dapl-utils 2.1.3mlnx   dbus 1.6.12   dbus-devel 1.6.12   dbus-glib 0.100   dbus-glib-devel 0.100   dbus-libs 1.6.12   dbus-python 1.1.1   dbus-x11 1.6.12   dbusmenu-qt 0.9.2   dejavu-fonts-common 2.33   dejavu-sans-fonts 2.33   dejavu-sans-mono-fonts 2.33   dejavu-serif-fonts 2.33   desktop-file-utils 0.21   device-mapper 1.02.93   device-mapper-event 1.02.93   device-mapper-event-libs 1.02.93   device-mapper-libs 1.02.93   device-mapper-persistent-data 0.4.1   dhclient 4.2.5   dhcp-common 4.2.5   dhcp-libs 4.2.5   diffutils 3.3   dmidecode 2.12   dnsmasq 2.66   docbook-dtds 1.0   docbook-style-xsl 1.78.1   dos2unix 6.0.3   dosfstools 3.0.20   doxygen 1.8.5   dracut 033   dracut-config-rescue 033   dracut-network 033   dump_pr 1.0   dwz 0.11   e2fsprogs 1.42.9   e2fsprogs-libs 1.42.9   ebtables 2.0.10   ed 1.9   elfutils 0.160   elfutils-libelf 0.160   elfutils-libs 0.160   elinks 0.12   emacs 24.3   emacs-common 24.3   emacs-filesystem 24.3   emacs-gnuplot 4.6.2   enchant 1.6.0   environment-modules 3.2.10   epel-release 7   ethtool 3.15   evince 3.8.3   evince-libs 3.8.3   exempi 2.2.0   exiv2 0.23   exiv2-libs 0.23   expat 2.1.0   expat-devel 2.1.0   fca 2.5.2409   fftw 3.3.3   fftw-devel 3.3.3   fftw-doc 3.3.3   fftw-libs 3.3.3   fftw-libs-double 3.3.3   fftw-libs-long 3.3.3   fftw-libs-single 3.3.3   fftw-static 3.3.3   fftw2 2.1.5   fftw2-devel 2.1.5   file 5.11   file-devel 5.11   file-libs 5.11   filesystem 3.2   findutils 4.5.11   finger 0.17   fipscheck 1.4.1   fipscheck-lib 1.4.1   firewalld 0.3.9   flac-libs 1.3.0   flex 2.5.37   flite 1.3   fltk 1.3.0   fltk-devel 1.3.0   fltk-fluid 1.3.0   fontconfig 2.10.95   fontconfig-devel 2.10.95   fontpackages-filesystem 1.44   fping 3.10   freeglut 2.8.1   freeglut-devel 2.8.1   freetype 2.4.11   freetype-devel 2.4.11   freexl 1.0.0f   fuse 2.9.2   fuse-devel 2.9.2   fuse-libs 2.9.2   fxload 2002_04_11   gamin 0.1.10   ganglia 3.7.2   ganglia-gmond 3.7.2   gawk 4.0.2   gc 7.2d   gcc 4.8.3   gcc-c++ 4.8.3   gcc-c6x-linux-gnu 4.8.1   gcc-gfortran 4.8.3   gd 2.0.35   gd-devel 2.0.35   gdal 1.11.2   gdal-libs 1.11.2   gdb 7.6.1   gdbm 1.10   gdbm-devel 1.10   gdisk 0.8.6   gdk-pixbuf2 2.28.2   gdk-pixbuf2-devel 2.28.2   geos 3.4.2   geronimo-jms 1.1.1   gettext 0.18.2.1   gettext-libs 0.18.2.1   ghc-HTTP 4000.2.8   ghc-aeson 0.6.2.1   ghc-aeson-pretty 0.7.1   ghc-array 0.4.0.1   ghc-attoparsec 0.10.4.0   ghc-base 4.6.0.1   ghc-base-unicode-symbols 0.2.2.4   ghc-base64-bytestring 1.0.0.1   ghc-binary 0.5.1.1   ghc-blaze-builder 0.3.1.1   ghc-blaze-html 0.6.1.1   ghc-blaze-markup 0.5.1.5   ghc-bytestring 0.10.0.2   ghc-conduit 1.0.3   ghc-containers 0.5.0.0   ghc-data-default 0.5.1   ghc-deepseq 1.3.0.1   ghc-digest 0.0.1.2   ghc-directory 1.2.0.1   ghc-dlist 0.5   ghc-extensible-exceptions 0.1.1.4   ghc-filepath 1.3.0.1   ghc-hashable 1.1.2.5   ghc-highlighting-kate 0.5.6   ghc-hs-bibutils 5.0   ghc-hslua 0.3.10   ghc-lifted-base 0.2.1.0   ghc-monad-control 0.3.2.1   ghc-mtl 2.1.2   ghc-network 2.4.1.2   ghc-old-locale 1.0.0.5   ghc-old-time 1.1.0.1   ghc-pandoc 1.12.3.1   ghc-pandoc-citeproc 0.3.0.1   ghc-pandoc-types 1.12.3.1   ghc-parsec 3.1.3   ghc-pcre-light 0.4   ghc-pretty 1.1.1.0   ghc-primitive 0.5.0.1   ghc-process 1.1.0.2   ghc-random 1.0.1.1   ghc-resourcet 0.4.6   ghc-rfc5051 0.1.0.3   ghc-semigroups 0.8.5   ghc-split 0.2.2   ghc-syb 0.4.0   ghc-tagsoup 0.12.8   ghc-template-haskell 2.8.0.0   ghc-temporary 1.1.2.4   ghc-texmath 0.6.6   ghc-text 0.11.3.1   ghc-time 1.4.0.1   ghc-transformers 0.3.0.0   ghc-transformers-base 0.4.1   ghc-unix 2.6.0.1   ghc-unordered-containers 0.2.3.0   ghc-utf8-string 0.3.7   ghc-vector 0.10.0.1   ghc-void 0.5.11   ghc-xml 1.3.13   ghc-yaml 0.8.5.3   ghc-zip-archive 0.1.3.4   ghc-zlib 0.5.4.1   ghostscript 9.07   ghostscript-cups 9.07   ghostscript-devel 9.07   ghostscript-fonts 5.50   giflib 4.1.6   giflib-devel 4.1.6   git 1.8.3.1   git-svn 1.8.3.1   gl-manpages 1.1   gl2ps 1.3.8   glew 1.9.0   glew-devel 1.9.0   glib 1.2.10   glib-devel 1.2.10   glib-networking 2.40.0   glib2 2.40.0   glib2-devel 2.40.0   glibc 2.17   glibc 2.17   glibc-common 2.17   glibc-devel 2.17   glibc-headers 2.17   glibc-static 2.17   glibmm24 2.36.2   globus-authz 3.12   globus-authz-callout-error 3.5   globus-callout 3.14   globus-common 16.4   globus-common-progs 16.4   globus-connect-server 4.0.36   globus-connect-server-common 4.0.36   globus-connect-server-id 4.0.36   globus-connect-server-io 4.0.36   globus-connect-server-web 4.0.36   globus-ftp-client 8.29   globus-ftp-control 6.10   globus-gass-copy 9.19   globus-gass-copy-progs 9.19   globus-gass-transfer 8.8   globus-gfork 4.8   globus-gridftp-server 10.4   globus-gridftp-server-control 4.1   globus-gridftp-server-progs 10.4   globus-gridmap-callout-error 2.4   globus-gridmap-eppn-callout 1.11   globus-gridmap-verify-myproxy-callout 2.7   globus-gsi-callback 5.8   globus-gsi-cert-utils 9.12   globus-gsi-cert-utils-progs 9.12   globus-gsi-credential 7.9   globus-gsi-openssl-error 3.5   globus-gsi-proxy-core 7.9   globus-gsi-proxy-ssl 5.8   globus-gsi-sysconfig 6.9   globus-gss-assist 10.15   globus-gss-assist-progs 10.15   globus-gssapi-error 5.4   globus-gssapi-gsi 12.1   globus-io 11.5   globus-openssl-module 4.6   globus-proxy-utils 6.15   globus-simple-ca 4.22   globus-toolkit-repo 6   globus-usage 4.4   globus-xio 5.12   globus-xio-gsi-driver 3.10   globus-xio-pipe-driver 3.8   globus-xio-popen-driver 3.5   globus-xio-udt-driver 1.23   gmp 6.0.0   gmp-devel 6.0.0   gnome-common 3.7.4   gnome-desktop3 3.8.4   gnome-icon-theme 3.8.2   gnome-icon-theme-legacy 3.8.2   gnome-themes-standard 3.8.4   gnome-vfs2 2.24.4   gnupg2 2.0.22   gnuplot 4.6.2   gnuplot-common 4.6.2   gnutls 3.3.8   gnutls-c++ 3.3.8   gnutls-dane 3.3.8   gnutls-devel 3.3.8   gobject-introspection 1.36.0   gpfs.base 4.1.1   gpfs.docs 4.1.1   gpfs.ext 4.1.1   gpfs.gpl 4.1.1   gpfs.gplbin.c-3.10.0-229.1.2.el7.x86_64 4.1.1   gpfs.gskit 8.0.50   gpfs.hadoop-2-connector 4.1.1   gpg-pubkey 352c64e5   gpg-pubkey f4a80eb5   gpg-pubkey faf24365   gpgme 1.3.2   gpm-libs 1.20.7   gpsbabel 1.5.0   graphite2 1.2.2   graphviz 2.30.1   grep 2.20   groff 1.22.2   groff-base 1.22.2   groff-perl 1.22.2   grub2 2.02   grub2-tools 2.02   grubby 8.28   gsettings-desktop-schemas 3.8.2   gsi-openssh 7.1p2f   gsi-openssh-clients 7.1p2f   gsl 1.15   gsl-devel 1.15   gsm 1.0.13   gssdp 0.14.3   gssproxy 0.3.0   gstreamer 0.10.36   gstreamer-devel 0.10.36   gstreamer-plugins-base 0.10.36   gstreamer-plugins-good 0.10.31   gstreamer-tools 0.10.36   gstreamer1 1.0.7   gstreamer1-plugins-base 1.0.7   gtk2 2.24.22   gtk3 3.8.8   gtk3-devel 3.8.8   guile 2.0.9   gupnp 0.20.3   gupnp-igd 0.2.2   gutenprint 5.2.9   gvfs 1.16.4   gzip 1.5   hardlink 1.0   harfbuzz 0.9.20   harfbuzz-devel 0.9.20   harfbuzz-icu 0.9.20   hcoll 3.1.690   hdf5 1.8.12   hdparm 9.43   hicolor-icon-theme 0.12   hostname 3.13   hpijs 3.13.7   hplip-common 3.13.7   hplip-libs 3.13.7   html2ps 1.0   httpd 2.4.6   httpd-tools 2.4.6   hunspell 1.3.2   hunspell-en 0.20121024   hunspell-en-GB 0.20121024   hunspell-en-US 0.20121024   hwdata 0.252   ibacm 1.0.8mlnx5   ibdump 2.0.0   ibsim 0.5   ibutils 1.5.7.1   ibutils2 2.1.1   icoutils 0.31.0   ilmbase 1.0.3   imlib2 1.4.5   infiniband-diags 1.6.4.MLNX20140817.10ee11a   infiniband-diags-compat 1.6.4.MLNX20140817.10ee11a   info 5.1   initscripts 9.49.24   ipmitool 1.8.13   iproute 3.10.0   iprutils 2.4.3   iptables 1.4.21   iputils 20121221   irqbalance 1.0.7   iso-codes 3.46   ivtv-firmware 20080701   iwl100-firmware 39.31.5.1   iwl1000-firmware 39.31.5.1   iwl105-firmware 18.168.6.1   iwl135-firmware 18.168.6.1   iwl2000-firmware 18.168.6.1   iwl2030-firmware 18.168.6.1   iwl3160-firmware 22.0.7.0   iwl3945-firmware 15.32.2.9   iwl4965-firmware 228.61.2.24   iwl5000-firmware 8.83.5.1_1   iwl5150-firmware 8.24.2.2   iwl6000-firmware 9.221.4.1   iwl6000g2a-firmware 17.168.5.3   iwl6000g2b-firmware 17.168.5.2   iwl6050-firmware 41.28.5.1   iwl7260-firmware 22.0.7.0   jack-audio-connection-kit 1.9.9.5   jansson 2.4   jasper-devel 1.900.1   jasper-libs 1.900.1   java-1.7.0-openjdk 1.7.0.91   java-1.7.0-openjdk-headless 1.7.0.91   java-1.8.0-openjdk 1.8.0.65   java-1.8.0-openjdk-headless 1.8.0.65   javamail 1.4.6   javapackages-tools 3.4.1   jbigkit-libs 2.0   jboss-servlet-3.0-api 1.0.1   jemalloc 3.6.0   js 1.8.5   json-c 0.11   kactivities 4.10.5   kbd 1.15.5   kbd-legacy 1.15.5   kbd-misc 1.15.5   kde-filesystem 4   kde-runtime 4.10.5   kde-runtime-drkonqi 4.10.5   kde-runtime-libs 4.10.5   kde-settings 19   kdelibs 4.10.5   kdelibs-common 4.10.5   kdepimlibs 4.10.5   kdepimlibs-kxmlrpcclient 4.10.5   kernel 3.10.0   kernel 3.10.0   kernel-devel 3.10.0   kernel-headers 3.10.0   kernel-tools 3.10.0   kernel-tools-libs 3.10.0   kexec-tools 2.0.7   keyutils 1.5.8   keyutils-libs 1.5.8   keyutils-libs-devel 1.5.8   kmod 14   kmod-iser 1.5   kmod-kernel-mft-mlnx 3.8.0   kmod-knem-mlnx 1.1.1.90mlnx   kmod-libs 14   kmod-mlnx-ofa_kernel 2.4   kmod-srp 1.4   knem-mlnx 1.1.1.90mlnx   kpartx 0.4.9   krb5-devel 1.12.2   krb5-libs 1.12.2   krb5-workstation 1.12.2   ksh 20120801   lapack 3.4.2   lapack-devel 3.4.2   lbzip2 2.5   lcms2 2.5   ldns 1.6.16   less 458   lftp 4.4.8   libGLEW 1.9.0   libGLEWmx 1.9.0   libICE 1.0.8   libICE-devel 1.0.8   libIDL 0.8.14   libSM 1.2.1   libSM-devel 1.2.1   libX11 1.6.0   libX11-common 1.6.0   libX11-devel 1.6.0   libXScrnSaver 1.2.2   libXau 1.0.8   libXau-devel 1.0.8   libXaw 1.0.11   libXcomposite 0.4.4   libXcomposite-devel 0.4.4   libXcursor 1.1.14   libXcursor-devel 1.1.14   libXdamage 1.1.4   libXdamage-devel 1.1.4   libXdmcp 1.1.1   libXevie 1.0.3   libXext 1.3.2   libXext-devel 1.3.2   libXfixes 5.0.1   libXfixes-devel 5.0.1   libXfont 1.4.7   libXft 2.3.1   libXft-devel 2.3.1   libXi 1.7.2   libXi-devel 1.7.2   libXinerama 1.1.3   libXinerama-devel 1.1.3   libXmu 1.1.1   libXp 1.0.2   libXpm 3.5.10   libXpm-devel 3.5.10   libXrandr 1.4.1   libXrandr-devel 1.4.1   libXrender 0.9.8   libXrender-devel 0.9.8   libXres 1.0.7   libXt 1.1.4   libXt-devel 1.1.4   libXtst 1.2.2   libXv 1.0.9   libXv-devel 1.0.9   libXxf86dga 1.1.4   libXxf86misc 1.0.3   libXxf86vm 1.1.3   libXxf86vm-devel 1.1.3   libacl 2.2.51   libaio 0.3.109   libarchive 3.1.2   libart_lgpl 2.3.21   libassuan 2.1.0   libasyncns 0.8   libatasmart 0.19   libattr 2.4.46   libavc1394 0.5.3   libbasicobjects 0.1.1   libblkid 2.23.2   libbluray 0.2.3   libbonobo 2.32.1   libbonoboui 2.24.5   libcanberra 0.30   libcap 2.22   libcap-ng 0.7.3   libcdio 0.92   libcdio-paranoia 10.2+0.90   libcollection 0.6.2   libcom_err 1.42.9   libcom_err-devel 1.42.9   libconfig 1.4.9   libconfuse 2.7   libcroco 0.6.8   libcurl 7.29.0   libcurl-devel 7.29.0   libdaemon 0.14   libdap 3.13.1   libdb 5.3.21   libdb-cxx 5.3.21   libdb-cxx-devel 5.3.21   libdb-devel 5.3.21   libdb-utils 5.3.21   libdrm 2.4.56   libdrm-devel 2.4.56   libdv 1.0.0   libedit 3.0   liberation-fonts-common 1.07.2   liberation-mono-fonts 1.07.2   libestr 0.1.9   libevent 2.0.21   libexif 0.6.21   libffado 2.1.0   libffi 3.0.13   libffi-devel 3.0.13   libfontenc 1.1.1   libgcc 4.8.3   libgcrypt 1.5.3   libgcrypt-devel 1.5.3   libgee 0.10.1   libgeotiff 1.2.5   libgfortran 4.8.3   libglade2 2.6.4   libgnome 2.32.1   libgnome-keyring 3.8.0   libgnomecanvas 2.30.3   libgomp 4.8.3   libgpg-error 1.12   libgpg-error-devel 1.12   libgphoto2 2.5.2   libgsf 1.14.26   libgta 1.0.4   libgudev1 208   libgusb 0.1.6   libgxps 0.2.2   libibcm 1.0.5mlnx1   libibcm-devel 1.0.5mlnx1   libibmad 1.3.11.MLNX20140817.fad53c6   libibmad-devel 1.3.11.MLNX20140817.fad53c6   libibmad-static 1.3.11.MLNX20140817.fad53c6   libibprof 1.0.98   libibumad 1.3.9.MLNX20140817.485ffa6   libibumad-devel 1.3.9.MLNX20140817.485ffa6   libibumad-static 1.3.9.MLNX20140817.485ffa6   libibverbs 1.1.8mlnx1   libibverbs-devel 1.1.8mlnx1   libibverbs-devel-static 1.1.8mlnx1   libibverbs-utils 1.1.8mlnx1   libical 0.48   libicu 50.1.2   libicu-devel 50.1.2   libidn 1.28   libiec61883 1.2.0   libieee1284 0.2.11   libimobiledevice 1.1.5   libini_config 1.1.0   libiodbc 3.52.7   libiptcdata 1.0.4   libirman 0.4.5   libjpeg-turbo 1.2.90   libjpeg-turbo-devel 1.2.90   libjpeg-turbo-utils 1.2.90   libldb 1.1.17   liblockfile 1.08   libmalaga 7.12   libmlx4 1.0.6mlnx1   libmlx4-devel 1.0.6mlnx1   libmlx5 1.0.1mlnx2   libmlx5-devel 1.0.1mlnx2   libmng 1.0.10   libmnl 1.0.3   libmodman 2.0.1   libmount 2.23.2   libmpc 1.0.1   libmudflap 4.8.3   libndp 1.2   libnetfilter_conntrack 1.0.4   libnfnetlink 1.0.1   libnfsidmap 0.25   libnice 0.1.3   libnl 1.1.4   libnl3 3.2.21   libnl3-cli 3.2.21   libnotify 0.7.5   libodb 2.3.0   libodb-boost 2.3.0   libogg 1.3.0   libopenraw 0.0.9   liboping 1.6.2   libosinfo 0.2.11   libotf 0.9.13   libpaper 1.1.24   libpath_utils 0.2.1   libpcap 1.5.3   libpciaccess 0.13.1   libpipeline 1.2.3   libplist 1.10   libpng 1.5.13   libpng-devel 1.5.13   libpng12 1.2.50   libproxy 0.4.11   libpwquality 1.2.3   libquadmath 4.8.3   libquadmath-devel 4.8.3   libquvi 0.4.1   libquvi-scripts 0.4.10   libraw1394 2.1.0   librdmacm 1.0.19mlnx   librdmacm-devel 1.0.19mlnx   librdmacm-utils 1.0.19mlnx   libref_array 0.1.4   libreport-filesystem 2.1.11   librsvg2 2.39.0   libsamplerate 0.1.8   libsecret 0.15   libselinux 2.2.2   libselinux-devel 2.2.2   libselinux-python 2.2.2   libselinux-utils 2.2.2   libsemanage 2.1.10   libsepol 2.1.9   libsepol-devel 2.1.9   libshout 2.2.2   libsigc++20 2.3.1   libsmbclient 4.1.12   libsndfile 1.0.25   libsoup 2.46.0   libspectre 0.2.7   libss 1.42.9   libssh2 1.4.3   libstdc++ 4.8.3   libstdc++-devel 4.8.3   libstdc++-static 4.8.3   libsysfs 2.1.0   libtalloc 2.1.1   libtasn1 3.8   libtasn1-devel 3.8   libtdb 1.3.0   libteam 1.15   libtevent 0.9.21   libthai 0.1.14   libtheora 1.1.1   libtiff 4.0.3   libtiff-devel 4.0.3   libtirpc 0.2.4   libtomcrypt 1.17   libtommath 0.42.0   libtool 2.4.2   libtool-ltdl 2.4.2   libudisks2 2.1.2   libunistring 0.9.3   libunwind 1.1   libusb 0.1.4   libusbx 1.0.15   libuser 0.60   libutempter 1.1.6   libuuid 2.23.2   libv4l 0.9.5   libverto 0.2.5   libverto-devel 0.2.5   libverto-tevent 0.2.5   libvisual 0.4.0   libvorbis 1.3.3   libwbclient 4.1.12   libwebp 0.3.0   libwmf-lite 0.2.8.4   libwnck3 3.4.5   libxcb 1.9   libxcb-devel 1.9   libxkbfile 1.0.8   libxml++ 2.37.1   libxml2 2.9.1   libxml2-devel 2.9.1   libxml2-python 2.9.1   libxslt 1.1.28   libxslt-devel 1.1.28   libyaml 0.1.4   libyaml-devel 0.1.4   libzip 0.10.1   links 2.8   linux-firmware 20140911   lirc 0.9.1a   lirc-libs 0.9.1a   lksctp-tools 1.0.13   lm_sensors 3.3.4   lm_sensors-libs 3.3.4   lockdev 1.0.4   lockfile-progs 0.1.15   log4j 1.2.17   logrotate 3.8.6   lshw B.02.17   lsof 4.87   lua 5.1.4   lvm2 2.02.115   lvm2-libs 2.02.115   lzo 2.06   lzo-devel 2.06   lzo-minilzo 2.06   m17n-contrib 1.1.14   m17n-db 1.6.4   m17n-lib 1.6.4   m2crypto 0.21.1   m4 1.4.16   mailcap 2.1.41   mailx 12.5   make 3.82   malaga 7.12   man-db 2.6.3   mariadb 5.5.44   mariadb-devel 5.5.44   mariadb-libs 5.5.44   marisa 0.2.4   mdadm 3.3.2   media-player-info 17   mercurial 2.6.2   mesa-libEGL 10.2.7   mesa-libEGL-devel 10.2.7   mesa-libGL 10.2.7   mesa-libGL-devel 10.2.7   mesa-libGLU 9.0.0   mesa-libGLU-devel 9.0.0   mesa-libgbm 10.2.7   mesa-libglapi 10.2.7   mft 3.8.0   microcode_ctl 2.1   mlnx-ofa_kernel 2.4   mlnx-ofa_kernel-devel 2.4   mlnxofed-docs 2.4   mod_ssl 2.4.6   mod_wsgi 3.4   motif 2.3.4   mozjs17 17.0.0   mpfr 3.1.1   mpi-selector 1.0.3   mstflint 3.8.0   mtools 4.0.18   mxm 3.2.2990   myproxy 6.1.18   myproxy-libs 6.1.18   myproxy-oauth 0.21   myproxy-server 6.1.18   nagios-common 4.0.8   nagios-plugins 2.0.3   nagios-plugins-all 2.0.3   nagios-plugins-breeze 2.0.3   nagios-plugins-by_ssh 2.0.3   nagios-plugins-cluster 2.0.3   nagios-plugins-dhcp 2.0.3   nagios-plugins-dig 2.0.3   nagios-plugins-disk 2.0.3   nagios-plugins-disk_smb 2.0.3   nagios-plugins-dns 2.0.3   nagios-plugins-dummy 2.0.3   nagios-plugins-file_age 2.0.3   nagios-plugins-flexlm 2.0.3   nagios-plugins-fping 2.0.3   nagios-plugins-game 2.0.3   nagios-plugins-hpjd 2.0.3   nagios-plugins-http 2.0.3   nagios-plugins-icmp 2.0.3   nagios-plugins-ide_smart 2.0.3   nagios-plugins-ircd 2.0.3   nagios-plugins-ldap 2.0.3   nagios-plugins-load 2.0.3   nagios-plugins-log 2.0.3   nagios-plugins-mailq 2.0.3   nagios-plugins-mrtg 2.0.3   nagios-plugins-mrtgtraf 2.0.3   nagios-plugins-mysql 2.0.3   nagios-plugins-nagios 2.0.3   nagios-plugins-nt 2.0.3   nagios-plugins-ntp 2.0.3   nagios-plugins-ntp-perl 2.0.3   nagios-plugins-nwstat 2.0.3   nagios-plugins-oracle 2.0.3   nagios-plugins-overcr 2.0.3   nagios-plugins-perl 2.0.3   nagios-plugins-pgsql 2.0.3   nagios-plugins-ping 2.0.3   nagios-plugins-procs 2.0.3   nagios-plugins-real 2.0.3   nagios-plugins-rpc 2.0.3   nagios-plugins-sensors 2.0.3   nagios-plugins-smtp 2.0.3   nagios-plugins-snmp 2.0.3   nagios-plugins-ssh 2.0.3   nagios-plugins-swap 2.0.3   nagios-plugins-tcp 2.0.3   nagios-plugins-time 2.0.3   nagios-plugins-ups 2.0.3   nagios-plugins-users 2.0.3   nagios-plugins-wave 2.0.3   nano 2.3.1   nautilus 3.8.2   nautilus-extensions 3.8.2   ncftp 3.2.5   ncurses 5.9   ncurses-base 5.9   ncurses-devel 5.9   ncurses-libs 5.9   neon 0.30.0   nepomuk-core 4.10.5   nepomuk-core-libs 4.10.5   net-snmp-libs 5.7.2   net-snmp-utils 5.7.2   net-tools 2.0   netcdf 4.3.3.1   netpbm 10.61.02   nettle 2.7.1   nettle-devel 2.7.1   newt 0.52.15   newt-python 0.52.15   nfs-utils 1.3.0   nrpe 2.15   nscd 2.17   nspr 4.10.8   nss 3.19.1   nss-pam-ldapd 0.8.13   nss-softokn 3.16.2.3   nss-softokn-freebl 3.16.2.3   nss-softokn-freebl 3.16.2.3   nss-sysinit 3.19.1   nss-tools 3.19.1   nss-util 3.19.1   nss_compat_ossl 0.9.6   ntfs-3g 2016.2.22   ntp 4.2.6p5   ntpdate 4.2.6p5   numactl 2.0.9   numactl-libs 2.0.9   nut-client 2.7.2   ocaml 4.01.0   ocaml-compiler-libs 4.01.0   ocaml-runtime 4.01.0   ofed-scripts 2.4   ogdi 3.2.0   openjade 1.3.2   openjpeg-libs 1.5.1   openldap 2.4.39   openldap-clients 2.4.39   openslp 2.0.0   opensm 4.3.0.MLNX20141222.713c9d5   opensm-devel 4.3.0.MLNX20141222.713c9d5   opensm-libs 4.3.0.MLNX20141222.713c9d5   opensm-static 4.3.0.MLNX20141222.713c9d5   opensp 1.5.2   openssh 6.6.1p1   openssh-clients 6.6.1p1   openssh-server 6.6.1p1   openssl 1.0.1e   openssl-devel 1.0.1e   openssl-libs 1.0.1e   opus 1.0.2   orc 0.4.17   os-prober 1.58   oxygen-icon-theme 4.10.5   p11-kit 0.20.7   p11-kit-devel 0.20.7   p11-kit-trust 0.20.7   p7zip 15.09   pakchois 0.4   pam 1.1.8   pam_krb5 2.4.8   pandoc 1.12.3.1   pandoc-citeproc 0.3.0.1   pango 1.34.1   pango-devel 1.34.1   pangomm 2.34.0   parted 3.1   passwd 0.79   patch 2.7.1   patchutils 0.3.3   pciutils 3.2.1   pciutils-libs 3.2.1   pcre 8.32   pcre-devel 8.32   pcsc-lite-libs 1.8.8   perftest 2.4   perl 5.16.3   perl-Any-Moose 0.21   perl-App-Nopaste 0.90   perl-App-SVN-Bisect 1.1   perl-AppConfig 1.66   perl-Authen-SASL 2.15   perl-B-Hooks-EndOfScope 0.13   perl-B-Keywords 1.13   perl-B-Utils 0.21   perl-Bit-Vector 7.3   perl-Browser-Open 0.04   perl-Business-ISBN 2.06   perl-Business-ISBN-Data 20120719.001   perl-Cairo 1.104   perl-Cairo-GObject 1.001   perl-Carp 1.26   perl-Carp-Clan 6.04   perl-Class-Inspector 1.28   perl-Class-Load 0.20   perl-Clipboard 0.13   perl-Clone 0.34   perl-Compress-Raw-Bzip2 2.061   perl-Compress-Raw-Zlib 2.061   perl-Config-GitLike 1.10   perl-Convert-BinHex 1.119   perl-Crypt-DES 2.05   perl-Crypt-RC4 2.02   perl-DBD-MySQL 4.023   perl-DBD-SQLite 1.39   perl-DBI 1.627   perl-DB_File 1.830   perl-Data-Dump-Streamer 2.36   perl-Data-Dumper 2.145   perl-Data-Dumper-Concise 2.020   perl-Data-OptList 0.107   perl-Date-Calc 6.3   perl-Date-Manip 6.41   perl-Devel-Caller 2.06   perl-Devel-GlobalDestruction 0.12   perl-Devel-LexAlias 0.05   perl-Devel-PartialDump 0.15   perl-Devel-REPL 1.003015   perl-Digest 1.17   perl-Digest-HMAC 1.03   perl-Digest-MD5 2.52   perl-Digest-Perl-MD5 1.9   perl-Digest-SHA 5.85   perl-Digest-SHA1 2.13   perl-Dist-CheckConflicts 0.06   perl-Email-Date-Format 1.002   perl-Encode 2.51   perl-Encode-Locale 1.03   perl-Env 1.04   perl-Error 0.17020   perl-Eval-Closure 0.08   perl-Exporter 5.68   perl-ExtUtils-Install 1.58   perl-ExtUtils-MakeMaker 6.68   perl-ExtUtils-Manifest 1.61   perl-ExtUtils-ParseXS 3.18   perl-File-BaseDir 0.03   perl-File-DesktopEntry 0.08   perl-File-HomeDir 1.00   perl-File-Listing 6.04   perl-File-MimeInfo 0.21   perl-File-Next 1.12   perl-File-Path 2.09   perl-File-ReadBackwards 1.05   perl-File-Remove 1.52   perl-File-Temp 0.23.01   perl-File-Which 1.09   perl-Filter 1.49   perl-Font-AFM 1.20   perl-Font-TTF 1.02   perl-FreezeThaw 0.5001   perl-GD 2.49   perl-GDGraph 1.44   perl-GDTextUtil 0.86   perl-GSSAPI 0.28   perl-Getopt-Long 2.40   perl-Getopt-Long-Descriptive 0.093   perl-Getopt-Simple 1.49   perl-Git 1.8.3.1   perl-Git-SVN 1.8.3.1   perl-Glib 1.305   perl-GraphViz 2.14   perl-HTML-Form 6.03   perl-HTML-Format 2.10   perl-HTML-FormatText-WithLinks 0.14   perl-HTML-FormatText-WithLinks-AndTables 0.02   perl-HTML-Parser 3.71   perl-HTML-Tagset 3.20   perl-HTML-Template 2.95   perl-HTML-Tree 5.03   perl-HTTP-Cookies 6.01   perl-HTTP-Daemon 6.01   perl-HTTP-Date 6.02   perl-HTTP-Message 6.06   perl-HTTP-Negotiate 6.01   perl-HTTP-ProxyAutoConfig 0.3   perl-HTTP-Tiny 0.033   perl-IO-All 0.61   perl-IO-Compress 2.061   perl-IO-HTML 1.00   perl-IO-Multiplex 1.13   perl-IO-SessionData 1.03   perl-IO-Socket-IP 0.21   perl-IO-Socket-SSL 1.94   perl-IO-String 1.08   perl-IO-Tty 1.10   perl-IO-stringy 2.110   perl-IPC-Run 0.92   perl-IPC-Signal 1.00   perl-Image-Base 1.07   perl-Inline 0.53   perl-JSON 2.59   perl-Jcode 2.07   perl-LWP-MediaTypes 6.02   perl-Lexical-Persistence 1.020   perl-List-MoreUtils 0.33   perl-Locale-Maketext 1.23   perl-Locale-Maketext-Simple 0.21   perl-Log-Message 0.08   perl-Log-Message-Simple 0.10   perl-MIME-Lite 3.030   perl-MIME-Types 1.38   perl-MIME-tools 5.505   perl-MRO-Compat 0.12   perl-Mail-Sendmail 0.79   perl-MailTools 2.12   perl-Module-Implementation 0.06   perl-Module-Load 0.24   perl-Module-Pluggable 4.8   perl-Module-Refresh 0.17   perl-Module-Runtime 0.013   perl-Moose 2.1005   perl-MooseX-Getopt 0.47   perl-MooseX-Object-Pluggable 0.0011   perl-MooseX-Role-Parameterized 1.07   perl-Net-DNS 0.72   perl-Net-Daemon 0.48   perl-Net-HTTP 6.06   perl-Net-LibIDN 0.12   perl-Net-SMTP-SSL 1.01   perl-Net-SNMP 6.0.1   perl-Net-SSLeay 1.55   perl-Net-Server 2.007   perl-Net-XMPP 1.02   perl-Number-Compare 0.03   perl-OLE-Storage_Lite 0.19   perl-OpenGL 0.6702   perl-PDF-API2 2.021   perl-PDL 2.7.0   perl-PPI 1.215   perl-Package-DeprecationManager 0.13   perl-Package-Generator 0.103   perl-Package-Stash 0.34   perl-Package-Stash-XS 0.26   perl-PadWalker 1.96   perl-Params-Check 0.38   perl-Params-Util 1.07   perl-Params-Validate 1.08   perl-Parse-RecDescent 1.967009   perl-PathTools 3.40   perl-PlRPC 0.2020   perl-Pod-Escapes 1.04   perl-Pod-Parser 1.61   perl-Pod-Perldoc 3.20   perl-Pod-Simple 3.28   perl-Pod-Usage 1.63   perl-Prima 1.37   perl-Proc-WaitStat 1.00   perl-SOAP-Lite 1.10   perl-Scalar-List-Utils 1.27   perl-Set-Scalar 1.25   perl-Socket 2.010   perl-Socket6 0.23   perl-Spreadsheet-ParseExcel 0.5900   perl-Spreadsheet-WriteExcel 2.40   perl-Storable 2.45   perl-Sub-Exporter 0.986   perl-Sub-Exporter-Progressive 0.001011   perl-Sub-Identify 0.04   perl-Sub-Install 0.926   perl-Sub-Name 0.09   perl-Sys-SigAction 0.20   perl-Sys-Syslog 0.33   perl-Task-Weaken 1.04   perl-Term-UI 0.36   perl-TermReadKey 2.30   perl-Test-Harness 3.28   perl-Test-Pod 1.48   perl-Test-Simple 0.98   perl-Text-ParseWords 3.29   perl-Text-Unidecode 0.04   perl-Thread-Queue 3.02   perl-Tie-IxHash 1.22   perl-Time-HiRes 1.9725   perl-Time-Local 1.2300   perl-Time-Piece 1.20.1   perl-TimeDate 2.30   perl-Tk 804.030   perl-Try-Tiny 0.12   perl-URI 1.60   perl-Unicode-Map 0.112   perl-Variable-Magic 0.54   perl-WWW-Mechanize 1.72   perl-WWW-Pastebin-PastebinCom-Create 0.004   perl-WWW-RobotRules 6.02   perl-XML-Filter-BufferText 1.01   perl-XML-LibXML 2.0018   perl-XML-LibXSLT 1.80   perl-XML-NamespaceSupport 1.11   perl-XML-Parser 2.41   perl-XML-SAX 0.99   perl-XML-SAX-Base 1.08   perl-XML-SAX-Writer 0.53   perl-XML-Simple 2.20   perl-XML-Stream 1.23   perl-XML-Twig 3.44   perl-XML-Writer 0.623   perl-XML-XPath 1.13   perl-YAML 0.84   perl-YAML-Syck 1.27   perl-constant 1.27   perl-devel 5.16.3   perl-libintl 1.20   perl-libs 5.16.3   perl-libwww-perl 6.05   perl-macros 5.16.3   perl-namespace-autoclean 0.19   perl-namespace-clean 0.24   perl-parent 0.225   perl-podlators 2.5.1   perl-srpm-macros 1   perl-threads 1.87   perl-threads-shared 1.43   perltidy 20121207   phonon 4.6.0   phonon-backend-gstreamer 4.6.3   php-channel-horde 1.0   php-cli 5.4.16   php-common 5.4.16   php-horde-Horde-Util 2.5.6   php-horde-Horde-Xml-Wbxml 2.0.2   php-intl 5.4.16   php-mbstring 5.4.16   php-pear 1.9.4   php-process 5.4.16   php-xml 5.4.16   pigz 2.3.3   pinentry 0.8.1   pixman 0.32.4   pixman-devel 0.32.4   pkgconfig 0.27.1   plymouth 0.8.9   plymouth-core-libs 0.8.9   plymouth-scripts 0.8.9   pm-utils 1.4.1   policycoreutils 2.2.5   polkit 0.112   polkit-pkla-compat 0.1   polkit-qt 0.103.0   poppler 0.22.5   poppler-data 0.4.6   poppler-glib 0.22.5   poppler-qt 0.22.5   poppler-utils 0.22.5   popt 1.13   portaudio 19   postfix 2.10.1   postgresql 9.2.14   postgresql-libs 9.2.14   postgresql-plperl 9.2.14   postgresql-server 9.2.14   ppp 2.4.5   procps-ng 3.3.10   proj 4.8.0   proj-epsg 4.8.0   proj-nad 4.8.0   protobuf 2.5.0   psacct 6.6.1   psmisc 22.20   psutils 1.17   psutils-perl 1.17   pth 2.0.7   pulseaudio 3.0   pulseaudio-libs 3.0   pulseaudio-libs-glib2 3.0   pv 1.4.6   pyOpenSSL 0.13.1   pygobject3-base 3.8.2   pygpgme 0.3   pyliblzma 0.5.3   pyparsing 1.5.6   pytalloc 2.1.1   python 2.7.5   python-GnuPGInterface 0.3.2   python-backports 1.0   python-backports-ssl_match_hostname 3.4.0.2   python-chardet 2.2.1   python-configobj 4.7.2   python-decorator 3.4.0   python-devel 2.7.5   python-iniparse 0.4   python-javapackages 3.4.1   python-kitchen 1.1.1   python-ldap 2.4.15   python-libs 2.7.5   python-lxml 3.2.1   python-pycurl 7.19.0   python-pyudev 0.15   python-setuptools 0.9.8   python-slip 0.4.0   python-slip-dbus 0.4.0   python-urlgrabber 3.10   python2-crypto 2.6.1   pyxattr 0.5.1   qca2 2.0.3   qpdf-libs 5.0.1   qperf 0.4.9   qrencode-libs 3.4.1   qstat 2.11   qt 4.8.5   qt-devel 4.8.5   qt-settings 19   qt-x11 4.8.5   quota 4.01   quota-nls 4.01   raptor2 2.0.9   rasqal 0.9.30   rds-devel 2.0.7   rds-tools 2.0.7   readline 6.2   readline-devel 6.2   redhat-menus 12.0.2   redhat-rpm-config 9.1.0   redland 1.0.16   redland-virtuoso 1.0.16   rootfiles 8.1   rpcbind 0.2.0   rpm 4.11.1   rpm-build 4.11.1   rpm-build-libs 4.11.1   rpm-libs 4.11.1   rpm-python 4.11.1   rrdtool 1.4.8   rsync 3.0.9   rsyslog 7.4.7   rtkit 0.11   ruby 2.0.0.598   ruby-irb 2.0.0.598   ruby-libs 2.0.0.598   ruby-shadow 1.4.1   rubygem-bigdecimal 1.2.0   rubygem-io-console 0.4.2   rubygem-json 1.7.7   rubygem-psych 2.0.0   rubygem-rdoc 4.0.0   rubygem-sqlite3 1.3.5   rubygems 2.0.14   samba-client 4.1.12   samba-common 4.1.12   samba-libs 4.1.12   sane-backends-libs 1.0.24   screen 4.1.0   sed 4.2.2   selinux-policy 3.13.1   selinux-policy-targeted 3.13.1   setup 2.8.71   sgml-common 0.6.3   shadow-utils 4.1.5.1   shapelib 1.3.0   shared-desktop-ontologies 0.11.0   shared-mime-info 1.1   sip 4.14.6   sip-devel 4.14.6   sip-macros 4.14.6   slang 2.2.4   snappy 1.1.0   soprano 2.9.2   sound-theme-freedesktop 0.8   source-highlight 3.1.6   speex 1.2   sqlite 3.7.17   sqlite-devel 3.7.17   srptools 1.0.1   startup-notification 0.12   strace 4.8   strigi-libs 0.7.7   subversion 1.7.14   subversion-libs 1.7.14   subversion-perl 1.7.14   sudo 1.8.6p7   suitesparse 4.0.2   svn-bisect 1.1   swig 2.0.10   systemd 208   systemd-libs 208   systemd-sysv 208   systemtap-sdt-devel 2.6   sysvinit-tools 2.88   taglib 1.8   tar 1.26   tbb 4.1   tcl 8.5.13   tcl-devel 8.5.13   tcp_wrappers 7.6   tcp_wrappers-libs 7.6   tcpdump 4.5.1   tcsh 6.18.01   teamd 1.15   texinfo 5.1   texinfo-tex 5.1   texlive 2012   texlive-ae svn15878.1.4   texlive-algorithms svn15878.0.1   texlive-amscls svn29207.0   texlive-amsfonts svn29208.3.04   texlive-amsmath svn29327.2.14   texlive-anysize svn15878.0   texlive-attachfile svn21866.v1.5b   texlive-avantgar svn28614.0   texlive-babel svn24756.3.8m   texlive-babelbib svn25245.1.31   texlive-base 2012   texlive-beamer svn29349.3.26   texlive-bera svn20031.0   texlive-beton svn15878.0   texlive-bibtex svn26689.0.99d   texlive-bibtex-bin svn26509.0   texlive-bookman svn28614.0   texlive-booktabs svn15878.1.61803   texlive-breakurl svn15878.1.30   texlive-caption svn29026.3.3__2013_02_03_   texlive-carlisle svn18258.0   texlive-charter svn15878.0   texlive-chngcntr svn17157.1.0a   texlive-cite svn19955.5.3   texlive-cm svn29581.0   texlive-cm-super svn15878.0   texlive-cmap svn26568.0   texlive-cmextra svn14075.0   texlive-collection-basic svn26314.0   texlive-collection-documentation-base svn17091.0   texlive-collection-fontsrecommended svn28082.0   texlive-collection-latex svn25030.0   texlive-collection-latexrecommended svn25795.0   texlive-colortbl svn25394.v1.0a   texlive-courier svn28614.0   texlive-crop svn15878.1.5   texlive-csquotes svn24393.5.1d   texlive-ctable svn26694.1.23   texlive-currfile svn29012.0.7b   texlive-dvipdfm svn26689.0.13.2d   texlive-dvipdfm-bin svn13663.0   texlive-dvipdfmx svn26765.0   texlive-dvipdfmx-bin svn26509.0   texlive-dvipdfmx-def svn15878.0   texlive-dvips svn29585.0   texlive-dvips-bin svn26509.0   texlive-ec svn25033.1.0   texlive-enctex svn28602.0   texlive-enumitem svn24146.3.5.2   texlive-epsf svn21461.2.7.4   texlive-eso-pic svn21515.2.0c   texlive-etex svn22198.2.1   texlive-etex-pkg svn15878.2.0   texlive-etoolbox svn20922.2.1   texlive-euler svn17261.2.5   texlive-euro svn22191.1.1   texlive-eurosym svn17265.1.4_subrfix   texlive-extsizes svn17263.1.4a   texlive-fancybox svn18304.1.4   texlive-fancyhdr svn15878.3.1   texlive-fancyref svn15878.0.9c   texlive-fancyvrb svn18492.2.8   texlive-filecontents svn24250.1.3   texlive-filehook svn24280.0.5d   texlive-fix2col svn17133.0   texlive-float svn15878.1.3d   texlive-fontspec svn29412.v2.3a   texlive-footmisc svn23330.5.5b   texlive-fp svn15878.0   texlive-fpl svn15878.1.002   texlive-geometry svn19716.5.6   texlive-glyphlist svn28576.0   texlive-graphics svn25405.1.0o   texlive-gsftopk svn26689.1.19.2   texlive-gsftopk-bin svn26509.0   texlive-helvetic svn28614.0   texlive-hyperref svn28213.6.83m   texlive-hyph-utf8 svn29641.0   texlive-hyphen-base svn29197.0   texlive-ifetex svn24853.1.2   texlive-ifluatex svn26725.1.3   texlive-ifxetex svn19685.0.5   texlive-index svn24099.4.1beta   texlive-jknapltx svn19440.0   texlive-kastrup svn15878.0   texlive-koma-script svn27255.3.11b   texlive-kpathsea svn28792.0   texlive-kpathsea-bin svn27347.0   texlive-kpathsea-lib 2012   texlive-l3experimental svn29361.SVN_4467   texlive-l3kernel svn29409.SVN_4469   texlive-l3packages svn29361.SVN_4467   texlive-latex svn27907.0   texlive-latex-bin svn26689.0   texlive-latex-bin-bin svn14050.0   texlive-latex-fonts svn28888.0   texlive-latexconfig svn28991.0   texlive-listings svn15878.1.4   texlive-lm svn28119.2.004   texlive-lm-math svn29044.1.958   texlive-ltxmisc svn21927.0   texlive-lua-alt-getopt svn29349.0.7.0   texlive-lualatex-math svn29346.1.2   texlive-luaotfload svn26718.1.26   texlive-luaotfload-bin svn18579.0   texlive-luatex svn26689.0.70.1   texlive-luatex-bin svn26912.0   texlive-luatexbase svn22560.0.31   texlive-makeindex svn26689.2.12   texlive-makeindex-bin svn26509.0   texlive-marginnote svn25880.v1.1i   texlive-marvosym svn29349.2.2a   texlive-mathpazo svn15878.1.003   texlive-mdwtools svn15878.1.05.4   texlive-memoir svn21638.3.6j_patch_6.0g   texlive-metafont svn26689.2.718281   texlive-metafont-bin svn26912.0   texlive-metalogo svn18611.0.12   texlive-mflogo svn17487.0   texlive-mfnfss svn19410.0   texlive-mfware svn26689.0   texlive-mfware-bin svn26509.0   texlive-mh svn29420.0   texlive-microtype svn29392.2.5   texlive-misc svn24955.0   texlive-mparhack svn15878.1.4   texlive-mptopdf svn26689.0   texlive-mptopdf-bin svn18674.0   texlive-ms svn24467.0   texlive-multido svn18302.1.42   texlive-natbib svn20668.8.31b   texlive-ncntrsbk svn28614.0   texlive-ntgclass svn15878.0   texlive-oberdiek svn26725.0   texlive-palatino svn28614.0   texlive-paralist svn15878.2.3b   texlive-parallel svn15878.0   texlive-parskip svn19963.2.0   texlive-pdfpages svn27574.0.4t   texlive-pdftex svn29585.1.40.11   texlive-pdftex-bin svn27321.0   texlive-pdftex-def svn22653.0.06d   texlive-pgf svn22614.2.10   texlive-plain svn26647.0   texlive-powerdot svn25656.1.4i   texlive-psfrag svn15878.3.04   texlive-pslatex svn16416.0   texlive-psnfss svn23394.9.2a   texlive-pspicture svn15878.0   texlive-pst-3d svn17257.1.10   texlive-pst-blur svn15878.2.0   texlive-pst-coil svn24020.1.06   texlive-pst-eps svn15878.1.0   texlive-pst-fill svn15878.1.01   texlive-pst-grad svn15878.1.06   texlive-pst-math svn20176.0.61   texlive-pst-node svn27799.1.25   texlive-pst-plot svn28729.1.44   texlive-pst-slpe svn24391.1.31   texlive-pst-text svn15878.1.00   texlive-pst-tree svn24142.1.12   texlive-pstricks svn29678.2.39   texlive-pstricks-add svn28750.3.59   texlive-pxfonts svn15878.0   texlive-qstest svn15878.0   texlive-rcs svn15878.0   texlive-rotating svn16832.2.16b   texlive-rsfs svn15878.0   texlive-sansmath svn17997.1.1   texlive-sauerj svn15878.0   texlive-scheme-basic svn25923.0   texlive-section svn20180.0   texlive-seminar svn18322.1.5   texlive-sepnum svn20186.2.0   texlive-setspace svn24881.6.7a   texlive-showexpl svn27790.v0.3j   texlive-soul svn15878.2.4   texlive-subfig svn15878.1.3   texlive-symbol svn28614.0   texlive-tetex svn29585.3.0   texlive-tetex-bin svn27344.0   texlive-tex svn26689.3.1415926   texlive-tex-bin svn26912.0   texlive-tex-gyre svn18651.2.004   texlive-tex-gyre-math svn29045.0   texlive-texconfig svn29349.0   texlive-texconfig-bin svn27344.0   texlive-texlive.infra svn28217.0   texlive-texlive.infra-bin svn22566.0   texlive-textcase svn15878.0   texlive-thumbpdf svn26689.3.15   texlive-thumbpdf-bin svn6898.0   texlive-times svn28614.0   texlive-tipa svn29349.1.3   texlive-tools svn26263.0   texlive-txfonts svn15878.0   texlive-type1cm svn21820.0   texlive-typehtml svn17134.0   texlive-ucs svn27549.2.1   texlive-underscore svn18261.0   texlive-unicode-math svn29413.0.7d   texlive-url svn16864.3.2   texlive-utopia svn15878.0   texlive-varwidth svn24104.0.92   texlive-wasy svn15878.0   texlive-wasysym svn15878.2.0   texlive-xcolor svn15878.2.11   texlive-xdvi svn26689.22.85   texlive-xdvi-bin svn26509.0   texlive-xkeyval svn27995.2.6a   texlive-xunicode svn23897.0.981   texlive-zapfchan svn28614.0   texlive-zapfding svn28614.0   tix 8.4.3   tk 8.5.13   tk-devel 8.5.13   tkinter 2.7.5   tmux 1.8   totem-pl-parser 3.4.5   tracker 0.16.2   trousers 0.3.11.2   ttmkfdir 3.0.9   tuned 2.4.1   tzdata 2015g   tzdata-java 2015g   udisks2 2.1.2   unbound-libs 1.4.20   unixODBC 2.3.1   unzip 6.0   upower 0.9.20   urw-fonts 2.4   usbmuxd 1.0.8   ustr 1.0.4   util-linux 2.23.2   uuid 1.6.2   uuid-devel 1.6.2   valgrind 3.10.0   vim-X11 7.4.160   vim-common 7.4.160   vim-enhanced 7.4.160   vim-filesystem 7.4.160   vim-minimal 7.4.160   virt-what 1.13   virtuoso-opensource 6.1.6   wavpack 4.60.1   webrtc-audio-processing 0.1   wget 1.14   which 2.20   words 3.0   wpa_supplicant 2.0   xcb-util 0.3.9   xclip 0.12   xdg-user-dirs 0.15   xdg-utils 1.1.0   xerces-c 3.1.1   xfsdump 3.1.4   xfsprogs 3.2.1   xkeyboard-config 2.9   xml-common 0.6.3   xml-commons-apis12 1.2.04   xml2 0.5   xorg-x11-font-utils 7.5   xorg-x11-fonts-ISO8859-1-100dpi 7.5   xorg-x11-fonts-ISO8859-1-75dpi 7.5   xorg-x11-fonts-Type1 7.5   xorg-x11-proto-devel 7.7   xorg-x11-server-Xorg 1.15.0   xorg-x11-server-common 1.15.0   xorg-x11-server-utils 7.7   xorg-x11-xauth 1.0.7   xorg-x11-xbitmaps 1.1.1   xorg-x11-xinit 1.3.2   xorg-x11-xkb-utils 7.7   xpdf 3.04   xterm 295   xz 5.1.2   xz-devel 5.1.2   xz-libs 5.1.2   yajl 2.0.4   yum 3.4.3   yum-cron 3.4.3   yum-metadata-parser 1.1.4   yum-plugin-fastestmirror 1.1.31   yum-plugin-priorities 1.1.31   yum-plugin-versionlock 1.1.31   yum-utils 1.1.31   zip 3.0   zlib 1.2.7   zlib-devel 1.2.7   zlib-static 1.2.7   zsh 5.0.2   zziplib 0.13.62    ","excerpt":"The following packages are provided as part of the CentOS 7 operating …","ref":"/about/software/system/","title":"System Software"},{"body":"The following commercial software is available only for UCR accounts on the cluster.\n   Package Name Version     Gaussian 16   Intel Parallel Suite (replaced by OneAPI) 2018   Mathematica 12.1   MATLAB R2020b   SAS 9.4    Please contact us at support@hpcc.ucr.edu to be added to the corresponding group in order to gain access to the software via our module system.\n","excerpt":"The following commercial software is available only for UCR accounts …","ref":"/about/software/commercial/","title":"Commercial Software"},{"body":"HPCC cfnCluster Setup. This will show how to use your HPCC account to configure cfnCluster; allowing you to create and control your own clusters.\n1. Login to the HPCC Cluster ssh -X username@cluster.hpcc.ucr.edu  From Windows Please refer to the login instructions of our Linux Basics manual.\n2. Run hpcc_cloud configure hpcc_cloud configure   Cluster Template  Just hit enter to choose the default template   AWS Access Key ID  YOUR-aws_access_key_id (found in your credentials file)   AWS Secret Access Key ID  YOUR-aws_secret_access_key (found in your credentials file)   AWS Region ID  us-west-1   VPC Name  Just hit enter to choose the default public   Key Name  Choose your Key Name from the list   VPC ID  Choose any of the available options   Master Subnet ID  Choose any of the available options   IAM User Name  Choose your IAM User Name from the list    3. Setup complete Now that your config file is setup correctly. You can begin to create and interact with your own cluster (described in the Cluster Control and Operation section Link)\nSetup Walk Through ","excerpt":"HPCC cfnCluster Setup. This will show how to use your HPCC account to …","ref":"/manuals/ext_cloud/aws/setup/","title":"HPCC cfnCluster Setup"},{"body":"Overview In Linux (and Unix systems in general), access to files and directories is controlled by a system of owners, groups, and permission bits. Changing these settings is necessary to control access by other users. The permission system also affects what files can be executed.\nOwnership Levels  user (u) - User ownership of a file/directory. This user has the special right to change the permission bits and group ownership. group (g) - Group ownership of a file/directory. Members of this group may be assigned greater access rights than non-members. other (o) - Everyone else that isn’t the owning user or from the owning group.  Permission Bits The elemental permissions in Linux/Unix are read, write, and execute. Users and groups can have one many, or none of these rights. Their meanings are as follows:\n    Letter Number File Directory     Read r 4 View the contents View the listings   Write w 2 Modify the contents Create a new file, or rename or delete existing files   Execute x 1 Execute a program/script Traversal rights    Checking Permissions Annotated output for ls -la:\n---------- File type (d = directory, - = regular file, l = symlink) |--------- User permission triplet || ------ Group permission triplet || | --- Other permission triplet || | | || | | [user] [group] drwx-----x 61 username groupname 4096 Feb 24 16:39 ./ drwxr-xr-x 688 root root 262144 Feb 24 11:05 ../ drwx------ 2 username groupname 4096 Feb 2 22:45 .ssh/ drwxr-xr-x 5 username groupname 4096 Dec 12 15:57 Downloads/ drwxr-xr-x 2 username groupname 4096 Jan 9 16:29 bin/ -rw------- 1 username groupname 7960 Feb 23 18:37 .bash_history -rw-r--r-- 1 username groupname 306 Nov 3 15:08 .bashrc -rw-r--r-- 1 username groupname 677 Apr 8 2013 .profile -rw-r--r-- 1 username groupname 128 Nov 30 12:38 .tmux.conf -rw-r--r-- 1 username groupname 12126 Nov 2 13:14 .vimrc lrwxrwxrwx 1 username groupname 23 Sep 12 10:49 bigdata -\u003e /bigdata/groupname/username/ -rw-r--r-- 1 username groupname 5657 Sep 19 11:31 bookmarks.html lrwxrwxrwx 1 username groupname 23 Sep 12 10:49 shared -\u003e /bigdata/groupname/shared/  Assign write and execute permissions to user and group\nchmod ug+rx my_file\nTo remove all permissions from all three user groups\nchmod ugo-rwx my_file # '+' causes the permissions selected to be added # '-' causes them to be removed # '=' causes them to be the only permissions that the file has. chmod +rx public_html/ or $ chmod 755 public_html/ # Example for number system:  Change ownership chown \u003cuser\u003e \u003cfile or dir\u003e # changes user ownership chgrp \u003cgroup\u003e \u003cfile or dir\u003e # changes group ownership chown \u003cuser\u003e:\u003cgroup\u003e \u003cfile or dir\u003e # changes user \u0026 group ownership  ","excerpt":"Overview In Linux (and Unix systems in general), access to files and …","ref":"/manuals/linux_basics/permissions/","title":"Permissions and Ownership"},{"body":"Start Times Start times are a great way to track your jobs:\nsqueue -u $USER --start  Start times are rough estimates based on the current state of the queue.\nFair-Share Users that have not submitted any jobs in a long time usually have a higher priority over others that have ran jobs recently. Thus the estimated start times can be extended to allow everyone their fair share of the system. This prevents a few large groups from dominating the queuing system for long periods of time.\nYou can see with the sqmore command what priority your job has (list is sorted from lowest to highest priority). You can also check to see how your group’s priority is compared to other groups on the cluster with the “sshare” command.\nFor example:\nsshare  It may also be useful to see your entire group’s fairshare score and who has used the most shares:\nsshare -A $GROUP --all  Lastley, if you only want to see your own fairshare score:\nsshare -A $GROUP -u $USER  The fairshare score is a number between 0 and 1. The best score being 1, and the worst being 0. The fairshare score approches zero the more resource you (or your group) consume. Your individual consumption of resources (usage) does affect your entire group’s fiarshare score. The affects of your running/completed jobs on your fairshare score are halved each day (half-life). Thus, after waiting several days without running any jobs, you should see an improvment in your fairshare score.\nHere is a very good explaination of fairshare.\nPriority The fairshare score and jobs queue wait time is used to calculate your job’s priority. You can use the sprio command to check the priority of your jobs:\nsprio -u $USER  Even if your group has a lower fairshare score, your job may still have a very high priority. This would be likely due to the job’s queue wait time, and it should start as soon as possible regardless of fairshare score. You can use the sqmore command to see a list of all jobs sorted by priority.\nBackfill Some small jobs may start before yours, only if they can complete before yours starts and thus not negatively affecting your start time.\nPriority Partition Some groups on our system have purchased additional hardware. These nodes will not be affected by the fairshare score. This is because jobs submitted to the group’s partition will be evaluated first before any other jobs that have been submitted to those nodes from a different partition.\n","excerpt":"Start Times Start times are a great way to track your jobs:\nsqueue -u …","ref":"/manuals/hpc_cluster/queue/","title":"Queue Policies"},{"body":"Login to HPCC ssh username@cluster.hpcc.ucr.edu  Create a Cluster hpcc_cloud create \u003cNameForYourCluster\u003e  Show status of a Cluster hpcc_cloud status \u003cNameForYourCluster\u003e  Output:\nStatus: cfncluster-new-cluster - CREATE_COMPLETE Output:\"MasterPublicIP\"=\"52.52.227.148\" Output:\"MasterPrivateIP\"=\"172.31.24.51\" Output:\"GangliaPublicURL\"=\"http://52.52.227.148/ganglia/\" Output:\"GangliaPrivateURL\"=\"http://172.31.24.51/ganglia/\"  Note the “MasterPublicIP” Address from the output. Use this IP Address when connecting to the cluster via “ssh” or uploading and downloading via “scp”.\nShow running Clusters hcpp_cloud list  Delete cluster hpcc_cloud delete \u003cNameForYourCluster\u003e  Connecting to your cluster Note - /path/to/your/key-file.pem = where you saved your AWS account key file\nMasterPublicIP = Master Public IP address from the cluster status\nssh -i /path/to/your/key-file.pem centos@\u003cMasterPublicIP\u003e  Uploading data to your cluster This will transfer the local files to your home directory on the cluster.\nscp -i /path/to/your/key-file.pem \u003clocal-files-to-copy\u003e centos@\u003cMasterPublicIP\u003e:.  Downloading data/results This would be called from the HPCC cluster and it will download the specified remote files to your current directory.\nscp -i /path/to/your/key-file.pem centos@\u003cMasterPublicIP\u003e:./\u003cfiles-to-download\u003e .  Running a job on your cluster This will show all the steps needed to create a cluster and run a simple batch job\n1. Start a new cluster hpcc_cloud create new-cluster  2. Get the IP address of the new cluster Once the cluster build is complete you will be presented with the status informatiom. Note the “MasterPublicIP” Address from the output. Use this IP Address when connecting to the cluster via “ssh” or uploading and downloading via “scp”.\nOutput:\nhpcc_cloud create new-cluster Beginning cluster creation for cluster: new-cluster Creating stack named: cfncluster-new-cluster Status: cfncluster-new-cluster - CREATE_COMPLETE Output:\"MasterPublicIP\"=\"52.52.227.148\" Output:\"MasterPrivateIP\"=\"172.31.24.51\" Output:\"GangliaPublicURL\"=\"http://52.52.227.148/ganglia/\" Output:\"GangliaPrivateURL\"=\"http://172.31.24.51/ganglia/\"  Find the “MasterPublicIP”\n3. Upload your input files and slurm submission script This will transfer the local files to your home directory on the cluster.\nscp -i /path/to/your/key-file.pem \u003clocal-files-to-copy\u003e centos@\u003cMasterPublicIP\u003e:.  4. SSH to your new cluster ssh -i /path/to/your/key-file.pem centos@\u003cMasterPublicIP\u003e  5. Submit your job to the cluster sbatch slurm-submission-script.sh  6. Monitor your job squeue  7. Download results This command would be called from the HPCC cluster and it will download the specified remote files to your current directory.\nscp -i /path/to/your/key-file.pem centos@\u003cMasterPublicIP\u003e:./\u003cresults-to-download\u003e .  8. Delete cluster hpcc_cloud delete new-cluster  Start sample cluster walk through (sped up)  ","excerpt":"Login to HPCC ssh username@cluster.hpcc.ucr.edu  Create a Cluster …","ref":"/manuals/ext_cloud/aws/operation/","title":"Cluster Operation"},{"body":"Find Files find ~ -name \"*pattern*\" # Searches for *pattern* in and below your home directory find ~ -iname \"*pattern*\" # Same as above, but case insensitive find ~ -type f -mtime -2 # Searches for files you have modified in the last two days  Useful find arguments:\n -user \u003cuserName\u003e -group \u003cgroupName\u003e -ctime \u003cnumber of days ago changed\u003e -exec \u003ccommand to run on each file\u003e {} \\;  Find Text grep \"pattern\" \u003cFILENAME\u003e # Provides lines in a file where \"pattern\" appears grep -H \"pattern\" # -H prints out file name in front of pattern find ~ -name \"*.txt\" -exec grep -H \"pattern\" {} \\; # Search lines where \"pattern\" appears in files with names that end with \".txt\"  Find Applications which \u003cAPPLICATION_NAME\u003e # Location of application whereis \u003cAPPLICATION_NAME\u003e # Searches for executables in set of directories rpm -qa | grep \"pattern\" # List all RPM packages and filter based on \"pattern\"  ","excerpt":"Find Files find ~ -name \"*pattern*\" # Searches for *pattern* in and …","ref":"/manuals/linux_basics/finding_things/","title":"Finding Things"},{"body":"Python The scope of this manual is a brief introduction on how to manage Python packages.\nPython Versions Different Python versions do not play nice with each other. It is best to only load one Python module at any given time. The miniconda2 module for Python is the default version. This will enable users to leverage the conda installer, but with as few Python packages pre-installed as possible. This is to avoid conflicts with future needs of individuals.\nConda We have several Conda software modules:\n miniconda2 - Basic Python 2 install (default) miniconda3 - Basic Python 3 install anaconda2 - Full Python 2 install anaconda3 - Full Python 3 install For more information regarding our module system please refer to Environment Modules.  The miniconda modules are very basic installs, however users can choose to unload this basic install for a fuller one (anaconda), like so:\nmodule load miniconda2 #This is the default  After loading anaconda, you will see that there are many more Python packages installed (ie. numpy, scipy, pandas, jupyter, etc…). For a list of installed Python packages try the following:\npip list  Virtual Environments Sometimes it is best to create your own environment in which you have full control over package installs. Conda allows you to do this through virtual environments.\nInitialize Conda will now auto initialize when you load the corresponding module. No need to run the conda init or make any modifications to your ~/.bashrc file.\nConfigure Installing many packages can consume a large (ie. \u003e20GB) amount of disk space, thus it is recommended to store conda environments under your bigdata space. If you have bigdata, create the .condarc file (otherwise conda environments will be created under your home directory).\nCreate the file .condarc in your home, with the following content:\nchannels: - defaults pkgs_dirs: - ~/bigdata/.conda/pkgs envs_dirs: - ~/bigdata/.conda/envs auto_activate_base: false  Then create your Python 2 conda environment, like so:\nconda create -n NameForNewEnv python=2.7.14 # Many Python versions are available  For Python 3, please use the miniconda3, like so:\nmodule unload miniconda2 module load miniconda3 conda create -n NameForNewEnv python=3.6.4 # Many Python versions are available  Activating Once your virtual environment has been created, you need to activate it before you can use it:\nconda activate NameForNewEnv  Deactivating In order to exit from your virtual environment, do the following:\nconda deactivate  Installing packages Before installing your packages, make sure you are on a computer node. This ensures your downloads to be done quickly and with less chance of running out of memory. This can be done using the following command:\nsrun -p short -c 4 --mem=10g --pty bash -l # Adjust the resource request as needed  Here is a simple example for installing packages under your Python virtual environment via conda:\nconda install -n NameForNewEnv PackageName  You may need to enable an additional channel to install the package (refer to your package’s documentation):\nconda install -n NameForNewEnv -c ChannelName PackageName  Cloning It is possible for you to copy an existing environment into a new environment:\nconda create --name AnotherNameForNewEnv --clone NameForNewEnv  Listing Environments Run the following to get a list of currently installed conda evironments:\nconda env list  Removing If you wish to remove a conda environment run the following:\nconda env remove --name myenv  More Info For more information regarding conda please visit Conda Docs.\nJupyter You can run jupyter as an interactive job using tunneling, or you can use the web portal Jupyter-Hub.\nVirtual Environment In order to enable your conda virtual environemnt within the Jupyter web portal you will need to do the following:\n# Create a virtual environment, if you don't already have one conda create -n ipykernel_py2 python=2 ipykernel # Load the new environment conda activate ipykernel_py2 # Install kernel python -m ipykernel install --user --name myenv --display-name \"JupyterPy2\"  Now when you visit Jupyter-Hub you should see the option “JupyterPy2” when you click the “New” dropdown menu in the upper left corner of the home page.\nR For instructions on how to configure your R environment please visit IRkernel. Since we should already have IRkernel install in the latest version of R, you would only need to do the following within R:\nIRkernel::installspec(name = 'ir44', displayname = 'R 4.0.1')  R This section is regarding how to manage R packages.\nCurrent R Version  NOTE: Please be aware that this version of R is built with GCC/8.3.0, which means that previously compiled modules may be incompatible.\n Currently the default version of R is R/4.1.0 and is NOT loaded automatically for you.\nYou will have to do this manually on your own, like so:\nmodule load R/4.1.0_gcc-8.3.0  Or, you can rebase by loading the base/gcc/8.3.0 module, which will load the latest version of R and many other compatible modules:\nmodule load base/gcc/8.3.0  If you wish to revert back to your previous modules, then you can simply unload the base module, like so:\nmodule unload base  When a new release of R is available, you should reinstall any local R packages, however keep in mind of the following:\n Remove redundantly installed local R packages with the RdupCheck command. Newer version of R packages are not backward compatible, once installed they only work for that specific version of R.  Older R Versions The older version of R/4.0.1 is loaded by default.\nYou can load other versions of R with the following:\nmodule unload R module load R/3.4.2  Installing R Packages The default version of R has many of the most popular R packages already installed and available. It is also possible for you to install additional R packages in your local environment.\nOnly install packages if they are not already available, this will minimize issues later. You can check the current version of R from the command line, like so:\nRscript -e \"library('some-package-name')\"  Or you can check from within R, like so:\nlibrary('some-package-name')  If the package is not available, then proceed with installation.\nBioconductor Packages To install from Bioconductor you can use the following method:\nBiocManager::install(c(\"package-to-install\", \"another-packages-to-install\")) Update all/some/none? [a/s/n]: n  For more information please visit Bioconductor Install Page.\nGitHub Packages # Load devtools library(devtools) # Replace name with the GitHub account/repo install_github(\"duncantl/RGoogleDocs\")  Local Packages # Replace URL with your URL or local path to your .tar.gz file install.packages(\"http://hartleys.github.io/QoRTs/QoRTs_LATEST.tar.gz\",repos=NULL,type=\"source\")  ","excerpt":"Python The scope of this manual is a brief introduction on how to …","ref":"/manuals/hpc_cluster/package_manage/","title":"Package Management"},{"body":"Introduction This page will talk about AWS costs and billing along with setting some controls for them.\nCosts AWS Costs come from each and every type of resources you consume on AWS.\nSome Examples:\n Node Utilization (How many computers and of what type you use for how long) Storage Backend services such as network traffic, security groups, and API transactions. (These costs are relatively very small)  Node Utilization and Storage are the two biggest contributors to cost. So when controlling costs focus here. This can be done by only uploading and downloading the data you need, and deleting your cluster when your not using it. There are also some saving to be realized by choosing the best type of compute node for the type of work you plan on doing. It is easy to change the compute type and HPCC can help you to make the best decision for your work. You can also simply use HPCCs default configuration.\nAs a practical example: (~$0.43 cents per hour to run in default configuration)\n Using the most common cluster configuration - One headnode and one 8 core compute node with 16G Ram. Without using spot pricing. Assuming 25GB of input and/or output data.  HPCC can help with cost projection if you need it.\nAWS Costs Calculator Here is a link to the AWS Costs Calculator .\nThis calculator is very comprehensive and can be difficult to navigate at first. (If you have questions you can ask HPCC)\nBilling Billing is done my Amazon on a monthly basis and is calculated from the previous months usage. This billing is done via PO associated with a FAU.\nAWS Billing \u0026 Cost Management Dashboard is a important webpage to have bookmarked. It will be your main interface for the billing aspects AWS.\nBilling access is allowed for the main AWS account only. If you have sub accounts for lab members all their activity is accumulated and reported back through the main aws account.\nBudgets Controls/Alerts It is possable to configure budget alerts. This allows you to get notified if you are spending or are projected to spend more that you would like.\nThere are many different options to customize.\nBasic and practical control example: This will alert you if you are projected to spend more than $50 in a givien month so you can take action on it.\nBilling Dashboard  Navigate to the AWS Billing \u0026 Cost Management Dashboard All aspects of billing are controlled from here  Click Budget from the left  Budgets  This is the budget control page  Click Create Budget  Define budget and alerts  This is the budget definition page  Fill out the Name field (what ever you would like to call it) Fill out the Period (Monthly is preffered) Fill out the Budgeted Amount (Limit we are interested in, in this example its $50) The “Refine your Budget” section can be left as is Fill out the “Notify me when” fields as shown Fill out your main email address in the “Email contacts” section. Click Create  Budget created  We are back to the budget page  Notice the new budget defined You will now be notified it your speding is projected to exceed your budget Complete  ","excerpt":"Introduction This page will talk about AWS costs and billing along …","ref":"/manuals/ext_cloud/aws/billing/","title":"Cost Control and Billing"},{"body":"Dashboard HPCC cluster users are able to check on their home and bigdata storage usage from the Dashboard Portal.\nHome Home directories are where you start each session on the HPC cluster and where your jobs start when running on the cluster. This is usually where you place the scripts and various things you are working on. This space is very limited. Please remember that the home storage space quota per user account is 20 GB.\n   Path /rhome/username     User Availability All Users   Node Availability All Nodes   Quota Responsibility User    Bigdata Bigdata is an area where large amounts of storage can be made available to users. A lab purchases bigdata space separately from access to the cluster. This space is then made available to the lab via a shared directory and individual directories for each user.\nLab Shared Space This directory can be accessed by the lab as a whole.\n   Path /bigdata/labname/shared     User Availability Labs that have purchased space.   Node Availability All Nodes   Quota Responsibility Lab    Individual User Space This directory can be accessed by specific lab members.\n   Path /bigdata/labname/username     User Availability Labs that have purchased space.   Node Availability All Nodes   Quota Responsibility Lab    Non-Persistent Space Frequently, there is a need for faster temporary storage. For example activities like the following would fall under this category: 1. Output a significant amount of intermediate data during a job 2. Access a dataset from a faster medium than bigdata or the home directories 3. Write out lock files\nThese types of activities are well suited to the use of fast non-persistent spaces. Below are the filesystems available on the HPC cluster that would best suited for these actions.\nTemporary Space This is a standard space available on all Linux systems. Please be aware that it is limited to the amount of free disk space on the node you are running on.\n   Path /tmp     User Availability All Users   Node Availability All Nodes   Quota Responsibility N/A    SSD Backed Space This space is much faster than the persistent space (/rhome,/bigdata), but slower than using RAM based storage.\n   Path /scratch     User Availability All Users   Node Availability All Nodes   Quota Responsibility N/A    RAM Space This type of space takes away from physical memory but allows extremely fast access to the files located on it. When submitting a job you will need to factor in the space your job is using in RAM as well. For example, if you have a dataset that is 1G in size and use this space, it will take at least 1G of RAM.\n   Path /dev/shm     User Availability All Users   Node Availability All Nodes   Quota Responsibility N/A    Usage and Quotas To quickly check your usage and quota limits:\ncheck_quota home check_quota bigdata  To get the usage of your current directory, run the following command:\ndu -sh .  To calculate the sizes of each separate sub directory, run:\ndu -shc *  This may take some time to complete, please be patient.\nFor more information on your home directory, please see the Linux Basics Orientation.\nAutomatic Backups The cluster does create backups however it is still advantageous for users to periodically make copies of their critical data to a separate storage device. The cluster is a production system for research computations with a very expensive high-performance storage infrastructure. It is not a data archiving system.\nHome backups are created daily and kept for one week. Bigdata backups are created weekly and kept for one month.\nHome and bigdata backups are located under the following respective directories:\n/rhome/.snapshots/ /bigdata/.snapshots/  The individual snapshot directories have names with numerical values in epoch time format. The higher the value the more recent the snapshot.\nTo view the exact time of when each snapshot was taken execute the following commands:\nmmlssnapshot home mmlssnapshot bigdata  ","excerpt":"Dashboard HPCC cluster users are able to check on their home and …","ref":"/manuals/hpc_cluster/storage/","title":"Data Storage"},{"body":"Facility description  Facility description (e.g. for grant applications)  Recharging rates  Recharging rates: 2022/2023 Recharging rates: 2021/2022 Recharging rates: 2020/2021 Recharging rates: 2019/2020 Recharging rates: 2018/2019 Recharging rates: 2017/2018 Recharging rates: 2016/2017  External user accounts Accounts for external customers can only be granted if a lab has a strong affiliation with UC Riverside, such as a research collaboration with UCR researchers. Both the corresponding UCR PI and external collaborator need to maintain an HPCC subscription. External accounts are subject to an annual review and approval process. To be approved, the external and internal PIs have to complete this External Usage Justification.\n","excerpt":"Facility description  Facility description (e.g. for grant …","ref":"/about/facility/rates/","title":"Rates"},{"body":"Text Viewing Here are a few commands that are used to just display the text within a file:\nmore \u003cFILENAME\u003e # Views text, use space bar to browse, 'q' to quit less \u003cFILENAME\u003e # Also views text, uses arrow keys to browse, 'q' to quit cat \u003cFILENAME\u003e # Concatenates files and prints content to screen  Text Editors  Nano  A simple terminal-based editor.   Neovim  Non-graphical (terminal-based) editor. Neovim is an improved version of vim.   Vim Gvim  Non-graphical (vim) or window-based editor (gvim). Vim is the improved version of vi.   Emacs  Non-graphical or window-based editor.   Atom  Window-based editor that runs on your local machine.   Visual Studio Code  Graphical editor that runs on your local machine that supports different plugins.    Nano The nano editor is the simplest to use and can be good for beginners:\nnano \u003cFILENAME\u003e # Open file if it exists, or create it  Navigation in nano uses the arrow keys, and all other commands are noted at the bottom of the screen. The CTRL key is used in combination with other keys to execute commands in nano.\nFor example, at the bottom of the nano screen it is noted that ^X is used to exit. This means you will need to hold the CTRL key and then press x in order to quit. After that, just follow the on screen prompts at the bottom.\nFor more nano commands, please visit Overview of nano shortcuts.\nNeovim / Vim / GVim / VI All of these editors follow the same principals.\nnvim \u003cFILENAME\u003e # Open file if it exists, or create it vim \u003cFILENAME\u003e # Open file if it exists, or create it gvim \u003cFILENAME\u003e # Open file if it exists, or create it (must have XForwarding or VNC) vi \u003cFILENAME\u003e # Open file if it exists, or create it  For more information please visit Vim Manual.\nEmacs Navigation in emacs also uses the arrow keys. It is similar to nano, in that, CTRL is combined with other keys to execute commands.\nFor example, to open a file, simply run the command with a file name:\nemacs \u003cFILENAME\u003e # Open file if it exists, or create it  Then, after you have made some changes, exit by holding the CTRL key and then pressing c, releasing and then holding the CTRL key once more and pressing c again. After that, just follow the on screen prompts at the bottom.\nFor more commands in emacs please visit GNU Emacs Reference Card\nAtom Install This editor should be installed on your local machine (ie. workstation, laptop). Please visit Atom for software download.\nRemote Atom After you have atom installed, you need to install the Remote Atom plugin. Click on edit, then preferences, then look for the install item on the left side menu. You should then be able to type remote-atom in the search field, find it and install it. After installation, atom should restart.\nStart Server Once you have remote-atom installed, click Packages in the top menu, then Remote Atom, and then click Start Server. Atom may need to be restarted in order for you to see these new menu items.\nCluster SSH into cluster using a socket (replace \u003cUSERNAME\u003e with your real username on the cluster):\nssh -R /rhome/\u003cUSERNAME\u003e/.rmate.socket:localhost:52698 cluster.hpcc.ucr.edu   Note: Do not use a remote PORT, you must use a SOCKET FILE as shown above. There are security issues otherwise.\n After you have logged into the cluster load rmate (alias is optional):\nmodule load rmate alias ratom=rmate  You can add this into your ~/.bashrc for convenience.\nThen you should be able to open a file on the cluster and have it appear on your local machine:\nrmate \u003cFILENAME\u003e  Once you have finished all your editing and close atom, be sure to delete the socket file from the cluster:\nrm -f /rhome/\u003cUSERNAME\u003e/.rmate.socket'  For more information regarding remote-atom, please visit Remote-Atom.\nVisual Studio Code Install This editor should be installed on your local machine (ie. workstation, laptop). Please visit Visual Studio Code for software download.\nRemote Editing To setup Visual Studio Code to remotely edit files on the cluster, please go to slides 13 on this guide\nRStudio Server Two options exist to access the RStudio Server IDE:\n Shared Web Instance Compute Node Instance  While the Shared Web Instance is easier for less experienced users, it does not allow to load a specific R version nor access more extensive computing resources. Thus, experienced users may prefer the Compute Node Instance as it does not share these limitations.\n1. Shared Web Instance R users can log in to their HPCC accounts via an RStudio Server instance. To do so, visit the HPCC RStudio Server. Next provide your HPCC login credentials and click the Sign In button.\n2. Compute Node Instance a. Interactive Alternatively, an RStudio Server instances can be started directly on a compute node and accessed via an SSH tunnel. This involves the following steps.\n  SSH into the cluster as outlined here.\n  Log in to a compute node interactively via srun, where the proper parameters need to be specified by the user. These include partition name, RAM, number of CPU cores, wall time limit, etc. Additional details on using srun are available here.\n  srun --partition=short --mem=8gb --cpus-per-task=2 --ntasks=1 --time=2:00:00 --pty bash -l  Load the latest versions of R and RStudio Server using the module system:  module unload R module load R/4.1.2 # Or latest version (previously it was R/4.1.1_gcc-8.3.0) module load rstudio-server/2022.02.0-443 # Or latest version (previously it was rstudio-server/2021.09.1-372)  Start RStudio Server:  start-rserver.sh  Next follow the instructions printed to the terminal after running start-rserver.sh. The command-lines given in these instructions need be executed in a terminal of a user’s local system, and not on the remote system where start-rserver.sh was exectuted.  b. Non-Interactive The steps for launching an interactive job can be integrated into a script and submitted non-interactvely for a quicker deployment of a RStudio Server instance on a compute node. Instructions outling how to do this are located here.\n","excerpt":"Text Viewing Here are a few commands that are used to just display the …","ref":"/manuals/linux_basics/text/","title":"Text Editors"},{"body":" ","excerpt":" ","ref":"/about/facility/slides/","title":"Overview slides for HPCC"},{"body":"Permissions It is useful to share data and results with other users on the cluster, and we encourage collaboration The easiest way to share a file is to place it in a location that both users can access. Then the second user can simply copy it to a location of their choice. However, this requires that the file permissions permit the second user to read the file. Basic file permissions on Linux and other Unix like systems are composed of three groups: owner, group, and other. Each one of these represents the permissions for different groups of people: the user who owns the file, all the group members of the group owner, and everyone else, respectively Each group has 3 permissions: read, write, and execute, represented as r,w, and x. For example the following file is owned by the user username (with read, write, and execute), owned by the group groupname (with read and execute), and everyone else cannot access it.\nusername@pigeon:~$ ls -l myFile -rwxr-x--- 1 username groupname 1.6K Nov 19 12:32 myFile  If you wanted to share this file with someone outside the groupname group, read permissions must be added to the file for ‘other’:\nusername@pigeon:~$ chmod o+r myFile  To learn more about ownership, permissions, and groups please visit Linux Basics Permissions.\nSet Default Permissions In Linux, it is possible to set the default file permission for new files. This is useful if you are collaborating on a project, or frequently share files and you do not want to be constantly adjusting permissions The command responsible for this is called ‘umask’. You should first check what your default permissions currently are by running ‘umask -S’.\nusername@pigeon:~$ umask -S u=rwx,g=rx,o=rx  To set your default permissions, simply run umask with the correct options. Please note, that this does not change permissions on any existing files, only new files created after you update the default permissions. For instance, if you wanted to set your default permissions to you having full control, your group being able to read and execute your files, and no one else to have access, you would run:\nusername@pigeon:~$ umask u=rwx,g=rx,o=  It is also important to note that these settings only affect your current session. If you log out and log back in, these settings will be reset. To make your changes permanent you need to add them to your .bashrc file, which is a hidden file in your home directory (if you do not have a .bashrc file, you will need to create an empty file called .bashrc in your home directory). Adding umask to your .bashrc file is as simple as adding your umask command (such as umask u=rwx,g=rx,o=r) to the end of the file. Then simply log out and back in for the changes to take affect. You can double check that the settings have taken affect by running umask -S.\nTo learn more about umask please visit What is Umask and How To Setup Default umask Under Linux?.\nFile Transfers For file transfers and data sharing, both command-line and GUI applications can be used. For beginners we recommend the FileZilla GUI application (download/install from here) since it is available for most OSs. A basic user manual for FileZilla is here and a video tutorial is here. Alternative user-friendly SCP/SFTP GUI applications include Cyberduck and WinSCP for Mac and Windows OSs, respectively.\nFileZilla Usage When using FileZilla you must create a new site to connect, click File -\u003e Site Manager. From the new window click New Site.\nOn the right pane of the General tab fill in the information as follows:\nProtocol: SFTP Host: cluster.hpcc.ucr.edu Logon Type: Interactive User: YOUR_CLUSTER_USER_NAME  Remeber to fill in YOUR_CLUSTER_USER_NAME with your actual cluster username. The Logon Type can be either Interactive or Key File, this depends on if you have setup Password+DUO or SSH Keys respectively.\nIf you choose a Password+DUO authentication, then you should also configure the max connections. Navigate to the Transfer Settings tab and set the following:\n Limit Number of simultaneous connections: checked Maximum number of connections: 1  After all of the above has been completed, then you can click “OK” to save the new site. Then from the main window you can click the arrow next to the site lists, or just reopen the Site Manager and click the “connect” button from your new site window.\nCommand-line SCP Advantages of this method include: batch up/downloads and ease of automation. A detailed manual is available here.\n  To copy files To the server run the following on your workstation or laptop:\nscp -r \u003cpath_to_directory\u003e \u003cyour_username\u003e@\u003chost_name\u003e:\n  To copy files From the server run the following on your workstation or laptop:\nscp -r \u003cyour_username\u003e@\u003chost_name\u003e:\u003cpath_to_directory\u003e .\n  Copying bigdata Rsync can:\n Copy (transfer) directories between different locations Perform transfers over the network via SSH Compare large data sets (-n, --dry-run option) Resume interrupted transfers  Rsync Notes:\n Rsync can be used on Windows, but you must install Cygwin. Most Mac and Linux systems already have rsync install by default. Always put the / after both folder names, e.g: FOLDER_A/ Failing to do so will result in the nesting folders every time you try to resume. If you don’t put / you will get a second folder_B inside folder_B FOLDER_A/FOLDER_A/ Rsync only copies by default. Once the rsync command is done, run it again. The second run will be shorter and can be used as a double check. If there was no output from the second run then nothing changed. To learn more try man rsync  If you are transfering to, or from, your laptop/workstation it is required that you run the rsync command locally from your laptop/workstation.\nTo transfer to the cluster:\nrsync -av --progress FOLDER_A/ cluster.hpcc.ucr.edu:FOLDER_A/  To transfer from the cluster:\nrsync -av --progress cluster.hpcc.ucr.edu:FOLDER_A/ FOLDER_A/  Rsync will use SSH and will ask you for your cluster password, the same way SSH or SCP does.\nIf your rsync transer was interrupted, rsync can continue where it left off. Simply run the same command again to resume.\nCopying large folders on the cluster between Directories If you want to syncronize the contents from one directory to another, then use the following:\nrsync -av --progress PATH_A/FOLDER_A/ PATH_B/FOLDER_A/  Rsync does not move but only copies. Thus you would need to delete the original once you confirm that everything has been transfered.\nCopying large folders between the cluster and other servers If you want to copy data from the cluster to your own server, or another remote system, use the following:\nrsync -ai FOLDER_A/ sever2.xyz.edu:FOLDER_A/  The sever2.xyz.edu machine must be a server that accepts Rsync connections via SSH.\nSharing Files on the Web Simply create a symbolic link or move the files into your html directory when you want to share them. For exmaple, log into the HPC cluster and run the following:\n# Make sure you have an html directory mkdir ~/.html #Make sure permissions are set correctly chmod a+x ~/ chmod a+rx ~/.html # Make a new web project directory mkdir www-project chmod a+rx www-project # Create a default test file echo '\u003ch1\u003eHello!\u003c/h1\u003e' \u003e ~/www-project/index.html # Create shortcut/link for new web project in html directory ln -s ~/www-project ~/.html/  Now, test it out by pointing your web-browser to https://cluster.hpcc.ucr.edu/~username/www-project/ Be sure to replace username with your actual user name.\nPassword Protect Web Pages Files in web directories can be password protected. First create a password file and then create a new user:\ntouch ~/.html/.htpasswd htpasswd ~/.html/.htpasswd newwebuser  This will prompt you to enter a password for the new user ‘newwebuser’. Create a new directory, or go to an existing directory, that you want to password protect:\nmkdir ~/.html/locked_dir cd ~/.html/locked_dir  For the above commands you can choose any directory name you want.\nThen place the following content within a file called .htaccess:\nAuthName 'Please login' AuthType Basic AuthUserFile /rhome/username/.html/.htpasswd require user newwebuser  Now, test it out by pointing your web-browser to http://cluster.hpcc.ucr.edu/~username/locked_dir Be sure to replace username with your actual user name for the above code and URL.\nGoogle Drive There are several tools used to transfer files from Google Drive to the cluster, however RClone may be the easiest to setup.\n  Create an SSH tunnel to the cluster, (MS Windows users should use MobaXterm):\nssh -L 53682:localhost:53682 username@cluster.hpcc.ucr.edu    Once you have logged into the cluster with the above command, then load rclone via the module system and run it, like so:\nmodule load rclone rclone config    After that, follow this RClone Walkthrough to complete your setup.\n  ","excerpt":"Permissions It is useful to share data and results with other users on …","ref":"/manuals/hpc_cluster/sharing/","title":"Sharing Data"},{"body":"Streams On the command line, or terminal, there are three very important lanes where information can be sent, we call these streams. A single command can take information in from STDIN and then send information out on both STDOUT and STDERR simultaneously.\nSTDIN For example, we can send the contents of a file as a STDIN steam to the wc command in order to count the lines:\nwc -l \u003c file.txt  STDOUT The STDOUT steam is probably the most often used, since this is how commands send information to the screen. However, if we do not want the information printed to the screen, we can send it into a file for later review:\nls \u003e output.txt # Overwrite contents in output file with `ls` results  You can also append to the same file, if more information is to be saved:\nls \u003e\u003e output.txt # Append results from `ls` to the bottom of the file  STDERR The error stream is very useful to separate error messages (or warnings) from real output (your results). Since there is no -e flag for the ls command this will generate an error. We can then store this error in a by redirecting the error stream with 2\u003e.\nls -e 2\u003e errors.txt  Tips Combined streams If you want to combined your STDOUT with your STDERR stream and store it into a file, you can do this with \u0026\u003e, like so:\ncommand \u0026\u003e output_and_errors.txt  Trash Streams If you want to ignore all information from STDOUT and STDERR you can send both of these streams to the trash (/dev/null):\ncommand \u0026\u003e /dev/null  This can be useful when you are only interested in the resulting file that your command will create.\n","excerpt":"Streams On the command line, or terminal, there are three very …","ref":"/manuals/linux_basics/streams/","title":"Streams"},{"body":"Protection Levels and Classification UCR protection levels, and data classifications are outlined by UCOP as a UC wide policy: UCOP Institutional Information and IT Resource Classification According to the above documentation, there are 4 levels of protection for 4 classifications of data:\n   Protection Level Policy Examples     P1 - Minimal IS-1 Internet facing websites, press releases, anything intended for public use   P2 - Low IS-2 Unpublished research work, intellectual property NOT classified as P3 or P4   P3 - Moderate IS-3 Research information classified by an Institutional Review Board as P3 (ie. dbGaP from NIH)   P4 - High IS-4 Protected Health Information (PHI/HIPAA), patient records, sensitive identifiable human subject research data, Social Security Numbers    The HPC cluster could be compliant with with other security polices (ie. NIH), however the policy must be reviewed by our security team.\nAt this time the HPC cluster is not a IS-4 (P4) compliant cluster. If you have needs for very sensitive data, it may be best to work with UCSD and their Sherlock service. Our cluster is IS-3 compliant, however there are several responsibilities that users will need to adhere to.\nGeneral Guidelines First, please contact us (support@hpcc.ucr.edu) before transferring any data to the cluster. After we have reviewed your needs, data classification and appropriate protection level, then it may be possible to proceed to use the HPCC.\nHere are a few basic rules to keep in mind:\n Always be aware of access control methods (Unix permissions and ACLs), do not allow others to view the data (ie. chmod 400 filename) Do not make unnecessary copies of the data Do not transfer the data to insecure locations Encrypt data when/where possible Delete all data when it is no longer needed  Access Controls When sharing files with others, it is imperative that proper permission are used. However, basic Unix permissions (user,group,other) may not be adequate. It is better to use ACLs in order to allow fine grained access to sensitive files.\nGPFS ACLs GPFS is used for most of our filesystems (/rhome and /bigdata) and it uses nfsv4 style ACLs. Users are able to explicitly allow many individuals, or groups, access to specific files or directories.\n# Get current permissions and store in acls file mmgetacl /path/to/file \u003e ~/acls.txt # Edit acls file containing permissions vim ~/acls.txt # Apply new permissions to file mmputacl -i ~/acls.txt /path/to/file # Delete acls file rm ~/acls.txt  For more information regarding GPFS ACLs refer to the following: GPFS ACLs\nXFS ACLs The XFS filesystem is used for the CentOS operating system and typical unix locations (/,/var,/tmp,etc), as well as /secure. For more information on how to use ACLs under XFS, please refer to the following: CentOS 7 XFS\n Note: ACLs are not applicable to gocryptfs, which is a FUSE filesystem, not GPFS nor XFS.\n Encryption Under the IS-3 policy, P3 data encryption is mandatory. It is best if you get into the habit of doing encryption in transit, as well as encryption at rest. This means, when you move the data (transit) or when the data is not in use (rest), it should be encrypted.\nIn Transit When transferring files make sure that files are encrypted in flight with one of the following transfer protocols:\n SCP SFTP RSYNC (via SSH)  The destination for sensitive data on the cluster must also be encrypted at rest under one of the follow secure locations:\n /dev/shm/ - This location is in RAM, so it does not exist at rest (ensure proper ACLs) /secure - This location is encrypted at rest with AES 256 key length (ensure proper ACLs) /run/user/$EUID/unencrypted - This location is manually managed, and should be created for access to unencrypted files.  It is also possible to encrypt your files with GPG (GPG Example), before they are transferred. Thus, during transfer they will be GPG encrypted. However, decryption must occur in one of the secure locations mentioned above.\n Note: Never store passphrases/passwords/masterkeys in an unsecure location (ie. a plain text script under /rhome).\n At Rest There are 3 methods available on the cluster for encryption at rest:\n GPG encryption of files via the command line GPG Example, however you must ensure proper ACLs and decryption must occur in a secure location. The location “/secure” is encrypted and is mounted on the head nodes, however you must ensure proper ACLs. Create your own location with gocryptfs.  GocryptfsMgr You can use gocryptfs directly or use the gocryptfsmgr, which automates a few steps in order to simplify things.\nHere are the basics when using gocryptfsmgr:\n# Create new encrypted data directory gocryptfsmgr create bigdata privatedata1 # List all encrypted and unencrypted (access point) directories gocryptfsmgr list # Unencrypted privatedata1 (create access point) gocryptfsmgr open bigdata privatedata1 rw # Transfer files (ie. SCP,SFTP,RSYNC) scp user@remote-server:sensitive_file.txt $UNENCRYPTED/privatedata1/sensitive_file.txt # Remove access point (re-encrypt) privatedata1 gocryptfsmgr close privatedata1 # Remove all access points (re-encrypt all) gocryptfsmgr quit  For subsequent access to the encrypted space, (ie. computation or analysis) the follow procedure is recommended:\n# Request a 2hr interactive job on an exclusive node, resources can be adjusted as needed srun -p short --exclusive=user --pty bash -l # Unencrypted privatedata1 in read-only mode (create access point) gocryptfsmgr open bigdata privatedata1 ro # Read file contents from privatedata1 (simulating work or analysis) cat $UNENCRYPTED/privatedata1/sensitive_file.txt # List all encrypted and unencrypted (access points) directories gocryptfsmgr list # Make sure we re-encrypt (close access point) for privatedata1 gocryptfsmgr close privatedata1 # Exit from interactive job exit  With the above methods you can create multiple encrypted directories and access points and move between them.\nGocryptfs When using the gocryptfs directly, you will need to know a bit more details on how it works. The gocryptfs module on the HPCC cluster uses these predefined variables:\n HOME_ENCRYPTED = /rhome/$USER/encrypted - Very small encrypted space, not recommended to use BIGDATA_ENCRYPTED = /rhome/$USER/bigdata/encrypted - Best encrypted space for private data sets SHARED_ENCRYPTED = /rhome/$USER/shared/encrypted - Encrypted space when intending to share data sets with group UNENCRYPTED = /run/user/$UID/unencrypted - Access directory where encrypted data will be viewed as unencrypted  Here is an example how to create an encrypted directory under the BIGDATA_ENCRYPTED location using gocryptfs:\n# Load gocyptfs software module load gocryptfs # Create empty data directory mkdir -p $BIGDATA_ENCRYPTED/privatedata1 # Then intialize empty directory and encrypt it gocryptfs -aessiv -init $BIGDATA_ENCRYPTED/privatedata1 # Create access point directory where encrypted files will be viewed as unencrypted mkdir -p $UNENCRYPTED/privatedata1 # After that mount the encrypted directory on the access point and open a new shell within it gocryptfssh $BIGDATA_ENCRYPTED/privatedata1 # Transfer files (ie. SCP,SFTP,RSYNC) scp user@remote-server:sensitive_file.txt $UNENCRYPTED/sensitive_file.txt # Exiting this shell will automatically unmount the unencrypted directory exit  For subsequent access to the encrypted space, (ie. computation or analysis) the follow procedure is recommended:\n# Request a 2hr interactive job on an exclusive node, resources can be adjusted as needed srun -p short --exclusive=user --pty bash -l # Load cyptfs software module load gocryptfs # Create unencrypted directory mkdir -p $UNENCRYPTED/privatedata1 # Mount encrypted filesystem as read-only and unmount idling for 1 hour gocryptfs -ro -i 1h -sharedstorage $BIGDATA_ENCRYPTED/privatedata1 $UNENCRYPTED/privatedata1 # Read file contents (simulating work or analysis) cat $UNENCRYPTED/privatedata1/sensitive_file.txt # Manually close access point when analysis has completed fusermount -u $UNENCRYPTED/privatedata1 # Delete old empty access point rmdir $UNENCRYPTED/privatedata1   WARNING: Avoid writing to the same file at the same time from different nodes. The encrypted file system cannot handle simultaneous writes and will corrupt the file. If simultaneous jobs are necessary then using write mode from a head node and read-only mode from compute nodes may be the best solution here. Also, be mindful of reamaining job time and make sure that you have unmounted the unencrypted directories before your job ends.\n For another example on how to use gocrypfs on an HPC cluster: Luxembourg HPC gocryptfs Example\nDeletion To ensure the complete removal of data, it is best to shred files instead of removing them with rm. The shred program will overwrite the contents of a file with randomized data such that recovery of this file will be very difficult, if not impossible.\nInstead of using the common rm command to delete something, please use the shred command, like so:\nshred -u somefile  The above command will overwrite the file with random data, and then remove (unlink) it.\nIf we want to be even more secure, we can pass over the file seven times to ensure that reconstruction is nearly impossible, then remove it:\nshred -v -n 6 -z -u somefile  ","excerpt":"Protection Levels and Classification UCR protection levels, and data …","ref":"/manuals/hpc_cluster/security/","title":"Security"},{"body":"Facility  Thomas Girke, Director of HPC Center Jordan Hayes, Sr. HPC Systems Administrator Viet Pham, HPC System Administrator Melody Asghari, HPC Systems Administrator, Assistant  Advisory Board (executive committee) The responsibilities of the Advisory Board are outlined here.\n  UCR faculty members with hands-on experience in HPC\n Stefano Lonardi (CS) Wenxiu Ma (Statistics) Laura Sales (Physics) Leonard Mueller (Chemistry) David Kisailus (Chemical and Environmental Engineering) Jason E Stajich (IIGB)    HPC expert staff members from UCR\n Keith Richards-Dinger (Earth Sciences) Victor Hill (CS) Bill Strossman (C\u0026C)    External members from academia and industry\n One of each to be added here.    ","excerpt":"Facility  Thomas Girke, Director of HPC Center Jordan Hayes, Sr. HPC …","ref":"/about/facility/people/","title":"People"},{"body":"Piping One of the the most powerful things you can do in Linux is piping. This allows chaining of commands so that the output (STDOUT) of one command is the input (STDIN) for another. This is done by placing a | (pipe) character between the commands. Please note that not all commands support this, for example if your command is not taking input from STDIN.\nAs an example, let’s collect all the lines where pattern is found in a file, then count how many lines were found:\ngrep 'pattern' filename | wc -l  You can pipe as many commands together as you like, not just two. For example, you can combined two CSV files and extract the first column, then filter for only unique values:\ncat filename1.csv filename2.csv | cut -f 1 | sort | uniq  For a few more simple examples, please visit here Pipe, Grep and Sort Command in Linux/Unix with Examples. Or you can try searching Google for even more complex examples, the possibilities are endless.\n","excerpt":"Piping One of the the most powerful things you can do in Linux is …","ref":"/manuals/linux_basics/pipes/","title":"Piping"},{"body":"Communicating with others The cluster is a shared resource, and communicating with other users can help to schedule large computations.\nLooking-Up Specific Users\nA convenient overview of all users and their lab affiliations can be retrieved with the following command:\nuser_details.sh  You can search for specific users by running:\nMATCH1='username1' # Searches by real name, and username, and email address and PI name MATCH2='username2' user_details.sh | grep -P \"$MATCH1|$MATCH2\"  Listing Users with Active Jobs on the Cluster To get a list of usernames:\nsqueue --format '%u' | sort | uniq  To get the list of real names:\ngrep \u003c(user_details.sh | awk '{print $2,$3,$4}') -f \u003c(squeue --format '%u' --noheader | sort | uniq) | awk '{print $1,$2}'  To get the list of emails:\ngrep \u003c(user_details.sh | awk '{print $4,$5}') -f \u003c(squeue --format '%u' --noheader | sort | uniq) | awk '{print $2}'  ","excerpt":"Communicating with others The cluster is a shared resource, and …","ref":"/manuals/hpc_cluster/users/","title":"Communicating"},{"body":"Variables The HPCC cluster uses bash as the default shell environment. Within this environment, variables can be set and reused.\nFor example:\nMYVAR=’Something’ export MYVAR=’Something’ echo $MYVAR  Default Variables Some softwares utilize this feature and require that specific environment variables be set. For example, every time you login, the following variables are set by default:\necho $HOME #Contains your home path echo $USER #Contains your username echo $PATH #Contains paths of executables echo $LD_LIBRARY_PATH #Contains paths of library dependencies  Finding Variables To see a list of all variables currently set in your shell, use the env command. You can also grep through this list to find variables, like so:\nenv | grep -i home  Or if you are in a Slurm job, you can find all related Slurm variables:\nenv | grep -i slurm  Setting variables Try to choose unique names when setting variables. It is best to not overwrite a variable that is already set, unless on purpose.\nTo set a variable in your current shell, you can do so like this:\nMYVAR='Something Important'   Notice that there is no spaces around the = sign.\n If you would like to set a variable that is carried over to all other commands or sub-shells, then it must be exported:\nexport MYVAR='Something Important'  ","excerpt":"Variables The HPCC cluster uses bash as the default shell environment. …","ref":"/manuals/linux_basics/variables/","title":"Variables"},{"body":"Scripting Converting code into a script is useful, and almost necessary when running jobs on the cluster.\nThere are many benifits of doing this:\n Easy to run - blackbox Easy to maintain - consolidated code Easy to distribute - capsulated code Easy to automate (crontab?) - does not require interaction  Breakdown There are four basic parts that are needed to convert your commands into a script:\n  First save all your commands into a file and call it myscript.sh, you can do this with a Text Editor or Transferring it from your computer.\n  Add the #! (SheBang) as the first line in file, this defines the interpreter. In this example, we are using bash as the interpreter, which will run all subsequent lines in this file:\n  #!/bin/bash  Add the proper permissions to the script, allow user (or group) execution:  chmod u+x myscript.sh  OR\nchmod g+x myscript.sh  You can pass arguments via command line into a script, this step is optional, but important to note.  For example if I want to call my script, like so:\n/where/my/script/lives/myscript.sh username number  Then inside my script I can capture the command line arguments into variables, like this:\nusername=$1 number=$2  Lastly adding the path to a script to the PATH environment variable, allows us to call the script without a prefixed path:  export PATH=/where/my/script/lives/:$PATH # Can be added to .bashrc for convenience  After we have exported PATH with the new path of our script, we call it like so:\nmyscript.sh username number  Walkthrough My bash commands:\nsacct -n -p -u jhayes -S 2020-01-01 -l \u003e myjobs.txt cut -d'|' -f4 myjobs.txt \u003e partitions.txt wc -l partitions.txt \u003e count.txt  Convert the above commands into a script named myscript.sh, with the following contents:\n#!/bin/bash # Gather Slurm job information sacct -n -p -u jhayes -S 2020-01-01 -l \u003e myjobs.txt # Filter on parittion column cut -d'|' -f4 myjobs.txt \u003e partitions.txt # Count how many records per partition cat partitions.txt | sort | uniq -c \u003e count.txt  Optional, we can alter the above commands by adding some pipes, as well as adding some variables to make this script count records for only a given partition:\n#!/bin/bash -l # Gather Slurm job information # filter on partition column # count how many records for given partition sacct -n -p -u $1 -S 2020-01-01 -l | cut -d'|' -f4 | grep $2 | wc -l \u003e count.txt  Add correct permissions:\nchmod u+x myscript.sh  Add to my PATH:\nmkdir -p ~/bin mv myscript.sh ~/bin export PATH=~/bin:$PATH  Now run my new script:\nmyscript.sh  Or, if we did the optional step of adding variables, we can do this:\nmyscript.sh johndoe001 intel # Arguments are \u003cUSERNAME\u003e and \u003cPARTITION\u003e  ","excerpt":"Scripting Converting code into a script is useful, and almost …","ref":"/manuals/linux_basics/scripting/","title":"Scripting"},{"body":"Terminal IDEs This page introduces several terminal-based working environments available on UCR’s HPC cluster that are useful for a variety of computer languages.\nVim/Nvim Basics To work efficiently on remote systems like a computer cluster, it is essential to learn how to work in a pure command-line interface. GUI environments like RStudio and similar coding environments are not suitable for this. In addition, there is a lot of value of knowing how to work in an environment that is not restricted to a specific programming language. Therefore, for working on remote systems like HPCC Cluster, this site focuses on Nvim and Tmux. Both are useful for many programming languages. Combinded with the nvim-r plugin they also provide a powerful command-line working environment for R. Users of Emacs may want to consider using ESS instead. The following provides a brief introduction to the Nvim-R-Tmux environment.\nVim overview The following opens a file (here myfile) with nvim (or vim)\nnvim myfile.txt # for neovim (or 'vim myfile.txt' for vim)  Once you are in Nvim, there are three main modes: normal, insert and command mode. The most important commands for switching between the three modes are:\n i: The i key brings you from the normal mode to the insert mode. The latter is used for typing. Esc: The Esc key brings you from the insert mode back to the normal mode. :: The : key starts the command mode at the bottom of the screen.  Use the arrow keys to move your cursor in the text. Using Fn Up/Down key allows to page through the text quicker. In the following command overview, all commands starting with : need to be typed in the command mode. All other commands are typed in the normal mode after pushing the Esc key.\nImportant modifier keys to control vim/nvim\n :w: save changes to file. If you are in editing mode you have to hit Esc first. :q: quit file that has not been changed :wq: save and quit file :!q: quit file without saving any changes  Useful resources for learning vim/nvim  Interactive Vim Tutorial Official Vim Documentation HPCC Linux Manual  For R: nvim-R Basics Tmux is a terminal multiplexer that allows to split terminal windows and to detach/reattach to existing terminal sessions. Combinded with the nvim-r plugin it provides a powerful command-line working environment for R where users can send code from a script to the R console or command-line. Both tmux and the nvim-r plugin need to be installed on a system. On HPCC Cluster both are configured in each user account. If this is not the case then follow the quick configuration instructions given in the following subsection.\n Nvim-R IDE for R Quick configuration in user accounts Skip these steps if Nvim-R-Tmux is already configured in your account. Or follow the detailed instructions to install Nvim-R-Tmux from scratch on your own system.\n Log in to your user account on HPCC and execute Install_Nvim-R_Tmux (old: install_nvimRtmux). Alternatively, follow these step-by-step install commands. To enable the nvim-R-tmux environment, log out and in again. Follow usage instructions of next section.  Basic usage of Nvim-R-Tmux The official and much more detailed user manual for Nvim-R is available here. The following gives a short introduction into the basic usage of Nvim-R-Tmux:\n1. Start tmux session (optional)\nNote, running Nvim from within a tmux session is optional. Skip this step if tmux functionality is not required (e.g. reattaching to sessions on remote systems).\ntmux # starts a new tmux session tmux a # attaches to an existing session  2. Open nvim-connected R session\nOpen a *.R or *.Rmd file with nvim and intialize a connected R session with \\rf. This command can be remapped to other key combinations, e.g. uncommenting lines 10-12 in .config/nvim/init.vim will remap it to the F2 key. Note, the resulting split window among Nvim and R behaves like a split viewport in nvim or vim meaning the usage of Ctrl-w w followed by i and Esc is important for navigation.\nnvim myscript.R # or *.Rmd file  3. Send R code from nvim to the R pane\nSingle lines of code can be sent from nvim to the R console by pressing the space bar. To send several lines at once, one can select them in nvim’s visual mode and then hit the space bar. Please note, the default command for sending code lines in the nvim-r-plugin is \\l. This key binding has been remapped in the provided .config/nvim/init.vim file to the space bar. Most other key bindings (shortcuts) still start with the \\ as LocalLeader, e.g. \\rh opens the help for a function/object where the curser is located in nvim. More details on this are given below.\nImportant keybindings for nvim The main advantages of Neovim compared to Vim are its better performance and its built-in terminal emulator facilitating the communication among Neovim and interactive programming environments such as R. Since the Vim and Neovim environments are managed independently, one can run them in parallel on the same system without interfering with each other. The usage of Neovim is almost identical to Vim.\nNvim commands\n \\rf: opens vim-connected R session. If you do this the first time in your user account, you might be asked to create an R directory under ~/. If so approve this action by pressing y. spacebar: sends code from vim to R; here remapped in init.vim from default \\l :split or :vsplit: splits viewport (similar to pane split in tmux) gz: maximizes size of viewport in normal mode (similar to Tmux’s Ctrl-a z zoom utility) Ctrl-w w: jumps cursor to R viewport and back; toggle between insert (i) and command (Esc) mode is required for navigation and controlling the environment. Ctrl-w r: swaps viewports Ctrl-w =: resizes splits to equal size :resize \u003c+5 or -5\u003e: resizes height by specified value :vertical resize \u003c+5 or -5\u003e: resizes width by specified value Ctrl-w H or Ctrl-w K: toggles between horizontal/vertical splits Ctrl-spacebar: omni completion for R objects/functions when nvim is in insert mode. Note, this has been remapped in init.vim from difficult to type default Ctrl-x Ctrl-o. :h nvim-R: opens nvim-R’s user manual; navigation works the same as for any Vim/Nvim help document :Rhelp fct_name: opens help for a function from nvim’s command mode with text completion support Ctrl-s and Ctrl-x: freezes/unfreezes vim (some systems)  Important keybindings for tmux Pane-level commands\n Ctrl-a %: splits pane vertically Ctrl-a \": splits pane horizontally Ctrl-a o: jumps cursor to next pane Ctrl-a Ctrl-o: swaps panes Ctrl-a \u003cspace bar\u003e: rotates pane arrangement Ctrl-a Alt \u003cleft or right\u003e: resizes to left or right Ctrl-a Esc \u003cup or down\u003e: resizes to left or right  Window-level comands\n Ctrl-a n: switches to next tmux window Ctrl-a Ctrl-a: switches to previous tmux window Ctrl-a c: creates a new tmux window Ctrl-a 1: switches to specific tmux window selected by number  Session-level comands\n Ctrl-a d: detaches from current session Ctrl-a s: switch between available tmux sesssions $ tmux new -s \u003cname\u003e: starts new session with a specific name $ tmux ls: lists available tmux session(s) $ tmux attach -t \u003cid\u003e: attaches to specific tmux session $ tmux attach: reattaches to session $ tmux kill-session -t \u003cid\u003e: kills a specific tmux session Ctrl-a : kill-session: kills a session from tmux command mode that can be initiated with Ctrl-a :  For Bash, Python and other languages Basics For languages other than R one can use the vimcmdline plugin for nvim (or vim). Supported languages include Bash, Python, Golang, Haskell, JavaScript, Julia, Jupyter, Lisp, Macaulay2, Matlab, Prolog, Ruby, and Sage. The nvim terminal also colorizes the output, as in the screenshot below, where different colors are used for general output, positive and negative numbers, and the prompt line.\n vimcmdline Install To install it, one needs to copy from the vimcmdline resository the directories ftplugin, plugin and syntax and their files to ~/.config/nvim/. For user accounts of UCR’s HPCC, the above install script Install_Nvim-R_Tmux (old: install_nvimRtmux) includes the install of vimcmdline (since 09-Jun-18).\nUsage The usage of vimcmdline is very similar to nvim-R. To start a connected terminal session, one opens with nvim a code file with the extension of a given language (e.g. *.sh for Bash or *.py for Python), while the corresponding interactive interpreter session is initiated by pressing the key sequence \\s (corresponds to \\rf under nvim-R). Subsequently, code lines can be sent with the space bar. More details are available here.\n","excerpt":"Terminal IDEs This page introduces several terminal-based working …","ref":"/manuals/hpc_cluster/terminalide/","title":"Terminal-based Working Environments"},{"body":"Overview R provides a variety of packages for parallel computations. One of the most comprehensive parallel computing environments for R is batchtools (formerly BatchJobs). It supports both multi-core and multi-node computations with and without schedulers. By making use of cluster template files, most schedulers and queueing systems are also supported (e.g. Torque, Sun Grid Engine, Slurm).\nR code of this section To simplify the evaluation of the R code of this page, the corresponding text version is available for download from here.\nParallelization with batchtools The following introduces the usage of batchtools for a computer cluster using SLURM as scheduler (workload manager).\nSet up working directory for SLURM First login to your cluster account, open R and execute the following lines. This will create a test directory (here mytestdir), redirect R into this directory and then download the required files:\n slurm.tmpl .batchtools.conf.R  dir.create(\"mytestdir\") setwd(\"mytestdir\") download.file(\"https://bit.ly/3Oh9dRO\", \"slurm.tmpl\") download.file(\"https://bit.ly/3KPBwou\", \".batchtools.conf.R\")  Load package and define some custom function This is the test function (here toy example) that will be run on the cluster for demonstration purposes. It subsets the iris data frame by rows, and appends the host name and R version of each node where the function was executed. The R version to be used on each node can be specified in the slurm.tmpl file (under module load).\nlibrary('RenvModule') module('load','slurm') # Loads slurm among other modules library(batchtools) myFct \u003c- function(x) { result \u003c- cbind(iris[x, 1:4,], Node=system(\"hostname\", intern=TRUE), Rversion=paste(R.Version()[6:7], collapse=\".\")) }  Submit jobs from R to cluster The following creates a batchtools registry, defines the number of jobs and resource requests, and then submits the jobs to the cluster via SLURM.\nreg \u003c- makeRegistry(file.dir=\"myregdir\", conf.file=\".batchtools.conf.R\") Njobs \u003c- 1:4 # Define number of jobs (here 4) ids \u003c- batchMap(fun=myFct, x=Njobs) done \u003c- submitJobs(ids, reg=reg, resources=list(partition=\"short\", walltime=60, ntasks=1, ncpus=1, memory=1024)) waitForJobs() # Wait until jobs are completed  Summarize job status After the jobs are completed one instect their status as follows.\ngetStatus() # Summarize job status showLog(Njobs[1]) # killJobs(Njobs) # # Possible from within R or outside with scancel  Access/assemble results The results are stored as .rds files in the registry directory (here myregdir). One can access them manually via readRDS or use various convenience utilities provided by the batchtools package.\nreadRDS(\"myregdir/results/1.rds\") # reads from rds file first result chunk loadResult(1) lapply(Njobs, loadResult) reduceResults(rbind) # Assemble result chunks in single data.frame do.call(\"rbind\", lapply(Njobs, loadResult))  Remove registry directory from file system By default existing registries will not be overwritten. If required one can exlicitly clean and delete them with the following functions.\nclearRegistry() # Clear registry in R session removeRegistry(wait=0, reg=reg) # Delete registry directory # unlink(\"myregdir\", recursive=TRUE) # Same as previous line  Load registry into R Loading a registry can be useful when accessing the results at a later state or after moving them to a local system.\nfrom_file \u003c- loadRegistry(\"myregdir\", conf.file=\".batchtools.conf.R\") reduceResults(rbind)  ","excerpt":"Overview R provides a variety of packages for parallel computations. …","ref":"/manuals/hpc_cluster/parallelr/","title":"Parallel Evaluations in R"},{"body":"Process Management Basic Linux process management commands only apply to processes that are running on the current machine you are logged into. This means that you cannot use these commands to manage jobs. Jobs on the cluster are managed through Slurm, see Cluster Jobs for more details. However, these commands are still useful for pausing, backgrounding, killing processes on a login node directly. This commands could also be useful when running an interactive session on a compute node.\nUser Management top # view top consumers of memory and CPU (press 1 to see per-CPU statistics) who # Shows who is logged into system w # Shows which users are logged into system and what they are doing  Process Management Processes ps # Shows processes running by user ps -e # Shows all processes on system; try also '-a' and '-x' arguments ps ux -u \u003cUSERNAME\u003e # Shows all processes owned by user ps axjf # Shows the child-parent hierarchy of all processes ps -o %t -p \u003cPID\u003e # Shows how long a particular process was running. # (E.g. 6-04:30:50 means 6 days 4 hours ...)  Here are two common utilities for displaying processes, sorting, and even killing them:\ntop # Basic text based interface for exploring and managing processes htop # Text based interface for exploring and managing processes   Note q to quit and ? to see help\n Background Resume Cancel CTRL+z ENTER # Suspend a process in the background fg # Resume a suspended process and brings it into foreground bg # Resume a suspended process but keeps it running in the background CTRL+c # Cancel the process that is currently running in the foreground  PID echo $! # Get PID of last executed command  Killing kill -l # List all of the signals that can be sent to a process kill \u003cPID\u003e # Kill a specific process with process ID using SIGTERM kill -9 \u003cPID\u003e # Violently kill process with process ID using SIGKILL, may corrupt files  More on Terminating Processes DigitalOcean - How To Use ps, kill, and nice to Manage Processes in Linux\n","excerpt":"Process Management Basic Linux process management commands only apply …","ref":"/manuals/linux_basics/processes/","title":"Process Management"},{"body":"The Unix Shell When you log into UNIX/LINUX system, then is starts a program called the Shell. It provides you with a working environment and interface to the operating system. Usually there are several different shell programs installed. The shell program bash is one of the most common ones.\nfinger \u003cuser_name\u003e # shows which shell you are using chsh -l # gives list of shell programs available on your system (does not work on all UNIX variants) \u003cshell_name\u003e # switches to different shell  STDIN, STDOUT, STDERR, Redirections, and Wildcards See LINUX HOWTOs\nBy default, UNIX commands read from standard input (STDIN) and send their output to standard out (STDOUT).\nYou can redirect them by using the following commands:\n\u003cbeginning-of-filename\u003e* # * is wildcard to specify many files ls \u003e file # prints ls output into specified file command \u003c my_file # uses file after '\u003c' as STDIN command \u003e\u003e my_file # appends output of one command to file command | tee my_file # writes STDOUT to file and prints it to screen command \u003e my_file; cat my_file # writes STDOUT to file and prints it to screen command \u003e /dev/null # turns off progress info of applications by redirecting # their output to /dev/null grep my_pattern my_file | wc # Pipes (|) output of 'grep' into 'wc' grep my_pattern my_non_existing_file 2 \u003e my_stderr # prints STDERR to file  Useful shell commands cat \u003cfile1\u003e \u003cfile2\u003e \u003e \u003ccat.out\u003e # concatenate files in output file 'cat.out' paste \u003cfile1\u003e \u003cfile2\u003e \u003e \u003cpaste.out\u003e # merges lines of files and separates them by tabs (useful for tables) cmp \u003cfile1\u003e \u003cfile2\u003e # tells you whether two files are identical diff \u003cfileA\u003e \u003cfileB\u003e # finds differences between two files head -\u003cnumber\u003e \u003cfile\u003e # prints first lines of a file tail -\u003cnumber\u003e \u003cfile\u003e # prints last lines of a file split -l \u003cnumber\u003e \u003cfile\u003e # splits lines of file into many smaller ones csplit -f out fasta_batch \"%^\u003e%\" \"/^\u003e/\" \"{*}\" # splits fasta batch file into many files # at '\u003e' sort \u003cfile\u003e # sorts single file, many files and can merge (-m) # them, -b ignores leading white space, ... sort -k 2,2 -k 3,3n input_file \u003e output_file # sorts in table column 2 alphabetically and # column 3 numerically, '-k' for column, '-n' for # numeric sort input_file | uniq \u003e output_file # uniq command removes duplicates and creates file/table # with unique lines/fields join -1 1 -2 1 \u003ctable1\u003e \u003ctable2\u003e # joins two tables based on specified column numbers # (-1 file1, 1: col1; -2: file2, col2). It assumes # that join fields are sorted. If that is not the case, # use the next command: sort table1 \u003e table1a; sort table2 \u003e table2a; join -a 1 -t \"$(echo -e '\\t')\" table1a table2a \u003e table3 # '-a \u003ctable\u003e' prints all lines of specified table! # Default prints only all lines the two tables have in # common. '-t \"$(echo -e '\\t')\" -\u003e' forces join to # use tabs as field separator in its output. Default is # space(s)!!! cat my_table | cut -d , -f1-3 # cut command prints only specified sections of a table, # -d specifies here comma as column separator (tab is # default), -f specifies column numbers. grep # see chapter 4 egrep # see chapter 4  Screen Screen references\n Screen Turorial Screen Cheat Sheet  Starting a New Screen Session screen # Start a new session screen -S \u003csome-name\u003e # Start a new session and gives it a name  Commands to Control Screen\nCtrl-a d # Detach from the screen session Ctrl-a c # Create a new window inside the screen session Ctrl-a Space # Switch to the next window Ctrl-a a # Switch to the window that you were previously on Ctrl-a \" # List all open windows. Double-quotes \" are typed with the Shift key Ctrl-d or type exit # Exit out of the current window. Exiting form the last window will end the screen session Ctrl-a [ # Enters the scrolling mode. Use Page Up and Page Down keys to scroll through the window. Hit the Enter key twice to return to normal mode.  Attaching to Screen Sessions From any computer, you can attach to a screen session after SSH-ing into a server.\nscreen -r # Attaches to an existing session, if there is only one screen -r # Lists available sessions and their names, if there are more then one session running screen -r \u003csome-name\u003e # Attaches to a specific session screen -r \u003cfirst-few-letters-of-name\u003e # Type just the first few letters of the name # and you will be attached to the session you need  Destroying Screen Sessions  Terminate all programs that are running in the screen session. The standard way to do that is: Ctrl-c Exit out of your shell: exit Repeat steps 1 and 2 until you see the message: [screen is terminating]  There may be programs running in different windows of the same screen session. That’s why you may need to terminate programs and exit shells multiple time.\nTabs and a Reasonably Large History Buffer For a better experience with screen, run\ncp ~/.screenrc ~/.screenrc.backup 2\u003e /dev/null echo 'startup_message off defscrollback 10240 caption always \"%{=b dy}{ %{= dm}%H %{=b dy}}%={ %?%{= dc}%-Lw%?%{+b dy}(%{-b r}%n:%t%{+b dy})%?(%u)%?%{-dc}%?%{= dc}%+Lw%? %{=b dy}}\" ' \u003e ~/.screenrc  Simple One-Liner Shell Scripts Web page for script download.\nRenames many files *.old to *.new. To test things first, replace ‘do mv’ with ‘do echo mv’:\nfor i in *.input; do mv $i ${i/\\.old/\\.new}; done for i in *\\ *; do mv \"$i\" \"${i// /_}\"; done # Replaces spaces in files by underscores  Run an application in loops on many input files:\nfor i in *.input; do ./application $i; done  Run fastacmd from BLAST program in loops on many *.input files and create corresponding *.out files:\nfor i in *.input; do fastacmd -d /data/../database_name -i $i \u003e $i.out; done  Run SAM’s target99 on many input files:\nfor i in *.pep; do target99 -db /usr/../database_name -seed $i -out $i; done Search in many files for a pattern and print occurrences together with file names. for j in 0 1 2 3 4 5 6 7 8 9; do grep -iH \u003cmy_pattern\u003e *$j.seq; done  Example of how to run an interactive application (tmpred) that asks for file name input/output:\nfor i in *.pep; do echo -e \"$i\\n\\n17\\n33\\n\\n\\n\" | ./tmpred $i \u003e $i.out; done  Run BLAST2 for all .fasa1/.fasta2 file pairs in the order specified by file names and write results into one file:\nfor i in *.fasta1; do blast2 -p blastp -i $i -j ${i/_*fasta1/_*fasta2} \u003e\u003e my_out_file; done  This example uses two variables in a for loop. The content of the second variable gets specified in each loop by a replace function.  Runs BLAST2 in all-against-all mode and writes results into one file ('-F F' turns low-complexity filter off):\nfor i in *.fasta; do for j in *.fasta; do blast2 -p blastp -F F -i $i -j $j \u003e\u003e my_out_file; done; done;  How to write a real shell script   Create file which contains an interpreter as the first line:\n#!/bin/bash    Place shell commands in file below the interpreter line using a text editor.\n  Make file executable:\nchmod +x my_shell_script    Run shell script like this:\n./my_shell_script    Place it into your /rhome//bin directory\nmkdir -p ~/bin mv my_shell_script ~/bin/    Add the bin path to your shell permanently:\necho 'export PATH=~/bin:$PATH' \u003e\u003e ~/.bashrc source ~/.bashrc    Simple One-Liner Perl Scripts Small collection of useful one-liners:\nperl -p -i -w -e 's/pattern1/pattern2/g' my_input_file # Replaces a pattern in a file by a another pattern using regular expressions. # $1 or \\1: back-references to pattern placed in parentheses # -p: lets perl know to write program # -i.bak: creates backup file *.bak, only -i doesn't # -w: turns on warnings # -e: executable code follows  Parse lines based on patterns:\nperl -ne 'print if (/my_pattern1/ ? ($c=1) : (--$c \u003e 0)); print if (/my_pattern2/ ? ($d = 1) : (--$d \u003e 0))' my_infile \u003e my_outfile # Parses lines that contain pattern1 and pattern2. # The following lines after the pattern can be specified in '$c=1' and '$d=1'. # For logical OR use this syntax: '/(pattern1|pattern2)/'.  Remote Copy: wget, scp, ncftp Wget Use wget to download a file from the web:\nwget ftp://ftp.ncbi.nih.... # file download from www; add option '-r' to download entire directories  SCP Use scp to copy files between machines (ie. laptop to server):\nscp source target # Use form 'userid@machine_name' if your local and remote user ids are different. # If they are the same you can use only 'machine_name'.  Here are more scp examples:\nscp user@remote_host:file.name . # Copies file from server to local machine (type from local # machine prompt). The '.' copies to pwd, you can specify # here any directory, use wildcards to copy many files. scp file.name user@remote_host:~/dir/newfile.name # Copies file from local machine to server. scp -r user@remote_host:directory/ ~/dir # Copies entire directory from server to local machine.  Nice FTP From the linux command line run ncftp and use it to get files:\nncftp ncftp\u003e open ftp.ncbi.nih.gov ncftp\u003e cd /blast/executables ncftp\u003e get blast.linux.tar.Z (skip extension: @) ncftp\u003e bye  Archiving and Compressing Creating Archives tar -cvf my_file.tar mydir/ # Builds tar archive of files or directories. For directories, execute command in parent directory. Don't use absolute path. tar -czvf my_file.tgz mydir/ # Builds tar archive with compression of files or directories. For # directories, execute command in parent directory. Don't use absolute path. zip -r mydir.zip mydir/ # Command to archive a directory (here mydir) with zip. tar -jcvf mydir.tar.bz2 mydir/ # Creates *.tar.bz2 archive  Viewing Archives tar -tvf my_file.tar tar -tzvf my_file.tgz  Extracting Archives tar -xvf my_file.tar tar -xzvf my_file.tgz gunzip my_file.tar.gz # or unzip my_file.zip, uncompress my_file.Z, # or bunzip2 for file.tar.bz2 find -name '*.zip' | xargs -n 1 unzip # this command usually works for unzipping # many files that were compressed under Windows tar -jxvf mydir.tar.bz2 # Extracts *.tar.bz2 archive  Try also:\ntar zxf blast.linux.tar.Z tar xvzf file.tgz  Important options:\nf: use archive file p: preserve permissions v: list files processed x: exclude files listed in FILE z: filter the archive through gzip  Simple Installs Systems-wide installations Applications in user accounts Installation of RPMs Environment Variables xhost user@host # adds X permissions for user on server. echo $DISPLAY # shows current display settings export DISPLAY=\u003clocal_IP\u003e:0 # change environment variable unsetenv DISPLAY # removes display variable env # prints all environment variables  List of directories that the shell will search when you type a command:\necho $PATH  You can edit your default DISPLAY setting for your account by adding it to file .bash_profile\nExercises Exercise 1   Download proteome of Halobacterium spec. with wget and look at it:\nmodule load ncbi-blast/2.2.26 # Loads legacy blastall wget ftp://ftp.ncbi.nlm.nih.gov/genomes/genbank/archaea/Halobacterium_salinarum/representative/GCA_000069025.1_ASM6902v1/GCA_000069025.1_ASM6902v1_protein.faa.gz gunzip GCA_000069025.1_ASM6902v1_protein.faa.gz mv GCA_000069025.1_ASM6902v1_protein.faa AE004437.faa less AE004437.faa # press q to quit    Simple Analysis:\na. How many predicted proteins are there?\ngrep '^\u003e' AE004437.faa --count  b. How many proteins contain the pattern “WxHxxH” or “WxHxxHH”?\negrep 'W.H..H{1,2}' AE004437.faa --count  c. Use the find function (/) in ‘less’ to fish out the protein IDs containing the pattern or more elegantly do it with awk:\nawk --posix -v RS='\u003e' '/W.H..(H){1,2}/ { print \"\u003e\" $0;}' AE004437.faa | less # press q to quit    Create a BLASTable database with formatdb:\nls # before formatdb -i AE004437.faa -p T -o T ls # after '-p F' for nucleotide and '-p T' for protein database; '-o T' parse SeqId and create indexes    Generate myseq.fasta\na. Generate list of sequence IDs for the above pattern match result (i.e. retrieve my_IDs from step 2c). Alternatively, download the pre-generated file with wget.\nb. Retrieve the corresponding sequences for these IDs with the fastacmd command from the blastable database:\nwget https://cluster.hpcc.ucr.edu/~tgirke/Documents/UNIX/my_IDs fastacmd -d AE004437.faa -i my_IDs \u003e myseq.fasta less myseq.fasta # press q to quit    (Optional) Looking at several different patterns:\na. Generate several lists of sequence IDs from various pattern match results (i.e. retrieve a.my_ids, b.my_ids, and c.my_ids from step 2c).\nb. Retrieve the sequences in one step using the fastacmd in a for-loop:\nfor i in *.my_ids; do fastacmd -d AE004437.faa -i $i \u003e $i.fasta; done    Run blastall with a few proteins in myseq.fasta against your newly created Halobacterium proteome database.\nCreate first a complete blast output file including alignments. In a second step use the ’m -8' option to obtain a tabular output (i.e. tab separated values):\nblastall -p blastp -i myseq.fasta -d AE004437.faa -o blastp.out -e 1e-6 -v 10 -b 10 blastall -p blastp -i myseq.fasta -d AE004437.faa -m 8 -e 1e-6 \u003e blastp.tab less blastp.out # press q to quit less -S blastp.tab # -S disables line wrapping, press q to quit  The filed descriptions of the Blast tabular output (from the “-m 8” option) are available here:\n1 Query (The query sequence id) 2 Subject (The matching subject sequence id) 3 % id 4 alignment length 5 mismatches 6 gap openings 7 q.start 8 q.end 9 s.start 10 s.end 11 e-value 12 bit score    Is your blastp.out file equivalent to this one?\n  Parse blastall output into Excel spread sheet\na. Using biocore parser\nblastParse -i blastp.out -o blast.xls -c 5  b. Using BioPerl parser\nbioblastParse.pl blastp.out \u003e blastparse.txt    Exercise 2 Split sample fasta batch file with csplit (use sequence file myseq.fasta from Exercise 1).\ncsplit -z myseq.fasta '/\u003e/' '{*}'  Delete some of the files generated by csplit Concatenate single fasta files from (step 1) into to one file with cat (e.g. cat file1 file2 file3 \u003e bigfile). BLAST two related sequences, retrieve the result in tabular format and use comm to identify common hit IDs in the two tables.\nExercise 3 Run HMMPFAM search with proteins from Exercise 1 against Pfam database (will take ~3 minutes)\nhmmscan -E 0.1 --acc /srv/projects/db/pfam/2011-12-09-Pfam26.0/Pfam-A.hmm myseq.fasta \u003e output.pfam  Easier to parse/process tabular output\nhmmscan -E 0.1 --acc --tblout output.pfam /srv/projects/db/pfam/2011-12-09-Pfam26.0/Pfam-A.hmm myseq.fasta # also try --domtblout  Which query got the most hits? How many hits were found that query?\nExercise 4 Create multiple alignment with ClustalW (e.g. use sequences with ‘W.H..HH’ pattern)\nclustalw myseq.fasta mv myseq.aln myalign.aln  Exercise 5 Reformat alignment into PHYILIP format using ‘seqret’ from EMBOSS\nseqret clustal::myalign.aln phylip::myalign.phylip  Exercise 6 Create neighbor-joining tree with PHYLIP\ncp myalign.phylip infile protdist # creates distance matrix (you may need to press 'R' and then 'Y') cp outfile infile neighbor # use default settings (press 'Y') cp outtree intree  retree # displays tree and can use midpoint method for defining root of tree, my typical command sequence is: ‘N’ (until you see PHYLIP) ‘Y’ ‘M’ ‘W’ ‘R’ ‘R’ ‘X’\ncp outtree tree.dnd  View your tree in TreeBrowse or open it in TreeView\n","excerpt":"The Unix Shell When you log into UNIX/LINUX system, then is starts a …","ref":"/manuals/linux_basics/shell/","title":"Shell Bootcamp"},{"body":" The below links to detailed instructions. A shorter but more comprehensive summary for all three OSs is available here.\n ","excerpt":" The below links to detailed instructions. A shorter but more …","ref":"/manuals/hpc_cluster/sshkeys/","title":"SSH Keys"},{"body":"Compute Node We support running graphical programs on the cluster using VNC. For more information refer to Desktop Environments.\nGPU Workstation If a remote compute node does not fit your needs then we also have a GPU workstation specifically designed for rendering high resolution 3D graphics.\nHardware  Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz DDR4 256GB @ 2400 MHz NVIDIA Corporation GM204GL [Quadro M5000] 1TB RAID 1 HDD  Software The GPU workstation is uniquely configured to be an extension of the HPCC cluster. Thus, all software available to the cluster is also available on the GPU workstation through Environment Modules.\nAccess The GPU workstation is currently located in the Genomics building room 1208. Please check ahead of time to make sure the machine is available support@hpcc.ucr.edu. Once you have access to the GPU workstation, login with your cluster credentials. If your username does not appear in the list, you may need to click Not listed? at the bottom of the screen so that you are able to type in your username.\nUsage There are 2 ways to use the GPU workstation:\n Local - Run processes directly on the GPU workstation hardware Remote - Run processes remotely on the GPU cluster hardware  Local\nLocal usage is very simple. Open a terminal and use the Environment Modules to load the desired software, then run your software from the terminal. For example:\nmodule load amira Amira  Remotely\nOpen a terminal and submit a job. This is to reserve the time on the remote GPU node. Then once your job has started connect to the remote GPU node via ssh and forward the graphics back to the GPU workstation. For example:\n  Submit a job for March 28th, 2018 at 9:30am for a duration of 24 hours, 4 cpus, 100GB memory:\nsbatch --begin=2018-03-28T09:30:00 --time=24:00:00 -p gpu --gres=gpu:1 --mem=100g --cpus-per-task=4 --wrap='echo ${CUDA_VISIBLE_DEVICES} \u003e ~/.CUDA_VISIBLE_DEVICES; sleep infinity'  Read about GPU jobs for more information regarding the above.\n  Run the VirtualGL client in order to receive 3D graphics from the remove GPU node:\nvglclient \u0026    Wait for the job to start, and then check where your job is running:\nGPU_NODE=$(squeue -h -p gpu -u $USER -o '%N'); echo $GPU_NODE    The above command should result in a GPU node name, which you then need to SSH directly into with the following:\nssh -XY $GPU_NODE    Once you have SSH’ed into the remote GPU node, run setup the environment and run your software:\nexport NO_AT_BRIDGE=1 module load amira vglrun -display :$(head -1 ~/.CUDA_VISIBLE_DEVICES) Amira    ","excerpt":"Compute Node We support running graphical programs on the cluster …","ref":"/manuals/hpc_cluster/visual/","title":"Visualization"},{"body":"The HPC cluster is composed of various hardware.\n","excerpt":"The HPC cluster is composed of various hardware.\n","ref":"/about/hardware/","title":"Hardware"},{"body":"The HPC cluster continues to install various softwares.\n","excerpt":"The HPC cluster continues to install various softwares.\n","ref":"/about/software/","title":"Software"},{"body":"","excerpt":"","ref":"/about/","title":"About"},{"body":"","excerpt":"","ref":"/manuals/ext_cloud/aws/","title":"AWS"},{"body":"CyVersae is a NFS funded web platorm that has many pre defined apps. Mainly of the apps are for life sicence and can be very useful.\n NOTE: Not suitable for jobs that require more than 12 CPU cores (ie. MPI).\n Account Go to CyVerse and click on Create Account.\nThere are a few steps, just fill in the fields accordingly and complete the forms.\nAfter you have completed the form and submitted it, CyVerse will send you an email. Within the email will contain a link to set your password.\nData Management There are a few ways to upload/download data, for example you can browse your files from the Discovery Environment. However here we will focus on the command line method, since that is directly supported on the HPC cluster. Please refer to here for additoinal methods.\nFirst you will need to load the icommands tools:\nmodule load icommands/4.1.10  Then you will need to initialize the connection to CyVerse:\niinit  When you run the above command it will ask a few questions about your connection:\n   host name port # username zone password     data.cyverse.org 1247 CyVerse UserID iplant CyVerse Password    Once the iinit command has completed you are now able to list, push, get files and folders on CyVerse directlry from the HPCC.\nUpload The basic format to push files to CyVerse is like so:\niput FileName CyVersePath  For example:\niput hg18.fasta .  Since you automatically start in your home directory from CyVerse, the . will just place the fasta file directly within your home.\nOnce that command completes, you can double check that the the does exist on CyVerse, by listing the files, like so:\nils  The ils and iput command will work with relative and absolute paths.\nDownload The download method is identical to the upload method, just repalce iget instead of iput:\niget hg18.fasta .  The above command will download the hg18.fasta file to your current directory on the cluster.\nJobs Jobs on CyVerse are deployed via apps that you launch through the GUI here. Here is s video explaining how to create a docker image on the CyVerse system as well as configure a custom app to use it.\nPlease contact support for help creating a custom app, or any other questions.\n","excerpt":"CyVersae is a NFS funded web platorm that has many pre defined apps. …","ref":"/manuals/ext_cloud/cyverse/","title":"CyVerse"},{"body":"    Join us in Slack Get help and useful tips!\nRead more …\n   Contributions welcome! We do a Pull Request contributions workflow on GitHub.\nRead more …\n   Follow us on Twitter! For announcement of latest features etc.\nRead more …\n    ","excerpt":"    Join us in Slack Get help and useful tips!\nRead more … …","ref":"/events/","title":"Events"},{"body":"{% include taglogic.html %}\n{% include links.html %}\n","excerpt":"{% include taglogic.html %}\n{% include links.html %}\n","ref":"/tag_events/","title":"Events"},{"body":"  #td-cover-block-0 { background-image: url(\"background.jpg\") }  Research Computing at UCR's HPCC Welcome to the website of the High-Performance Computing Center (HPCC) at UC Riverside.  About  Events  Manuals           HPCC Operation During COVID-19 Crisis  Since the research computing infrastructure of the HPCC is designed to be accessed remotely, we are currently not expecting any major downtimes or restrictions for users due to the campus access restrictions caused by the COVID-19 pandemic.     System Changes \n  Rollout of Rocky OS and DUO from Feb 18 to Mar 17, 2022. For details see here.  Exceptions \n The following exceptions that are currently in effect:  Our offices in 1208/1207 Genomics Building are closed. The HPCC staff will continue to work as usual but exclusively via remote access from home. All in person contacts such as office visits or in-person training events are not possible during the current COVID-19 crisis. Alternatively, we will be offering remote training sessions via video conferencing using Zoom.      Mission and Services The High-Performance Computing Center (HPCC) provides state-of-the-art research computing infrastructure and training accessible to all UCR researchers and affiliates at low cost. The main advantage of a shared research computing environment is access to a much larger HPC infrastructure (with thousands of CPUs/GPUs and many PBs of directly attached storage) than what smaller clusters of individual research groups could afford, while also providing a long-term sustainability plan and professional systems administrative support.\n      Join us in Slack Get help and useful tips!\nRead more …\n   Contributions welcome! We do a Pull Request contributions workflow on GitHub.\nRead more …\n   Follow us on Twitter! For announcement of latest features etc.\nRead more …\n    ","excerpt":"  #td-cover-block-0 { background-image: url(\"background.jpg\") } …","ref":"/","title":"HPCC"},{"body":"Rollout of Rocky and DUO: Feb 18 through Mar 17, 2022 Also see email notification sent to users on 18-Feb-2022.  This is to inform users about several important system upgrades the HPCC will be implementing in the next 30 days (starting February 18th, 2022). Importantly, these changes are relevant to all users including those who are accessing the HPCC systems via JupyterHub, RStudio Server, SSH or sFTP/SCP. Thus, please read the instructions below carefully. If anything is unclear or there are questions, please email support@hpcc.ucr.edu for help.\nThe most important change will be the switch from the old CentOS/RHEL 7 platform to the new Rocky/RHEL 8. We anticipate to finalize this upgrade on March 17th, 2022. This gives users a 30-day transition period to log into the new Rocky/RHEL 8 platform and test whether the software and other resouces they are using for their research are properly working. It is important to understand the deployment of Rocky 8 is a major upgrade that requires the systems administrators recompiling most software from the old system onto the new system.\nTo avoid unnecessary extra downtimes, we are also elevating with this upgrade our security standards by adopting UCR’s DUO multi factor authentication system. This is important to prevent intrusions and comply with UC-wide IT standards.\nOperating System As mentioned above, the biggest change is that we are upgrading the OS from CentOS/RHEL 7 to Rocky/RHEL 8. Rocky Linux is the community equivalent and identical to RHEL (similar to how CentOS was). Currently, pigeon is the only head/login node that is linked to the new Rocky/RHEL 8 platform. To check which platform you are on, you can run the platform command.\nThe upgrade from RHEL 7 to RHEL 8 will result in the following user-facing changes.\nRocky Linux is an open-source enterprise operating system designed to be 100% bug-for-bug compatible with Red Hat Enterprise Linux -- Passwords User passwords will be expired on the current CentOS/RHEL 7 platform and users will need to reset their password during the next login. During this login, users need to provide their old password twice. The first time is to authenticate as cluster user (login password), and the second time to authorize the password reset (kerberos password). After this users will be prompted to provide their new password .\nWhen logging into the new Rocky/RHEL 8 platform as outlined here, users also need to configure DUO if using a password, or alternatively create an SSH key pair. If a user is new to DUO, the instructions from UCR’s ITS are here. Users accessing the cluster via SSH key pairs are strongly encouraged to also reset their password upon login by following the instructions here. Additional information about login related topics are provided on this manual page.\nExternal Users External users are unlikely to have a UCR NetID required for DUO. This includes users with HPCC cluster and restricted data transfer accounts. Those users want to access the cluster via SSH keys. This is both convenient (no need to type a password anymore) and secure. Please refer to our SSH keys manual for detailed instructions of configuring SSH key-based access. After creating an ssh key pair, users will need to email their public SSH key to support@hpcc.ucr.edu so that the systems admin can add their public SSH key to the corresponding user account.\nNote, the following instructions are only relevant for users who perform computations on our cluster(s). Users who are using our systems exclusively for data transfers can ignore them.\nSoftware All software installation requests will be restricted to the new cluster. New software may be installed under a different version, or may no longer be installed as a module, or may not be installed at all. Run the following command to list currently available modules:\nmodule avail  Or you can search for a specific software (ie. vim) like so:\nmodule avail vim  which vim  Since the new platform is built using GCC 8.5, then all previous compiled software must be re-compiled on the new platform. If you cannot find what you are looking for send an installation request to support@hpcc.ucr.edu.\nCompatibility mode (singularity) can be used to run the older CentOS 7 modules, however this may not work in all cases. Please refer to our Singularity Examples for more information.\nBash We officially support bash, even though other shells may work they have not been tested under the new Rocky/RHEL 8 platform.\nWhen logging in under a bash shell, some errors/warnings may be visible. The most common message being that a module cannot be loaded.\nCheck if the module (ie. vim) is available with the following:\nmodule avail vim  If there is no output, then the module is not currently available. Either remove the module load vim from your ~/.bashrc and/or ~/.bash_profile files, or request that it be installed.\nIf the software is available, ensure a proper check is in place around loading modules within your ~/.bashrc and/or ~/.bash_profile files.\nFor example:\nif [[ \"$(type -t module)\" == \"function\" ]]; then module load tmux module load neovim fi  It may also help to keep ~/.bashrc free of unnecessary bloat and only add customized changes to ~/.bash_profile. Also keep in mind that when running jobs with just /bin/bash the ~/.bashrc file is loaded. However, adding the lower case L to a job’s interpreter, as in /bin/bash -l this will load the ~/.bash_profile file. This can be useful since it provides flexibility to initialize a default job shell or a customized job shell.\nSlurm A newer version of Slurm is being used on the new Rocky/RHEL 8 platform, however very little is different from the previous version. All previous job submission scripts and commands/flags should still be compatible.\nDuring the transition period From the old CentOS/RHEL 7 platform any Slurm jobs scheduled to start after March 17th will never start. Please check your jobs and ensure that they run before this time frame. You can check your start times with the following command:\nsqueue --start -u $USER  Be sure to move to the newer Rocky/RHEL 8 platform as soon as possible.\nUser-facing Changes Implemented on 23-Aug-2019 Domain Names The old domain names biocluster.ucr.edu and bioinfo.ucr.edu have finally been discontinued. As a result, users need to use the new hpcc.ucr.edu name for the following services:\n ssh/scp/ftp/http: cluster.hpcc.ucr.edu instead of biocluster.ucr.edu RStudio Server: rstudio.hpcc.ucr.edu instead of rstudio.bioinfo.ucr.edu Jupyter: jupyter.hpcc.ucr.edu instead of jupyter.bioinfo.ucr.edu Email Support: support@hpcc.ucr.edu instead of support@biocluster.ucr.edu  In addition, URLs containing biocluster.ucr.edu need to be updated to cluster.hpcc.ucr.edu.\nPassword Reset After the upgrade on Aug 23, 2019, all HPCC users have been emailed a temporary password with instructions how to change it. This email was sent to the address we have on file for each user. In case you missed the corresponding email notification and/or you are not able to log into the cluster, please email us at support@hpcc.ucr.edu to receive a new password.\nUpdated OpenMPI If you have compiled or use software that was compiled with OpenMPI, then it will need to be recompiled. If you are running into any issues, please email us at support@hpcc.ucr.edu.\n","excerpt":"Rollout of Rocky and DUO: Feb 18 through Mar 17, 2022 Also see email …","ref":"/changes/","title":"Important Changes for HPCC Users"},{"body":"This page provides URLs to related resources  RED-UCR Data Science Center FIELDS Program Institute of Integrative Genome Biology  ","excerpt":"This page provides URLs to related resources  RED-UCR Data Science …","ref":"/links/","title":"Related Links"},{"body":"Requirements A user account is required to access HPCC’s research computing infrastructure. How to obtain a user account is described here. For external users it is important to know that the below Password+DUO multifactor authenication system is only available to UCR users. Thus, external users have to use the alternative SSH Key based authentication method, which is both secure and convenient to use.\nThe login instructions of this page are structured as follows:\nA. SSH Login via Terminal B. Web-based Access C. Data Sharing Access\nA. SSH Login from Terminal Terminal-based login is the most feature-rich method for accessing HPC resources. Web-based alternatives via Jupyter Hub and RStudio Server are also provided and introduced below. To access the HPCC cluster with the standard ssh protocol, users want to follow steps 1-3. Only step 1 is required after setting up ssh key based access.\n1. Type the following ssh login command from a terminal application, where \u003cusername\u003e needs to be replaced by the actual account name of a user. The \u003c\u003e characters indicate a placeholder and need to be removed. Next, press enter to execute the ssh command.\nssh -X \u003cusername\u003e@cluster.hpcc.ucr.edu  The -X argument enables X11 support, which is required for opening GUI applications on remote systems.\n2. Type your password and hit enter. Note, when typing the password the cursor will not move and nothing is printed to the screen. If ssh key access is enabled, both the password and Duo steps will be skipped automatically during the log in process.\n3. Follow the Duo multifactor authenication instructions printed to the screen. As external users do not have access to UCR’s Duo system, they can only log in via the alternative ssh key method. How to enable ssh keys is described here. Note, Duo will be bypassed if ssh key based login is enabled. This can be more conveniet than Duo when accessing the cluster frequently.\n  If the login is performed via a GUI application, which is an option in MobaXterm), then one can provide the same login information given under the above ssh commad in the corresponding fields of the login window as follows:\n Host name: cluster.hpcc.ucr.edu User name: … Password: …    Importantly, after the first login into a new account or a password reset, users need to change their password with the passwd command and then follow the on-screen instructions. This requires to enter the current password once and then enter the new password twice. New passwords need to be at least 8 characters long and meet at least 3 of the following requirments: lowercase character, uppercase character, number, and punctuation character.\nTerminal Options Various ssh terminal applications are available for all major operating systems. Examples include:\n macOS: built-in macOS Terminal or iTerm2 Windows: MobaXterm is a very feature rich terminal options for Windows users. Putty is an alternative, but outdated and not recommended anymore. Here are annimated usage introductions for MobaXterm. Linux: a wide range of Terminal applications is available for Linux. Usually, the default terminal available on a Linux distributions will be sufficient. ChromeOS: after enabling Linux apps on Chromebooks one can use the default terminal that is similar to those on Linux systems.  Remote Graphics Support X11 support is included in the terminal applications of most OSs. This includes MobaXterm on Windows, Linux and ChromeOS terminals. On macOS systems, users need to run XQuartz in the background to enable X11 graphics display support. XQuartz can be downloaded from here (also see this video here). Note, the install of XQuartz can be skipped if remote graphics support is not needed.\nAdditional Authentication Details In early 2022 the HPCC adopted a more secure authentication method for logging into its clusters. Passwords alone will no longer be allowed for SSH or file tansfer protocols. Instead Password+DUO or SSH Keys will be required. Because Password+DUO authentication requires a UCR NetID, this access method is only available to UCR users for both ssh and file transfer protocols (e.g. sFTP or SCP). Thus, external users need to use the alternative ssh key method. To enable ssh key access, the public key needs to be emailed to support@hpcc.ucr.edu (see below for details). One exception are web-based services where password-based access doesn’t require Duo multifactor authenication or ssh keys.\nPassword+Duo Users familiar with UCR’s Duo system can log in to HPCC’s clusters by following the on screen instructions during the ssh login. For new users, instructions for UCR’s Duo Multifactor Authenication system are available in this PDF and on UCR’s MyAccount page. Importantly, the login via the Password+DUO method will only work if a user’s NetID matches the username of the corresponding HPCC account. If this is not the case then the HPCC username can be changed to a user’s NetID. This change can be initiated by emailing suppor@hpcc.ucr.edu. As mentioned above, external users will not have access to UCR’s Duo system, and thus have to use the alternative ssh key access method to log in to HPCC’s resources.\nSSH Keys Ssh keys are an access credential used by the Secure Shell (SSH) protocol. For this a key pair is created comprised of a private key and a public key. The private key remains on a user’s system and should not be shared. The public key will be uploaded to the remote system, here ~/.ssh directory of a user’s account on the HPCC cluster. Ssh key based access works analogous to how a key and a lock are used in the real world, where one uniquely fits into the other. Access can only be established if the private key on a user’s system fits the public key on the remote system.\nThe following introduces how to create an ssh key pair from the command-line in a terminal and upload the public key to the remote system. The latter upload will only work if a user can access the remote system, e.g. via temporary Password+DUO access. User without this option have to email their public ssh key to suppor@hpcc.ucr.edu so that the systems administrator can upload the public key for them. Additional details on ssh key generation and uploads are provided here. This includes GUI based based options. However, we highly recommend to use the command-line options which are much more straigthforward to use, including MobaXterm on Windows systems.\n(a) SSH Key Creation An ssh key pair can be created with the following commands in a terminal application of all major operating systems, including Windows, macOS, Linux and ChromeOS.\n1. Create SSH directory\nmkdir -p ~/.ssh  2. Create key pair (private and public)\nssh-keygen -t rsa -f ~/.ssh/id_rsa  Follow the prompts and complete the processes. Once the command has completed, one will find two files in the ~/.ssh directory of a user account.\n3. Inspect ~/.ssh directory\nls ~/.ssh/ id_rsa id_rsa.pub  (b) SSH Key Upload The id_rsa and id_rsa.pub files are the private and public keys, respectively. The private key should never be shared with anyone. This means it should not be emailed or uploaded to a remote system. Only the public key will be uploaded to the remote system, here HPCC user account. Specifically, the public key will be stored in a file called authorized_keys under a directory called ~/.ssh. If not present yet both need to be created. Note, ~/ refers to the higest (root) level of a user account.\n1. Upload of first public ssh key\nIf the authorized_keys doesn’t exist yet, the following scp command can be run from a user’s system. This command will create the ~/.ssh/authorized_keys file and populate it with the public key.\nscp .ssh/id_rsa.pub username@cluster.hpcc.ucr.edu:.ssh/authorized_keys  2. Upload of subsequent public ssh keys\nIf the authorized_keys file already exists, one can append the new public key as follows.\nscp .ssh/id_rsa.pub username@cluster.hpcc.ucr.edu:tmpkey \u0026\u0026 ssh username@cluster.hpcc.ucr.edu \"cat tmpkey \u003e\u003e ~/.ssh/authorized_keys \u0026\u0026 rm tmpkey\"  3. Check ssh key based access\nTo test whether ssh key based access is functional, then the following log in should work without asking for a password. However, it may ask for a passphrase if the ssh key pair was created this way.\nssh \u003cusername\u003e@cluster.hpcc.ucr.edu  (c) Additional Details on SSH Keys See here.\nB. Web-based Access Web-based HPCC cluster access is provided via RStudio Server and JupyterHub. Users with an HPCC cluster account can access them with the same login credential used for ssh access. The Username+Password authentication method with Duo is currently not required for these services. For load balancing RStudio Server has two instances: RStudio Server 1 and RStudio Server 2. If one of these services is slow or not available, users want to choose the alternative option. A much more efficient method for using RStudio Server is provided via a custom compute node instance using srun. This option is described here.\nC. Data Sharing Access Users accessing HPCC’s infrastructure mainly for file transfers and data sharing want to follow the corresponding instructions on the Data Sharing page.\n","excerpt":"Requirements A user account is required to access HPCC’s research …","ref":"/manuals/login/","title":"Login"},{"body":"","excerpt":"","ref":"/manuals/","title":"Manuals"},{"body":"","excerpt":"","ref":"/news/","title":"News"},{"body":" CAVEATS:\nA fair amount of resources must be manually calculated from the currently available.\nAlso, it seems like a beta feature would repalce what I did here:\nhttps://kubernetes.io/blog/2021/04/19/introducing-indexed-jobs/\n The PRP (Pacific Research Platform) is a NSF funded Kubernetes infrastructure and therefore requires the use of a Kubernetes interface. The command line Kubernetes interface kubectl is what is used from the HPC cluster.\nAccount Follow the guides on the PRP website posted here. There is a link called Get access, depending on your role, you can request a admin account or a user account.\nInstall Once you have an account, and you have read all of the docs here, you can proceed to install. Submitting jobs, checking states, getting logs as well as interactive sessions can all be done from the HPCC.\nHowever, running these operations directly from your laptop/workstation can be faster. In order to run these actions locally you will need to install kubectl on your local laptop/workstation.\nYou can install kubectl via conda, like so:\nconda create -n kube -c anaconda-platform kubectl  If you do not yet have conda installed, follow these instructions.\nConfig For kubectl to function, it requires your config file provdied by PRP. In order to get the PRP Kubernetes config file, do the following:\n Visit Nautilus Portal Click on Login in upper right coner. Login using CILogon credentials (A.K.A UCR netID). Once authenticated, click on the Get config in the upper right conner. This takes a while to dynamically generate, just wait and eventually your browser will present you a download prompt. Place this file in your ~/.kube directory.  Next set the namespace, or else you will have to append the -n ucr-hpcc flag to every Kubernetes command. You may be under the ucr-hpcc namespace if you are testing, otherwise you should have your own namespace:\nkubectl config set-context nautilus --namespace=ucr-hpcc  Usage Here is an example of an array style job on utilizing redis to track job numbers, and the dockerfile stored within the PRP GitLab repository.\n  First copy scripts/* files from the repo to your code base. Make sure that your analysis workflow is started within the worker.py script.\n  Create redis service and deployment\n  kubectl create -f hpcc-redis.yml  Log into Redis pod and manually add items  # Get into pod kubectl exec -it redis-master -- /bin/bash # Add 10 items to list \"job2\" echo lpush job2 {1..10} | redis-cli -h redis --pipe # Print all items in \"job2\" redis-cli -h redis lrange job2 0 -1  Submit job  kubectl create -f hpcc-job.yml  Egress The PRP has fail2ban blocking rapid SSH connections, so copying files within a loop would fail. It is best to try and copy all needed files with a single rsync command, like so:\nnohup rsync -rvP --include='*/spades.log.gz' --include='*/scaffolds.fasta' --exclude='*/*' /output/ cluster.hpcc.ucr.edu:~/output \u0026\u003e rsync_spades.log  The above rsync command looks into the sub-directories within /output and will copy only the spades.log.gz and scaffolds.fasta files from each onto the HPCC cluster.\nTrouble From pod list, check log:\nkubectl logs hpcc-pod  Jobs and pods will expire after 1 week, however you can alter this with the following:\nttlSecondsAfterFinished=604800  Check on the pod details:\nkubectl describe pod hpcc-pod  Delete job if you want to rerun, this will also delete associated pods:\nkubectl delete pods hpcc-pod  For updating your repo to the lastest HPCC changes, you can sync like so:\n# Add upstream git remote add upstream git://github.com/ORIGINAL-DEV-USERNAME/REPO-YOU-FORKED-FROM.git # Get branchs git fetch upstream # Sync local files with master branch git pull upstream master  Links  Help - https://element.nrp-nautilus.io Resources - https://nautilus.optiputer.net/resources Monitoring - https://grafana.nautilus.optiputer.net  ","excerpt":" CAVEATS:\nA fair amount of resources must be manually calculated from …","ref":"/manuals/ext_cloud/prp/","title":"PRP"},{"body":"","excerpt":"","ref":"/search/","title":"Search Results"},{"body":"What is Singularity In short, Singularity is a program that will allow a user to run code or command, within a customized environment. We will refer to this customized environment as a container. This type of container system is common, the more popular one being Docker. Since Docker requires root access and HPC users are not typically granted these permissions, we use Singularity instead. Docker containers can be used via Singularity, with varying compatibility.\nSingularity is forking into 2 branches:\n Singularity-CE - Community Edition from Sylabs.io Apptainer - Original Sinularity open source project  We will be using Apptainer when it is ready for production use. However, in the meantime, singularity-ce is currently availble on the cluster.\nLimitations Currently we are not supporting Slurm jobs bening submitted from within a container. If you load the container centos/7.9 and try to submit a job from within it will fail. Please contact support in order to work around this issue.\nHow to use Singularity Singularity is easy to use. You can run singularity in an interactive mode by calling a shell, or you can run singularity in a non-interactive mode and just pass it a script. This 2 modes are very similar to job submission on the cluster; srun is used for interactive, while sbatch is used for non-interactive.\nInteractive Singularity When running singularity you need to provide the path to a singularity image file. For example, this would be the most basic way to get a shell within your container:\nmodule load singularity singularity shell /path/to/singularity/image  Typically when we install singularity enabled software, we will also create an environment variable which holds the location of the singularity image file. For example, this would be how we would use the centos module:\nmodule load centos singularity shell $CENTOS7_SING  There is a special shortcut for the centos module that allows us to run the above more simply, as:\nmodule load centos centos.sh  Here is a another example that utilizes an interactive job:\nmodule load centos srun -p batch --mem=1g -c 4 --time=2:00:00 --pty centos.sh  Non-Interactive Singularity When running singularity as non-interactive, the same basic rules apply, we need a path to our singularity image file as well as a command.\nBasics For example, here is the basic syntax:\nmodule load singularity singularity exec /path/to/singularity/image someCommand  Using centos as an example, you can execute an abitraty command like so:\nmodule load singularity singularity exec $CENTOS7_SING cat /etc/redhat-release  Shortcuts Now using the centos shortcut:\nmodule load centos centos.sh \"cat /etc/redhat-release\"  Here is a more complex example with modules:\nmodule load centos centos.sh \"module load samtools; samtools --help\"  Jobs Here is an example submitted as a job:\nmodule load centos sbatch -p batch --wrap=\"centos.sh 'module load samtools; samtools --help'\"  Variables Here is an example with passing environment variables:\nexport SINGULARITYENV_SOMETHING='stuff' centos.sh 'echo $SOMETHING'   Notice: Just add the SINGULARITYENV_ prefix to pass any varibales to the centos container.\n Enable GPUs First review how to submit a GPU job from here. Then request an interactive GPU job, or embed one of the following within your submission script.\nIn order to enable GPUs within your container you need to add the --nv option to the singularity command:\nmodule load centos singularity exec -nv $CENTOS7_SING cat /etc/redhat-release  However, when using the centos shortcut it is easier to just set the following environment variable then run centos.sh as usual:\nmodule load centos export SINGULARITY_NV=1 centos.sh  ","excerpt":"What is Singularity In short, Singularity is a program that will allow …","ref":"/manuals/hpc_cluster/singularity/","title":"Singularity Jobs"},{"body":"SSH Keys on macOS What are SSH Keys? SSH (Secure Shell) keys are an access credential that is used in the SSH protocol.\nThe private key (ie. id_rsa) remains on the system being used to access the HPCC cluster and is used to decrypt information that is exchanged in the transfer between the HPCC cluster and your system.\nA public key (ie. id_rsa.pub) is used to encrypt information, and is stored on the cluster. The authorized keys file that is stored on the HPCC cluster (ie. ~/.ssh/authorized_keys) contains one or more public keys (ie. id_rsa.pub).\nWhy do you need SSH Keys? HPCC supports two authentication methods; Password+DUO and SSH Keys. The Password+DUO method requires a UCR NetID, if you do not have one then you will need to use SSH keys in order to access the HPCC cluster.\nCreating SSH Keys from the Command-line By far the easiest way to create SSH keys on macOS systems is from the command-line following the instructions here. Users who prefer to do this in a graphical user interface can follow the instructions below.\nGUI-based SSH Key Creation Filezilla You will need to install Filezilla in order to transfer the public SSH key to the HPCC cluster.\n Download the Filezilla Client for Mac OS X here.  Make sure your Mac OS X system is updated to the latest version.   Follow the install wizard to complete the install of Filezilla.  Sourcetree You will need to install Sourcetree in order to generate your SSH keys (or use the command line method mentioned here.\n Download Sourcetree from here Click on Download for Mac OS X Install Sourcetree  Create SSH Keys (Sourcetree)   Open the Sourcetree application and under the top Sourcetree menu click on the Preferences... sub-menu item.\n  Navigate to Accounts category and click on Add....\n  Click on Auth Type: and change the drop down menu from OAuth to Basic. Make sure Protocol: is set to SSH in the drop down menu.\n  Enter id_rsa in the Username field.\n  Click the Generate Key button.\n  Press Cancel to exit out of the window.\n  SSH Keys Location By default, your key files are created in the path: /Users/macOSUsername/.ssh/.\nTo verify that the keys were created, do the following:\n  Open a new finder window. Click on your home directory on the left side pane.\n  Press the 3-button combo Command+Shift+. together (visualized below) to see hidden folders:\n  You will now be able to see your .ssh folder, open it by double-clicking.\n  You should see your newly generated pair of SSH key files in the folder.\n  Sourcetree adds the -Bitbucket to the end of the SSH key file names. Remove this by clicking on the file you want to rename and press the Enter key which allows us to rename the file before the extension.\n  After you have removed the -Bitbucket suffix from each of the SSH key file names, your new SSH key file names should be id_rsa and id_rsa.pub.\n  Configure SSH Keys Public SSH Key Now that you have created your SSH keys, and renamed them, you will need to place your public key (id_rsa.pub) on the cluster.\nIf you do not have a UCR NetID, or prefer not to use Password+DUO authentication, then email your public key (id_rsa.pub) to support and skip to Private SSH Key. If you already have configured Password+DUO authentication, then proceed with the following:\n  Start the Filezilla application.\n  Open Site Manager window by clicking the upper left most button in the top bar of icons.\n  Click on New Site, which will unlock the fields to the right.\n  From the newly unlocked fields in the General tab, fill in the following:\n Protocol: SFTP - SSH File Transfer Protocol Host: cluster.hpcc.ucr.edu Logon Type: Interactive User: Your HPCC Username    When using Password+DUO authentication, you must also set the maximum number of connections. Navigate to the Transfer Settings tab and set the following:\n Limit Number of simultaneous connections: checked Maximum number of connections: 1  Then click on Connect.\n  If a pop up prompts you to save your password, select the Save passwords option, then click the OK button.\n  Then enter in your password for the cluster, and click OK.\n  If the next pop up prompts you, then check the box that states Always trust this host, add this key to the cache, then click the OK button.\n  You should now see the DUO authentication dialog, ensure your User is correct then enter the number for the preferred option from the list presented, then click OK.\n  Now that you are connected with Filezilla, transfer your public SSH key from your MacOS system by dragging the file /Users/macOSUsername/.ssh/id_rsa.pub and dropping it into the HPCC cluster direcotry /rhome/username/.ssh/.\n  If the /rhome/username/.ssh/ directory does not exits, create it.\nOnce the id_rsa.pub file is transferred to the cluster, be sure to rename it to authorized_keys.  Private SSH Key Once your public key is in place, now you can configure Filezilla to use your private SSH key and connect to the cluster through the secure.hpcc.ucr.edu server.\n  Start the Filezilla application\n  Open Site Manager window by clicking the button in the top bar of icons.\n  Click on New Site, rename it (optional) and press enter.\n  Fill in the following fields from the General tab:\n Protocol: SFTP - SSH File Transfer Protocol Host: cluster.hpcc.ucr.edu Logon Type: Key file User: Your HPCC username Key file: /Users/macOSUsername/.ssh/id_rsa  Be sure to select the previously created private key (/Users/macOSUsername/.ssh/id_rsa) for the Key file field using the Browse... button.\n  Navigate to the folder you saved your key file in (default location is /Users/macOSUsername/.ssh) and open the private key file id_rsa.\n  You should see the added keyfile in the Key file: box, then click Connect.\nSubsequnt connections can be done from the Quickconnect history by clicking on the down arrow to the right side of the Quickconnect button.\n  Remember to select the secure.hpcc.ucr.edu address.\n  Transfer files by double clicking or drag-n-drop. For more details regarding file transfers vist Filezilla Usage.\n  ","excerpt":"SSH Keys on macOS What are SSH Keys? SSH (Secure Shell) keys are an …","ref":"/manuals/hpc_cluster/sshkeys/sshkeys_macos/","title":"SSH Keys Apple macOS"},{"body":"SSH Keys on Linux How to create SSH keys on LInux OSs or the command-line in general is described on the general login page here.\n","excerpt":"SSH Keys on Linux How to create SSH keys on LInux OSs or the …","ref":"/manuals/hpc_cluster/sshkeys/sshkeys_linux/","title":"SSH Keys Linux"},{"body":"SSH Keys on MS Windows What are SSH keys? SSH (Secure Shell) keys are an access credential that is used in the SSH protocol.\nThe private key remains on the system being used to access the HPCC cluster and is used to decrypt information that is exchanged in the transfer between the HPCC cluster and your system.\nA public key file is used to encrypt information, and is stored on your own system. The public key file is stored on the HPCC cluster and contains a list of authorized public keys.\nWhy do you need SSH keys? HPCC supports two authentication methods; Password+DUO and SSH Keys. The Password+DUO method requires a UCR NetID, if you do not have this then you will need to use SSH keys in order to access the HPCC cluster.\nWhat you need MobaXterm You will need to install MobaXterm in order to generate your SSH keys and also to transfer the keys to the cluster.\n Download MobaXterm from here. Unzip Double click portable version of exe and run the MobaXterm application.  FileZilla If you choose to upload you SSH key to the HPCC cluster with a GUI app, you will need to install FileZilla or a similar sFTP/SCP client. Note, FileZilla is not required if you use the command-line approach below.\n Download the FileZilla Client for Windows here. * Make sure your Windows system is updated to the latest version. Follow the install wizard to complete the install of Filezilla.  Create SSH Keys (MobaXterm) The following provides instructions for both (A) command-line-based and (B) GUI-based SSH key creation. Users need to choose which option is more suitable for them. Usually, the command-line based approach is much quicker even for users without command-line experience since it only requires to copy and paste a few lines of code.\n(A) Command-line-based SSH key creation Creating SSH keys in MobaXterm from the command-line is straightforward and almost identical to creating SSH keys under macOS and Linux (see here). To create the SSH key pair from the command-line, open the MobaXterm terminal and then execute the following commands. This can be done by a simple copy and paste rather than typing, and then pressing the enter key. Users who wish to use WinSCP instead of FileZilla as sFTP client need to follow the key generation instructions of this software as outlined here.\nmkdir -p ~/.ssh # creates SSH directory ssh-keygen -t rsa -f ~/.ssh/id_rsa # creates key pair (private and public)  Next, check the content of the newly created .ssh directory with ls -al .ssh/. It should contain files for the private and public keys that are named id_rsa and id_rsa.pub, respectively. Importantly, this private key file should not be shared.\nNote, when using PuTTY (and WinSCP) instead of MobaXterm for generating SSH keys, then the private key is stored in PuTTY’s proprietary key format, which is indicated by a .ppk file extension. A key of this format is required when using PuTTY as SSH client, and it cannot be used with other SSH client tools.\nThe public key is the one that needs to be uploaded to the remote system one wishes to connect to. On the HPCC cluster it needs to be saved in a file located under this location of your home directory: ~/.ssh/authorized_keys. The upload can be performed with an sFTP/SCP GUI app like the one built into MobaXterm or FileZilla (see GUI section below). Copying the key from MobaXterm into the clipboard (e.g. in less) and then pasting it into the corresponding file opened on the remote system with a code editor like vim is another but more advanced option. The following shows how to upload the private SSH key from the command-line in MobaXterm to the HPCC cluster using the scp command, where it is important that users replace \u003cusername\u003e with their own username on the HPCC cluster. Importantly, only one of the following two commands should be used. The first one should be used if an authorized_keys file does not exist yet, e.g. when a user configures SSH key accees on the HPCC system for the first time. The second one should be used to append a new public SSH key to an already existing authorized_keys file.\n  Create new authorized_keys file\nscp .ssh/id_rsa.pub \u003cusername\u003e@cluster.hpcc.ucr.edu:.ssh/authorized_keys    Append SSH key to already existing authorized_keys file\nscp .ssh/id_rsa.pub \u003cusername\u003e@cluster.hpcc.ucr.edu:tmpkey \u0026\u0026 ssh username@cluster.hpcc.ucr.edu \"cat tmpkey \u003e\u003e ~/.ssh/authorized_keys \u0026\u0026 rm tmpkey\"    Note, prior to setting up SSH key access both of the above scp commands require functional password/DUO credentials. Users who do not have password/DUO access (e.g. non-UCR users) will need to email their public SSH key to support@hpcc.ucr.edu so that the systems admin can add their public SSH key to ~/.ssh/authorized_keys of the corresponding user account.\n(B) GUI-based SSH key creation   Begin by clicking on the tools drop down on the upper menu bar\n  Find and click on the MobaKeyGen (SSH key generator) option\n  A window should appear to create a new SSH key. Click on generate to create a new SSH key pair. Follow the on menu instructions.\n  Once your key has been created, enter a password in the key passphrase field to password protect your key. Click on conversions in the tool bar and click on Export OpenSSH Key. Save this key as id_rsa and put the file in an easy to access location. Click on Save private key to save the private key with an extension of .ppk to use with MobaXterm or FileZilla. Save the key as mobaxterm_privkey and put the file in an easy to access location.\n  Highlight EVERYTHING in the box labeled “Public key for pasting into OpenSSH authorized_keys file” then right-click on it and choose Copy. Open Notepad and paste the copied text. Save the file as id_rsa.pub and put the file in an easy to access location.\n  Keys Location SSH keys should be saved under the location C:\\Users\\username\\.ssh.\nConfigure SSH Keys Public SSH Key Now that you have created your SSH keys, and renamed them, you will need to placed the public key (id_rsa.pub) on the cluster using the cluster.hpcc.ucr.edu\n  Start the Filezilla application.\n  Fill in the Quickconnect fields at the top of the application window:\n Enter your HPCC username in the Username field. Enter the HPCC servername cluster.hpcc.ucr.edu for the Host field. Enter your password in the Password field. Enter 22 in the Port field.    Click on Quickconnect\n  If the next pop up prompts you, then check the box that states Always trust this host, add this key to the cache, then click the OK button.\n  You will need to create a .ssh directory to hold your SSH keys. On the right hand side, right click and click on the Create directory option under your home folder location.   A window will appear to name the new directory. Name should be the following format: /rhome/username/.ssh. After naming the new directory click on OK.   Right click on the new .ssh directory that has been created. Find and click on File permissions.   A window with the directory permissions will appear. The .ssh directory needs exact permissions in order for it to function properly. Follow the image below to apply the permissions.   Now that you are connected to Filezilla transfer your public SSH key from your system by dragging the file id_rsa.pub and dropping it into the HPCC cluster direcotry /rhome/username/.ssh/.\n  Once the file is transferred to the cluster, be sure to rename id_rsa.pub to authorized_keys.\n  Private SSH Key Once your public key is in place, now you can configure Filezilla to use your private SSH key and connect to the cluster through the secure.hpcc.ucr.edu server.\n  Open Filezilla Site Manager button in the top bar of icons.\n  Click on New Site, rename it (optional) and press enter.\n  Make sure the following fields are correctly filled before adding your SSH key file:\n Protocol: should be set to SFTP - SSH File Transfer Protocol Host: type in secure.hpcc.ucr.edu Port: type 22 Logon Type: set to Key file User: type in your HPCC username  After these fields are finalized, click the Browse.. button.\n  Navigate to the folder you saved your private key file in and open the private key file mobaxterm_privkey.ppk. You should see the added keyfile in the Key file: box, then click Connect.\n  Subsequnt connections can be done from the Quickconnect history by clicking on the down arrow to the right side of the Quickconnect button. Remember to select the secure.hpcc.ucr.edu address.\n  Transfer files by double clicking or drag-n-drop. For more details regarding file transfers vist Filezilla Usage.\n  ","excerpt":"SSH Keys on MS Windows What are SSH keys? SSH (Secure Shell) keys are …","ref":"/manuals/hpc_cluster/sshkeys/sshkeys_winos/","title":"SSH Keys Microsoft Windows"},{"body":" Note: Neovim has replaced vim on the cluster, however most (if not all) items here still apply. Load Neovim via the module system, like so module load neovim. It may also be useful to alias nvim to vim, like so alias vim=nvim. Thia alias can be added to your ~/.bashrc file for convenience.\n Vim Manual This is an extensive overview of vim features and operations.\nBasics vim \u003cmy_file_name\u003e # open/create file with vim\nOnce you are in Vim the most important commands are i , : and ESC. The i key brings you into the insert mode for typing. ESC brings you out of there. And the : key starts the command mode at the bottom of the screen. In the following text, all commands starting with : need to be typed in the command mode. All other commands are typed in the normal mode after hitting the ESC key.\nModifiers i # INSERT MODE ESC # NORMAL (NON-EDITING) MODE : # Commands start with ':' :w # Save command; if you are in editing mode you have to hit ESC first!! :q # Quit file, don't save :q! # Exits WITHOUT saving any changes you have made :wq # Save and quit R # Replace MODE r # Replace only one character under cursor q: # History of commands (from NORMAL MODE!), to reexecute one of them, select and hit enter! :w new_filename # Saves into new file :#,#w new_filename # Saves specific lines (#,#) to new file :# # Go to specified line number  Moving Around $ # Moves cursor to end of line A # Same as $, but switches to insert mode 0 # Zero moves cursor to beginning of line CTRL-g # Shows file name and current line you are on SHIFT-G # Brings you to bottom of file  Lines :set wrap # Wrap lines around the screen if too long :set nowrap # No line wrapping :set number # Shows line numbers :set nonumber # No line numbers  Multiple Files vim -o *.txt # Opens many files at once and displays them with horizontal # Split, '-O' does vertical split vim *.txt # Opens many files at once; ':n' switches between files  :wall or :qall # Write or quit all open files :args *.txt # Places all the relevant files in the argument list :all # Splits all files in the argument list (buffer) horizontally CTRL-w # Switch between windows :split # Shows same file in two windows :split \u003cfile-to-open\u003e # Opens second file in new window :vsplit # Splits windows vertically, very useful for tables, \":set scrollbind\" let's you scroll all open windows simultaneously :close # Closes current window :only # Closes all windows except current one  Spell Checking :set spell # Turns on spell checking :set nospell # Turns spell checking off :! dict \u003cword\u003e # Meaning of word :! wn 'word' -over # Synonyms of word  Syntax Highlighting :set filetype=perl # Turns on syntax coloring for a chosen programming language. :syn on # Turns syntax highlighting on :syn off # Turns syntax highlighting off  Undo and Redo u # Undo last command U # Undo all changes on current line CTRL-R # Redo one change which was undone  Deleting x # Deletes what is under cursor dw # Deletes from curser to end of word including the space de # Deletes from curser to end of word NOT including the space cw # Deletes rest of word and lets you then insert, hit ESC to continue with NORMAL mode c$ # Deletes rest of line and lets you then insert, hit ESC to continue with with NORMAL mode d$ # Deletes from cursor to the end of the line dd # Deletes entire line 2dd # Deletes next two lines, continues: 3dd, 4dd and so on.  Copy and Paste yy # Copies line, for copying several lines do 2yy, 3yy and so on p # Pastes clipboard behind cursor  Search /my_pattern # Searches for my_pattern downwards, type n for next match ?my_pattern # Searches for my_pattern upwards, type n for next match :set ic # Switches to ignore case search (case insensitive) :set hls # Switches to highlight search (highlights search hits)  Replacements Great intro: A Tao of Regular Expressions\nQuick reference to some replacement techniques:\n:s/old_pat/new_pat/ # Replaces first occurrence in a line :s/old_pat/new_pat/g # Replaces all occurrence in a line :s/old_pat/new_pat/gc # Add 'c' to ask for confirmation :#,#s/old_pat/new_pat/g # Replaces all occurrence between line numbers: #,# :%s/old_pat/new_pat/g # Replaces all occurrence in file :%s/\\(pattern1\\)\\(pattern2\\)/\\1test\\2/g # Regular expression to insert, you need here '\\' in front of parentheses (\u003c# Perl) :%s/\\(pattern.*\\)/\\1 my_tag/g # Appends something to line containing pattern (\u003c# .+ from Perl is .* in VIM) :%s/\\(pattern\\)\\(.*\\)/\\1/g # Removes everything in lines after pattern :%s/\\(At\\dg\\d\\d\\d\\d\\d\\.\\d\\)\\(.*\\)/\\1\\t\\2/g # Inserts tabs between At1g12345.1 and Description :%s/\\n/new_pattern/g # Replaces return signs :%s/pattern/\\r/g # Replace pattern with return signs!! :%s/\\(\\n\\)/\\1\\1/g # Insert additional return signs :%s/\\(^At\\dg\\d\\d\\d\\d\\d.\\d\\t.\\{-}\\t.\\{-}\\t.\\{-}\\t.\\{-}\\t\\).\\{-}\\t/\\1/g # Replaces content between 5th and 6th tab (5th column), '{-}' turns off 'greedy' behavior :#,#s/\\( \\{-} \\|\\.\\|\\n\\)/\\1/g # Performs simple word count in specified range of text :%s/\\(E\\{6,\\}\\)/\u003cfont color=\"green\"\u003e\\1\u003c\\/font\u003e/g # Highlight pattern in html colors, here highlighting of \u003e= 6 occurences of Es :%s/\\([A-Z]\\)/\\l\\1/g # Change uppercase to lowercase, '%s/\\([A-Z]\\)/\\u\\1/g' does the opposite  Uses ‘global’ command to apply replace function only on those lines that match a certain pattern. The ‘copy $’ command after the pipe ‘|’ prints all matching lines at the end of the file.\n:g/my_pattern/ s/\\([A-Z]\\)/\\l\\1/g | copy $  Command ‘args’ places all relevant files in the argument list (buffer); ‘all’ displays each file in separate split window; command ‘argdo’ applies replacement to all files in argument list (buffer); flag ‘e’ is necessary to avoid stop at error messages for files with no matches; command ‘update’ saves all changes to files that were updated.\n:args *.txt | all | argdo %s/\\old_pat/new_pat/ge | update  Utilities   Matching Parentheses\n Place cursor on (, [ or { and type % # cursor moves to matching parentheses    Printing and Inserting Files\n :ha # Prints entire file :#,#ha # Prints specified lines: #,# :r \u003cfilename\u003e # Inserts content of specified file after cursor    Convert Text File to HTML Format\n :runtime! syntax/2html.vim # Run this command with open file in Vim    Shell Commands in Vim\n :!\u003cSHELL_COMMAND\u003e \u003cENTER\u003e # Executes any shell command, hit \u003center\u003e to return :sh # Switches window to shell, 'exit' switches back to vim    Using Vim as Table Editor\n v starts visual mode for selecting characters V starts visual mode for selecting lines` CTRL-V starts visual mode for selecting blocks (use CTRL-q in gVim under Windows). This allows column-wise selections and operations like inserting and deleting columns. To restrict substitute commands to a column, one can select it and switch to the command-line by typing :. After this the substitution syntax for a selected block looks like this: '\u003c,'\u003es///. :set scrollbind starts simultaneous scrolling of ‘vsplitted’ files. To set to horizontal binding of files, use command :set scrollopt=hor (after first one). Run all these commands before the :split command. :AlignCtrl I= \\t then :%Align This allows to align tables by column separators (here ‘\\t’) when the Align utility from Charles Campbell’s is installed. To sort table rows by selected lines or block, perform the visual select and then hit F3 key. The rest is interactive. To enable this function, one has to include in the .vimrc file the Vim sort script from Gerald Lai.    Settings The default settings in Vim are controlled by the .vimrc file in your home directory.\n see last chapter of vimtutor (start from shell) useful .vimrc sample when vim starts to respond very slowly then one may need to delete the .viminf* files in home directory  Help  Online Help  Find help on the web. Google will find answers to most questions on vi and vim (try searching for both terms). Purdue University Vi Tutorial Animated Vim Tutorial: https://linuxconfig.org/vim-tutorial Useful list of vim commands:  Vim Commands Cheat Sheet VimCard      You can run a tutor from the command Line:\nvimtutor # Open vim tutorial from shell, \":q\" to quit  You can also get help from within Vim:\n:help # opens help within vim, hit :q to get back to your file :help \u003ctopic\u003e # opens help on specified topic :help_topic| CTRL-] # when you are in help this command opens help topic specified between |...|, # CTRL-t brings you back to last topic :help \u003ctopic\u003e CTRL-D # gives list of help topics that contain key word : \u003cup-down keys\u003e # like in shell you get recent commands!!!!  ","excerpt":" Note: Neovim has replaced vim on the cluster, however most (if not …","ref":"/manuals/linux_basics/vim/","title":"Linux Basics - Vim Manual"},{"body":"If you require large amounts of resources (ie. 1000s of CPUs, or 100s of GPUs) then access to the computing resources at the NSF funded XSEDE might be a good option.\nAccount To create an account, visit the XSEDE Portal.\nProposal As stated previously, writting a propoasl and then getting it approved is required to gain access to XSEDE. Instructions on how to do this are outlined here.\nData Management There are several methods used to transfer data to and from XSEDE resources, they are outlined here\nJobs For submitting jobs, XSEDE also supports Slurm, which is similar to what we already use on the HPC cluster.\nExample on how to submit Slurm style jobs are described here\nUCR Campus Champion You can contact Jordan Hayes for additional information regarding XSEDE and how to gain access.\n","excerpt":"If you require large amounts of resources (ie. 1000s of CPUs, or 100s …","ref":"/manuals/ext_cloud/xsede/","title":"XSEDE"}]